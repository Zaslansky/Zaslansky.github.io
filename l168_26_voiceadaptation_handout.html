<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title></title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <style type="text/css">
  /*
   * I add this to html files generated with pandoc.
   * Originally from https://gist.github.com/killercup/5917178
   */

  html {
      overflow-y: scroll;
      -webkit-text-size-adjust: 100%;
      -ms-text-size-adjust: 100%;
  }

  body {
      color: #444;
      font-family: "Source Sans 3", Helvetica-Neue, Helvetica, Sans;
      line-height: 1.5;
      padding: 0.5em;
      margin: auto;
      max-width: 55em;
      background: #fefefe;
  }

  a {
      color: #2171b5;
      text-decoration: underline;
  }

  tr:nth-child(even) {background: #F8F8F8}
  tr:nth-child(odd) {background: #FFF}

  a:visited {
      color: #2171b5;
      text-decoration: none;
  }

  a:focus {
      outline: thin dotted;
  }

  *::-moz-selection {
      background: rgba(255, 255, 0, 0.3);
      color: #000;
  }

  *::selection {
      background: rgba(255, 255, 0, 0.3);
      color: #000;
  }

  a::-moz-selection {
      background: rgba(255, 255, 0, 0.3);
      color: #0645ad;
  }

  a::selection {
      background: rgba(255, 255, 0, 0.3);
      color: #0645ad;
  }

  p {
      margin: 0.75em 0;
  }

  img {
      max-width: 60%;
      max-height:400px;
  }

  video {
      max-width: 60%;
  }


  h1, h2, h3, h4, h5, h6 {
      color: #111;
      line-height: 80%;
      margin-top: 1em;
      margin-bottom: 0.5em;
      font-weight: normal;
  }

  h1, h2, h3, h4, h5, h6 {
      font-weight: bold;
  }

  h1 {
      font-size: 2em;
      line-height: 1.25;
      color:  #084594;

  }

  h1.title {
      margin-top:0.2em;
      font-size: 2em;
      line-height: 1.25;
  }

  h2 {
      font-size: 1.5em;
      line-height: 1.6em;
          color:  #084594;
      padding-bottom: 3px;

  }

  h3 {
      font-size: 1.2em;
      line-height: 1.6em;
  }


  h4 {
      font-size: 1.2em;
      line-height: 1.4em;
  }

  h5 {
      font-size: 1em;
  }

  h6 {
      font-size: 0.9em;
  }

  blockquote {
      color: #666666;
      margin: 0;
      padding-left: 3em;
      border-left: 0.5em #EEE solid;
  }

  hr {
      display: block;
      height: 2px;
      border: 0;
      border-top: 1px solid #aaa;
      border-bottom: 1px solid #eee;
      margin: 0.5em 0;
      padding: 0;
  }

  pre, code, kbd, samp {
      color: #000;
      font-family: monospace, monospace;
      _font-family: 'courier new', monospace;
      font-size: 0.98em;
  }

  pre {
      white-space: pre;
      white-space: pre-wrap;
      word-wrap: break-word;
  }

  .answer {
      color:#CC0033;
      font-style:italic;
  }

  b, strong {
      font-weight: bold;
  }

  dfn {
      font-style: italic;
  }

  ins {
      background: #ff9;
      color: #000;
      text-decoration: none;
  }

  mark {
      background: #ff0;
      color: #000;
      font-style: italic;
      font-weight: bold;
  }

  sub, sup {
      font-size: 75%;
      line-height: 0;
      position: relative;
      vertical-align: baseline;
  }

  sup {
      top: -0.5em;
  }

  sub {
      bottom: -0.25em;
  }

  ul, ol {
      margin: 0.5em 0;
      padding: 0em 0em 0em 1em;
  }

  ul img {
      list-style-type: none;
  }

  li p:last-child {
      margin-bottom: 0;
  }

  hr {
      border-top:none;
      height:0px;
      clear:both;
  }

  ul ul, ol ol {
      margin: .3em 0;
  }

  dl {
      margin-bottom: 1em;
  }

  dt {
      font-weight: bold;
      margin-bottom: .8em;
  }

  dd {
      margin: 0 0 .8em 2em;
  }

  dd:last-child {
      margin-bottom: 0;
  }

  img {
      border: 0;
      -ms-interpolation-mode: bicubic;
      vertical-align: middle;
  }

  figure {
      display: block;
      text-align: center;
      margin: 1em 0;
  }

  figure img {
      border: none;
      margin: 0 auto;
  }

  figcaption {
      font-size: 0.8em;
      font-style: italic;
      margin: 0 0 .8em;
  }

  table {
      margin-bottom: 2em;
      border-bottom: 1px solid #ddd;
      border-right: 1px solid #ddd;
      border-spacing: 0;
      border-collapse: collapse;
  }

  table th {
      padding: .2em 1em;
      background-color: #eee;
      border-top: 1px solid #ddd;
      border-left: 1px solid #ddd;
  }

  table td {
      padding: .2em 1em;
      border-top: 1px solid #ddd;
      border-left: 1px solid #ddd;
      vertical-align: top;
  }

  .author {
      font-size: 1.2em;
      text-align: center;
  }

  @media only screen and (min-width: 480px) {
      body {
  	font-size: 14px;
      }
  }
  @media only screen and (min-width: 768px) {
      body {
  	font-size: 16px;
      }
  }
  @media print {
      * {
  	background: transparent !important;
  	color: black !important;
  	filter: none !important;
  	-ms-filter: none !important;
      }

      body {
  	font-size: 12pt;
  	max-width: 100%;
      }

      a, a:visited {
  	text-decoration: underline;
      }

      hr {
  	height: 1px;
  	border: 0;
  	border-bottom: 1px solid black;
      }

      a[href]:after {
  	content: " (" attr(href) ")";
      }

      abbr[title]:after {
  	content: " (" attr(title) ")";
      }

      .ir a:after, a[href^="javascript:"]:after, a[href^="#"]:after {
  	content: "";
      }

      pre, blockquote {
  	border: 1px solid #999;
  	padding-right: 1em;
  	page-break-inside: avoid;
      }

      tr, img {
  	page-break-inside: avoid;
      }

      img {
  	max-width: 40% !important;
      max-height: 300px !important;
      }

      @page :left {
  	margin: 15mm 20mm 15mm 10mm;
      }

      @page :right {
  	margin: 15mm 10mm 15mm 20mm;
      }

      p, h2, h3 {
  	orphans: 3;
  	widows: 3;
      }

      h2, h3 {
  	page-break-after: avoid;
      }
  }


  ldata {
  	font-size: 0.7em;
  	margin-bottom: 0em;
  	color:#808080;
  	font-style:italic;
  }

  danger {
  	color:#FF0000;
  	font-weight:bold;
  }

  correct {
  	color:#39C900;
  	font-weight:bold;
  }

  clg{
      color:#39C900;
  	font-weight:bold;
  }

  clr{
  	color:#FF0000;
  	font-weight:bold;
  }

  clb{
  	color:#0000CC;
  	font-weight:bold;
  }

  clp{
  	color:#6600FF;
  	font-weight:bold;
  }

  clk{
  	color:#708cef;
  	font-weight:bold;
  }

  clo{
  	color:#CC6600;
  	font-weight:bold;
  }

  sc{
          font-variant: small-caps;
  }

  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="tts-voice-adaptation">TTS Voice Adaptation</h1>
<h3 id="will-styler---lign-168">Will Styler - LIGN 168</h3>
<hr />
<h3 id="todays-plan">Today’s Plan</h3>
<ul>
<li><p>What do voices tell us?</p></li>
<li><p>Encoding Identity and Emotion</p></li>
<li><p>The Neural Elements of Style</p></li>
<li><p>How well does this work?</p></li>
<li><p>Remaining TTS Questions</p></li>
</ul>
<hr />
<h2 id="what-is-a-voice">What is a voice?</h2>
<hr />
<h3 id="weve-talked-about-why-voices-matter">We’ve talked about why
voices matter</h3>
<ul>
<li><p>Voices carry social information</p></li>
<li><p>Voices carry identity information</p></li>
<li><p>Voices carry linguistic information</p></li>
<li><p>Voices carry emotional information</p></li>
</ul>
<hr />
<h3 id="voices-carry-social-information">Voices carry social
information</h3>
<ul>
<li><p>Information about gender presentation and sexuality</p></li>
<li><p>Information about racial identity (for better and <a
href="https://muse.jhu.edu/pub/24/article/900094/pdf">for
worse</a>)</p></li>
<li><p>Information about our social identity, class, and
background</p></li>
<li><p>Voices also indicate roles (e.g. newscaster, pilot, therapist
voice)</p></li>
</ul>
<hr />
<h3 id="voices-carry-identity-information">Voices carry identity
information</h3>
<ul>
<li><p>Our voice helps to ‘authenticate’ to trusted friends and
family</p></li>
<li><p>Consistency of voice helps us recognize humans</p></li>
<li><p>We can estimate things like age and height from voices</p></li>
<li><p>Some familiar voices bring comfort and ease</p></li>
</ul>
<hr />
<h3 id="voices-carry-linguistic-information">Voices carry linguistic
information</h3>
<ul>
<li><p>We can intuit a person’s language background and other
languages</p></li>
<li><p>We can guess a person’s dialect(s)</p></li>
<li><p>We can use a voice to adapt our perceptions</p>
<ul>
<li>“I think this person is Polish, so that was probably an /ɛ/
vowel”</li>
</ul></li>
</ul>
<hr />
<h3 id="voices-carry-emotional-information">Voices carry emotional
information</h3>
<ul>
<li><p>Voices can be ‘angry’ or ‘sad’ or ‘calm’ or ‘quiet’</p></li>
<li><p>Tones of voice can affect somebody’s emotional state</p></li>
<li><p>It is very possible to say the right thing incorrectly</p>
<ul>
<li>‘I don’t like your tone’</li>
</ul></li>
<li><p>‘Calming’ or ‘soft’ voices are a thing</p></li>
</ul>
<hr />
<h2 id="encoding-of-identity-and-emotion">Encoding of Identity and
Emotion</h2>
<hr />
<h3 id="voice-identity-has-many-factors">Voice Identity has many
factors</h3>
<ul>
<li><p>The story of determining voice identity isn’t based on a few
features</p></li>
<li><p>Current theories involve a high dimensional voice space and
‘distance from the average speaker’</p>
<ul>
<li>See <a
href="https://doi.org/10.1093/oxfordhb/9780198743187.013.24">Schweinberger
and Zäske 2019</a> for details</li>
</ul></li>
<li><p>The nature of this is a bit vague, but the key is that it’s not
‘just’ any few features</p></li>
</ul>
<hr />
<h3 id="social-elements-of-identity-are-tricky-too">Social Elements of
identity are tricky too</h3>
<ul>
<li><p>Our own Ben Lang has done work on our perception of Sexual
Orientation, Gender Identity, and Gender Expression</p>
<ul>
<li><em>Lang, Benjamin (2023). Reconstructing the perception of gender
identity, sexual orientation, and gender expression in American English.
Proceedings of the 20th International Congress of the Phonetic
Sciences.</em></li>
</ul></li>
<li><p>Turns out that people will tend to cluster people into social
categories</p></li>
<li><p>Meaningful features include f0, formants, fricative moments,
voicing type, and more</p></li>
<li><p>There’s still not ‘one simple cue’</p></li>
</ul>
<hr />
<h3 id="emotion-is-similarly-many-dimensional">Emotion is similarly many
dimensional</h3>
<p><img class="r-stretch" src="phonmedia/voice_emotions.png"></p>
<hr />
<h3 id="emotion-and-identity-are-noisy-channels-even-for-humans">Emotion
and Identity are noisy channels even for humans</h3>
<ul>
<li><p>There’s some evidence that linguistic content, as well as seeing
faces, affects human recognition ability</p></li>
<li><p>Cultural factors play a large role in emotion perception</p>
<ul>
<li>As well as stereotypes (e.g. <a
href="https://deepblue.lib.umich.edu/handle/2027.42/169661">the angry
black woman</a>)</li>
</ul></li>
<li><p>Anger and fear are best recognized, but even still, it’s
complicated.</p>
<ul>
<li>More details in <a
href="https://academic.oup.com/edited-volume/38687/chapter/335929179">Scherer
2019 ‘Acoustic Patterning of Emotion Vocalizations’</a> <a
href="https://doi.org/10.1093/oxfordhb/9780198743187.013.4">(DOI)</a></li>
</ul></li>
</ul>
<hr />
<h3 id="all-of-this-is-high-dimensional">All of this is
high-dimensional</h3>
<ul>
<li><p>This would be difficult to manually build a representation which
captures voice or emotion well</p></li>
<li><p>Parameterization for ‘angry’ or ‘Will Styler’ is not
straightforward</p></li>
<li><p>It sure is a shame that nobody’s developed a sort of machine
learning algorithm which is exceedingly good at learning
high-dimensional embeddings from diverse data…</p></li>
</ul>
<hr />
<h2 id="the-neural-elements-of-style">The Neural Elements of Style</h2>
<hr />
<h3
id="we-know-that-modern-neural-tts-can-reliably-carry-identity-and-emotion">We
know that Modern Neural TTS can reliably carry identity and emotion</h3>
<ul>
<li><p>We can ‘recognize’ neural voices</p>
<ul>
<li>‘Oh, that’s the TikTok voice!’</li>
</ul></li>
<li><p>We can detect emotion (or improper emotion, lack of emotion) in
TTS speech</p></li>
<li><p>These carry social information</p>
<ul>
<li>“I use the British Male voice for my Siri”</li>
</ul></li>
<li><p>But how do we do this purposefully?</p></li>
</ul>
<hr />
<h3 id="neural-tts-can-be-trained-using-any-voice">Neural TTS can be
trained using <em>any</em> voice</h3>
<ul>
<li><p>You can build a model from the ground up using any voice you’d
like</p>
<ul>
<li><a
href="https://www.npr.org/2024/05/20/1252495087/openai-pulls-ai-voice-that-was-compared-to-scarlett-johansson-in-the-movie-her">Except
Scarlett Johansson</a></li>
</ul></li>
<li><p>If all your training data are from a bored Bostonian, you’ll end
up with a bored Bostonian TTS voice</p></li>
<li><p>This is very expensive, though, and doesn’t scale well at all</p>
<ul>
<li>You also need <em>lots</em> of data from the new speaker</li>
</ul></li>
</ul>
<hr />
<h3 id="you-can-also-fine-tune-existing-models">You can also fine-tune
existing models</h3>
<ul>
<li><p>Train on one voice, and then do more training on data only from a
second voice</p>
<ul>
<li>Generally with slight modifications to learning rate and targeted
layers</li>
</ul></li>
<li><p>This is not so different from training an ASR model on a few
languages, and then giving it some (e.g.) Tira data to modify how it
does the task.</p></li>
<li><p>This is easier than making a new model, and makes quite accurate
voices</p>
<ul>
<li>But still rather expensive</li>
<li>… and requires expensive training to be done <em>long before the
voice is used</em></li>
</ul></li>
</ul>
<hr />
<h3
id="if-you-want-a-near-perfect-simulacrum-training-and-fine-tuning-may-be-best">If
you want a near-perfect simulacrum, training and fine-tuning may be
best</h3>
<ul>
<li><p>This allows you to fit <em>all elements of the model</em> to the
voice in question</p></li>
<li><p>You’ll train text analysis, prosody/duration modeling, phonemes,
all to match that particular human</p></li>
<li><p>If you have the budget and the amount of data from the speaker
needed, there’s no reason not to just create a full model!</p></li>
</ul>
<hr />
<h3 id="these-approaches-are-very-redundant">These approaches are very
<em>redundant</em></h3>
<ul>
<li><p>If a model knows how to say ‘penguin’ already, why re-train just
to have another person say it?</p></li>
<li><p>How much really differs prosodically across two
dialects?</p></li>
<li><p>Why build a new model for text analysis differ if you just want
somebody who sounds a bit different?</p></li>
<li><p><em>Most of the learned task doesn’t change, even when the voice
does!</em></p></li>
</ul>
<hr />
<h3 id="we-can-think-of-all-speech-as-having-content-and-style">We can
think of all speech as having ‘content’ and ‘style’</h3>
<ul>
<li>Voices express linguistic ‘content’
<ul>
<li>Phonemes, with ordering, and necessary tone/prosody for
comprehension</li>
<li>This is ‘all we need’ to understand the utterances</li>
</ul></li>
<li>‘Style’ is everything else we’ve been talking about
<ul>
<li>‘Speaker’ identity</li>
<li>Social components</li>
<li>Emotional content</li>
<li>Plus prosodic factors (e.g. speed, emphasis, prosodic ‘tunes’,
sarcasm)</li>
</ul></li>
</ul>
<hr />
<h3
id="multi-speaker-tts-systems-attempt-to-model-content-and-style-separately">Multi-Speaker
TTS systems attempt to model content and style separately</h3>
<ul>
<li><p>This is often a dual-encoder architecture</p></li>
<li><p>One encoder creates a ‘style embedding’</p>
<ul>
<li>This captures speaker, emotion, prosody, and other details</li>
</ul></li>
<li><p>The other encoder captures linguistic ‘content’ information</p>
<ul>
<li>This helps us to go from text input to an output mel
spectrogram</li>
</ul></li>
<li><p>Both encoders’ output is combined to help generate the
output</p></li>
</ul>
<hr />
<h3 id="heres-vanilla-tacotron2">Here’s Vanilla TacoTron2</h3>
<p><img class="r-stretch" src="phonmedia/tts_tacotron2.png"></p>
<hr />
<h3 id="heres-a-multi-speaker-version">Here’s a Multi-Speaker
Version</h3>
<p><img class="r-stretch" src="phonmedia/tts_tacotron2_multispeaker.png"></p>
<hr />
<h3 id="these-too-show-lots-of-variation">These too show lots of
variation</h3>
<ul>
<li><p>There are many ways to do multi-speaker TTS!</p></li>
<li><p>… but this is a good, illustrative case!</p></li>
<li><p>Note that this is an example of the larger phenomenon of ‘Neural
Style Transfer’</p></li>
</ul>
<hr />
<h3 id="multi-speaker-inference">Multi-Speaker Inference</h3>
<ul>
<li>Conventional TTS just needs one input: Text
<ul>
<li>“Generate the waveform which best approximates this sentence”</li>
</ul></li>
<li>Multi-Speaker TTS needs two inputs: Text, and a Style Embedding
<ul>
<li>“Generate the waveform which best approximates this sentence given
this speaker”</li>
</ul></li>
</ul>
<hr />
<h3 id="what-does-a-style-embedding-look-like">What does a style
embedding look like?</h3>
<ul>
<li><em>Previously, on LIGN 168!</em></li>
</ul>
<hr />
<h3 id="speaker-identification-has-often-used-i-vectors">Speaker
Identification has often used i-vectors</h3>
<ul>
<li><p>Pull MFCCs</p></li>
<li><p>Look at the overall variability in speech based on the Universal
Background Model (UBM)</p></li>
<li><p>Use Factor Analysis to identify the relevant elements of
variability</p></li>
<li><p>Now generate an ‘i-vector’ which models the person in the greater
variability approach</p></li>
</ul>
<hr />
<h3 id="x-vectors-are-a-neural-equivalent">x-vectors are a neural
equivalent</h3>
<ul>
<li><p>Pull MFCCs for many, many speakers</p></li>
<li><p>Use a specialized Deep Neural Network to classify many speakers
in a large training set</p>
<ul>
<li>This is trained using contrastive loss, to optimally separate
different speakers in the dataset</li>
</ul></li>
<li><p>Now, find the speaker’s individual embedding in that space by
looking across frames in an audio sample</p>
<ul>
<li><a href="https://arxiv.org/pdf/1910.10838">Here are some nice
references</a></li>
</ul></li>
<li><p>This embedding is the ‘x-vector’, and has a bit more robustness
and nuance</p>
<ul>
<li>… at cost of great computational complexity</li>
</ul></li>
</ul>
<hr />
<h3 id="these-embeddings-are-designed-to-distinguish-speakers">These
embeddings are <em>designed to distinguish speakers</em></h3>
<ul>
<li>Often times, specialized embedding methods are designed to
<em>maximize contrast between desired categories</em>
<ul>
<li>Remember Wav2Vec2 building embeddings to contrast linguistic
units?</li>
</ul></li>
<li>By using speaker identification techniques, we’re <em>building
contrast between voices into the model directly</em>
<ul>
<li>… and if it’s well designed, we should get differences in voice,
more than in language or words or context!</li>
</ul></li>
</ul>
<hr />
<h3 id="loss-isnt-straightforward-here">Loss isn’t straightforward
here</h3>
<ul>
<li><p>Loss for TTS systems is generally comparison with the expected,
training data mel spectrogram of the sentence</p>
<ul>
<li>“How closely does the TTS output text+mel spectrogram match the
ground-truth mel spectrogram?”</li>
</ul></li>
<li><p>When you’re taking a clip from a speaker and then making up
<em>different</em> sentence, this doesn’t make sense.</p>
<ul>
<li>There’s no existing sentence to compare the output against!</li>
</ul></li>
</ul>
<hr />
<h3 id="loss-strategies">Loss Strategies</h3>
<ul>
<li>Some systems validate the embedding directly
<ul>
<li>‘Given samples A B C, where A B are from one speaker and C is
another, the computed embeddings for A and B should be closer than to
C’</li>
</ul></li>
<li>Some systems calculate the speaker embedding of the output, and
compare to the input embedding
<ul>
<li>‘The output voice embedding should be cosine-similar to the input
voice’</li>
<li><a href="https://arxiv.org/pdf/1802.06984">Here’s a nice
example</a></li>
<li><a href="https://arxiv.org/pdf/1910.10838">Here’s another
system</a></li>
</ul></li>
</ul>
<hr />
<h3 id="loss-based-on-the-embedding-distance-forces-similar-voices">Loss
based on the embedding distance forces similar voices!</h3>
<ul>
<li><p>You are minimizing the distance between two embeddings which
represent voice similarity as distance</p></li>
<li><p>When the embedding is similar, the output should be
<em>identifiably similar</em> to the input embedding</p></li>
<li><p><em>It doesn’t matter what characteristics in the signal make a
voice sound like it does, this approach should force the TTS to be
similar to that!</em></p></li>
</ul>
<hr />
<h3
id="multi-speaker-tts-systems-allow-us-to-produce-arbitrary-texts-with-arbitrary-style">Multi-Speaker
TTS systems allow us to produce arbitrary texts with arbitrary
style</h3>
<ul>
<li><p>Part of the network does the classical TTS task</p></li>
<li><p>Another part processes the embedding and injects it into the
input stream</p></li>
<li><p>The output should approximate both the content and the
style!</p></li>
</ul>
<hr />
<h3
id="modern-tools-often-build-the-style-extractor-right-into-the-network">Modern
tools often build the style extractor right into the network</h3>
<ul>
<li><p>The encoder for style takes the audio as input, and outputs a
fixed-length x-vector (or equivalent)</p></li>
<li><p>The input linguistic content doesn’t matter, it just has to be
enough frames to build a representative x-vector</p>
<ul>
<li>Adding more data doesn’t make the result better here!</li>
<li>This is unlike fine tuning</li>
</ul></li>
<li><p><strong>This means that we don’t need a lot of data to ‘clone’ a
voice</strong></p></li>
</ul>
<hr />
<h3 id="style-is-truly-separate-from-content">Style is truly separate
from content!</h3>
<ul>
<li><p>This is about voice, not language, so style transfer works across
languages</p></li>
<li><p>The speaker is the same, it’s just the content that
changes</p></li>
<li><p>More on this in a minute!</p></li>
</ul>
<hr />
<h3 id="emotional-voices-are-a-similar-task">Emotional voices are a
similar task!</h3>
<ul>
<li><p>Instead of passing in new speakers, pass in x-vectors
corresponding to new emotional states</p></li>
<li><p>Give labeled states, and then apply those labels</p></li>
<li><p>You may want more granularity, but this is workable!</p></li>
</ul>
<hr />
<h2 id="how-well-does-this-work">How well does this work?</h2>
<hr />
<h3 id="unreasonably-well">Unreasonably well!</h3>
<ul>
<li><p>Modern tools are quite frighteningly good at this</p></li>
<li><p>The ‘goodness’ of the tool varies as a function of how close the
voice is to Standardized English</p></li>
</ul>
<hr />
<h3 id="neural-network-text-to-speech-style-transfer-examples">Neural
Network Text-to-Speech Style Transfer Examples</h3>
<audio controls src="comp/tts_squidward_ling.wav">
</audio>
<audio controls src="comp/tts_arnie_ling.wav">
</audio>
<audio controls src="comp/tts_snape_ling.wav">
</audio>
<audio controls src="comp/tts_clarkson_ling.wav">
</audio>
<audio controls src="comp/tts_gilbert_ling.wav">
</audio>
<audio controls src="comp/tts_optimus_ling.wav">
</audio>
<audio controls src="comp/tts_mario_ling.wav">
</audio>
<hr />
<h3 id="neural-styler-transfer">Neural Styler Transfer</h3>
<p><img class="r-stretch" src="comp/styler_transfer.jpg"></p>
<p>(Thanks to Winston Durand!)</p>
<hr />
<h3 id="neural-styler-transfer-1">Neural Styler Transfer</h3>
<audio controls src="comp/tts_will_ling.wav">
</audio>
<p>(TacoTron2)</p>
<audio controls src="comp/tts_will_elclone.wav">
</audio>
<p>(ElevenLabs)</p>
<p>(Credit to Erick Amaro and Mia Khattar!)</p>
<hr />
<h3 id="multilingual-examples">Multilingual Examples</h3>
<audio controls src="comp/tts_will_english.mp3">
</audio>
<p>(English)</p>
<audio controls src="comp/tts_will_french.mp3">
</audio>
<p>(French)</p>
<audio controls src="comp/tts_will_spanish.mp3">
</audio>
<p>(Spanish)</p>
<audio controls src="comp/tts_will_mandarin.mp3">
</audio>
<p>(Mandarin)</p>
<audio controls src="comp/tts_will_italian.mp3">
</audio>
<p>(Italian)</p>
<audio controls src="comp/tts_will_russian.mp3">
</audio>
<p>(Russian)</p>
<audio controls src="comp/tts_will_japanese.mp3">
</audio>
<p>(Japanese)</p>
<hr />
<h3 id="code-switching">Code Switching</h3>
<p>It’s like sometimes mezclo un poco de español con my English, cuando
me siento particularmente spicy, y tengo curiosidad to know cómo la TTS
handles it.</p>
<audio controls src="comp/tts_will_codeswitch.mp3">
</audio>
<hr />
<h3 id="this-system-isnt-perfect">This system isn’t perfect</h3>
<blockquote>
<p>Adenocarcinoma in Tubovillious Adenoma bona fide certiorari de jure
collusion RICO ex post facto CVN AWACS Escapement Tourbillion Remontoir
de Egalite</p>
</blockquote>
<audio controls src="comp/tts_will_jargon.mp3">
</audio>
<hr />
<h3 id="prosody-is-still-hard">Prosody is still hard</h3>
<audio controls src="comp/tts_will_rick.mp3">
</audio>
<hr />
<h3 id="yet-these-systems-are-concerningly-good">Yet, these systems are
concerningly good</h3>
<ul>
<li><p>Many of the classical issues in TTS aren’t really issues
anymore</p></li>
<li><p>We can do not just different voices, but specific voices</p></li>
<li><p>… and we can work across languages very effectively</p></li>
<li><p>Wow.</p></li>
</ul>
<hr />
<h3 id="tts-technology-has-far-outpaced-my-expectations">TTS technology
has far outpaced my expectations</h3>
<ul>
<li><p>Where we are in 2024 vs. 2011 is wildly different</p></li>
<li><p>Deep Neural Networks have enabled possibilities that I couldn’t
have imagined</p></li>
<li><p>We have much to improve, but less than I’d thought!</p></li>
</ul>
<hr />
<h2 id="lingering-tts-questions">Lingering TTS Questions?</h2>
<hr />
<h3 id="any-remaining-questions-on-tts">Any remaining questions on
TTS?</h3>
<hr />
<h3 id="wrapping-up">Wrapping Up</h3>
<ul>
<li><p>Voices carry lots of information, socially and
linguistically</p></li>
<li><p>Encoding identity and social elements is not remotely
straightforward, and uses many, many features</p></li>
<li><p>Neural style transfer separates content from style, and allows us
to reproduce the same content with a different approach</p></li>
<li><p>It works really, shockingly well</p></li>
</ul>
<hr />
<h3 id="next-time">Next time</h3>
<ul>
<li>We’ll give you some time to work on your projects and collaborate
with your colleagues!</li>
</ul>
<hr />
<p><huge>Thank you!</huge></p>
</body>
</html>
