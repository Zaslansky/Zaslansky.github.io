<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title></title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <style type="text/css">
  /*
   * I add this to html files generated with pandoc.
   * Originally from https://gist.github.com/killercup/5917178
   */

  html {
      overflow-y: scroll;
      -webkit-text-size-adjust: 100%;
      -ms-text-size-adjust: 100%;
  }

  body {
      color: #444;
      font-family: "Source Sans 3", Helvetica-Neue, Helvetica, Sans;
      line-height: 1.5;
      padding: 0.5em;
      margin: auto;
      max-width: 55em;
      background: #fefefe;
  }

  a {
      color: #2171b5;
      text-decoration: underline;
  }

  tr:nth-child(even) {background: #F8F8F8}
  tr:nth-child(odd) {background: #FFF}

  a:visited {
      color: #2171b5;
      text-decoration: none;
  }

  a:focus {
      outline: thin dotted;
  }

  *::-moz-selection {
      background: rgba(255, 255, 0, 0.3);
      color: #000;
  }

  *::selection {
      background: rgba(255, 255, 0, 0.3);
      color: #000;
  }

  a::-moz-selection {
      background: rgba(255, 255, 0, 0.3);
      color: #0645ad;
  }

  a::selection {
      background: rgba(255, 255, 0, 0.3);
      color: #0645ad;
  }

  p {
      margin: 0.75em 0;
  }

  img {
      max-width: 60%;
      max-height:400px;
  }

  video {
      max-width: 60%;
  }


  h1, h2, h3, h4, h5, h6 {
      color: #111;
      line-height: 80%;
      margin-top: 1em;
      margin-bottom: 0.5em;
      font-weight: normal;
  }

  h1, h2, h3, h4, h5, h6 {
      font-weight: bold;
  }

  h1 {
      font-size: 2em;
      line-height: 1.25;
      color:  #084594;

  }

  h1.title {
      margin-top:0.2em;
      font-size: 2em;
      line-height: 1.25;
  }

  h2 {
      font-size: 1.5em;
      line-height: 1.6em;
          color:  #084594;
      padding-bottom: 3px;

  }

  h3 {
      font-size: 1.2em;
      line-height: 1.6em;
  }


  h4 {
      font-size: 1.2em;
      line-height: 1.4em;
  }

  h5 {
      font-size: 1em;
  }

  h6 {
      font-size: 0.9em;
  }

  blockquote {
      color: #666666;
      margin: 0;
      padding-left: 3em;
      border-left: 0.5em #EEE solid;
  }

  hr {
      display: block;
      height: 2px;
      border: 0;
      border-top: 1px solid #aaa;
      border-bottom: 1px solid #eee;
      margin: 0.5em 0;
      padding: 0;
  }

  pre, code, kbd, samp {
      color: #000;
      font-family: monospace, monospace;
      _font-family: 'courier new', monospace;
      font-size: 0.98em;
  }

  pre {
      white-space: pre;
      white-space: pre-wrap;
      word-wrap: break-word;
  }

  .answer {
      color:#CC0033;
      font-style:italic;
  }

  b, strong {
      font-weight: bold;
  }

  dfn {
      font-style: italic;
  }

  ins {
      background: #ff9;
      color: #000;
      text-decoration: none;
  }

  mark {
      background: #ff0;
      color: #000;
      font-style: italic;
      font-weight: bold;
  }

  sub, sup {
      font-size: 75%;
      line-height: 0;
      position: relative;
      vertical-align: baseline;
  }

  sup {
      top: -0.5em;
  }

  sub {
      bottom: -0.25em;
  }

  ul, ol {
      margin: 0.5em 0;
      padding: 0em 0em 0em 1em;
  }

  ul img {
      list-style-type: none;
  }

  li p:last-child {
      margin-bottom: 0;
  }

  hr {
      border-top:none;
      height:0px;
      clear:both;
  }

  ul ul, ol ol {
      margin: .3em 0;
  }

  dl {
      margin-bottom: 1em;
  }

  dt {
      font-weight: bold;
      margin-bottom: .8em;
  }

  dd {
      margin: 0 0 .8em 2em;
  }

  dd:last-child {
      margin-bottom: 0;
  }

  img {
      border: 0;
      -ms-interpolation-mode: bicubic;
      vertical-align: middle;
  }

  figure {
      display: block;
      text-align: center;
      margin: 1em 0;
  }

  figure img {
      border: none;
      margin: 0 auto;
  }

  figcaption {
      font-size: 0.8em;
      font-style: italic;
      margin: 0 0 .8em;
  }

  table {
      margin-bottom: 2em;
      border-bottom: 1px solid #ddd;
      border-right: 1px solid #ddd;
      border-spacing: 0;
      border-collapse: collapse;
  }

  table th {
      padding: .2em 1em;
      background-color: #eee;
      border-top: 1px solid #ddd;
      border-left: 1px solid #ddd;
  }

  table td {
      padding: .2em 1em;
      border-top: 1px solid #ddd;
      border-left: 1px solid #ddd;
      vertical-align: top;
  }

  .author {
      font-size: 1.2em;
      text-align: center;
  }

  @media only screen and (min-width: 480px) {
      body {
  	font-size: 14px;
      }
  }
  @media only screen and (min-width: 768px) {
      body {
  	font-size: 16px;
      }
  }
  @media print {
      * {
  	background: transparent !important;
  	color: black !important;
  	filter: none !important;
  	-ms-filter: none !important;
      }

      body {
  	font-size: 12pt;
  	max-width: 100%;
      }

      a, a:visited {
  	text-decoration: underline;
      }

      hr {
  	height: 1px;
  	border: 0;
  	border-bottom: 1px solid black;
      }

      a[href]:after {
  	content: " (" attr(href) ")";
      }

      abbr[title]:after {
  	content: " (" attr(title) ")";
      }

      .ir a:after, a[href^="javascript:"]:after, a[href^="#"]:after {
  	content: "";
      }

      pre, blockquote {
  	border: 1px solid #999;
  	padding-right: 1em;
  	page-break-inside: avoid;
      }

      tr, img {
  	page-break-inside: avoid;
      }

      img {
  	max-width: 40% !important;
      max-height: 300px !important;
      }

      @page :left {
  	margin: 15mm 20mm 15mm 10mm;
      }

      @page :right {
  	margin: 15mm 10mm 15mm 20mm;
      }

      p, h2, h3 {
  	orphans: 3;
  	widows: 3;
      }

      h2, h3 {
  	page-break-after: avoid;
      }
  }


  ldata {
  	font-size: 0.7em;
  	margin-bottom: 0em;
  	color:#808080;
  	font-style:italic;
  }

  danger {
  	color:#FF0000;
  	font-weight:bold;
  }

  correct {
  	color:#39C900;
  	font-weight:bold;
  }

  clg{
      color:#39C900;
  	font-weight:bold;
  }

  clr{
  	color:#FF0000;
  	font-weight:bold;
  }

  clb{
  	color:#0000CC;
  	font-weight:bold;
  }

  clp{
  	color:#6600FF;
  	font-weight:bold;
  }

  clk{
  	color:#708cef;
  	font-weight:bold;
  }

  clo{
  	color:#CC6600;
  	font-weight:bold;
  }

  sc{
          font-variant: small-caps;
  }

  </style>
</head>
<body>
<h2 id="using-transparent-machine-learning-to-study-human-speech">Using
Transparent Machine Learning to study Human Speech</h2>
<h3 id="will-styler">Will Styler</h3>
<p>UC San Diego Department of Linguistics</p>
<p>wstyler@ucsd.edu</p>
<hr />
<h2 id="humans-are-necessary-for-linguistic-research">Humans are
necessary for linguistic research</h2>
<ul>
<li><p>Any hypothesis about human language must be tested with human
speakers</p></li>
<li><p>… but testing with human subjects is a painful process</p>
<ul>
<li><p>IRBs are required</p></li>
<li><p>It’s time consuming</p></li>
<li><p>It’s expensive</p></li>
<li><p>Studies can be difficult to design</p></li>
<li><p>Each participant has a different language background</p></li>
</ul></li>
</ul>
<hr />
<p>So, even though we need humans to test our hypotheses and
theories…</p>
<ul>
<li><p><strong>Any information we can get <em>without actually involving
humans</em> is great</strong></p></li>
<li><p>Today, we’re going to talk about collecting this kind of
information using…</p></li>
</ul>
<hr />
<h1 id="machine-learning">Machine Learning</h1>
<hr />
<h3 id="machine-learning-1">Machine Learning</h3>
<ul>
<li><p>Asking computer algorithms to take data and
<em>independently</em> build a model of it</p>
<ul>
<li>“I don’t know what this pattern is. You figure it out and apply
it”.</li>
</ul></li>
<li><p>Heavily statistical</p></li>
<li><p>Many different algorithms</p>
<ul>
<li>Each with strengths, weaknesses, and use cases</li>
</ul></li>
</ul>
<hr />
<h3 id="well-be-talking-about-classifiers">We’ll be talking about
classifiers</h3>
<ul>
<li><p>Classifiers read in features, then sort and classify data into
multiple categories</p></li>
<li><p>Usable for known data (to learn more about the patterns)</p></li>
<li><p>Usable for predicting new data</p></li>
</ul>
<hr />
<h3
id="we-can-think-about-classifiers-as-being-opaque-or-transparent">We
can think about classifiers as being ‘opaque’ or ‘transparent’</h3>
<ul>
<li><p>“Can we understand exactly how and why <em>this</em> decision was
made?”</p></li>
<li><p>Lots of modern ML is being done with Deep Neural Networks or
other <em>opaque</em>, ‘black box’ methods</p>
<ul>
<li>Here, the internal mechanisms used to make decisions are not
analyzable</li>
</ul></li>
<li><p>We’re going to focus on more transparent classifiers</p>
<ul>
<li>Decisions are scrutable and the mechanisms are clear(er)</li>
</ul></li>
</ul>
<hr />
<h3 id="supervised-machine-classification-101">Supervised Machine
Classification 101</h3>
<ul>
<li><p>Select a large corpus of data, and manually assign each
observation to a group</p>
<ul>
<li>“Good mail” vs. “Spam” or “/i/” vs. “/u/”</li>
</ul></li>
<li><p><strong>Training:</strong> Feed this labeled data into an
algorithm so it can learn the patterns</p>
<ul>
<li>Using a defined set of features of the data</li>
</ul></li>
<li><p><strong>Testing:</strong> Give the trained algorithm new data
without labels, and check the accuracy of its classifications</p>
<ul>
<li>Better accuracy often indicates more useful information was given to
the classifier!</li>
</ul></li>
</ul>
<hr />
<h3 id="machine-classification-is-everywhere">Machine Classification is
<em>everywhere</em></h3>
<ul>
<li><p>“Is this email spam, or not?”</p></li>
<li><p>“Should we lend this person money?”</p></li>
<li><p>“Is this handwritten symbol”1” or “2” or “3” or…?”</p></li>
<li><p>“Is this word a noun, or a verb, or an adjective, or…?”</p></li>
<li><p>So, these algorithms are well-studied and optimized</p>
<ul>
<li>… but they’re usually used for engineering and problem-solving, not
research</li>
</ul></li>
</ul>
<hr />
<h3 id="they-have-some-serious-advantages-for-studying-language">They
have some serious advantages for studying language!</h3>
<ul>
<li><p>Their decisions are easier to quantify than humans’</p></li>
<li><p>They’ll (often) tell you <em>how</em> they made the decision they
did</p></li>
<li><p>They have no knowledge that you don’t give to them</p></li>
<li><p>They make all decisions independently</p></li>
<li><p>They don’t require payment or scheduling</p></li>
<li><p>They’re available 24/7</p></li>
</ul>
<hr />
<h2 id="todays-talk">Today’s Talk</h2>
<ul>
<li><p><strong>How can machine learning be used to complement humans in
linguistic research?</strong></p></li>
<li><p>Two very different domains, problems and goals</p>
<ul>
<li><p><strong>Part 1: Speech Perception</strong>: What are the acoustic
cues to vowel nasality?</p></li>
<li><p><strong>Part 2: Speech Production</strong>: How can we identify
specific articulatory gestures or postures in connected speech?</p></li>
</ul></li>
<li><p>Both show strong evidence that algorithms can model human
judgement and perception in tough problems</p></li>
</ul>
<hr />
<h1 id="part-1-machine-learning-and-speech-perception">Part 1: Machine
Learning and Speech Perception</h1>
<hr />
<h2 id="vowel-nasality">Vowel Nasality</h2>
<ul>
<li>Vowel Nasality is the opening of the Velopharyngeal port during the
vowel <!-- .element: class="fragment" --></li>
</ul>
<center>
<!-- .element: class="fragment" -->
<table>
<tr>
<th>
‘Cat’<br>[kæt]
</th>
<th>
‘Can’t’<br>[kæ̃nt]
</th>
</tr>
</table>
</center>
<ul>
<li><p><strong>What are the acoustic cues used for perceiving vowel
nasality in English?</strong></p>
<ul>
<li>The biggest problem with this question is that…</li>
</ul></li>
</ul>
<hr />
<h2 id="the-signal-is-incredibly-rich">The signal is incredibly
rich</h2>
<p><img class="r-stretch" src="phonmedia/jasa2017_figure1.jpg"></p>
<hr />
<h2 id="the-signal-is-incredibly-rich-1">The signal is incredibly
rich</h2>
<p><img class="r-stretch" src="phonmedia/noisebbspectrogram.jpg"></p>
<hr />
<h3 id="potential-cues-for-evaluation">29 Potential Cues for
evaluation</h3>
<ul>
<li><p>All spectral or temporal features in the signal</p></li>
<li><p>Some absolute, some relative</p></li>
<li><p>Features like…</p>
<ul>
<li>Formant Frequencies and Bandwidths</li>
<li>Spectral Relationships (like A1-P0 or A3-P0)</li>
<li>Nasal Resonances</li>
<li>Spectral Tilt</li>
<li>Vowel Duration</li>
<li>… and more!</li>
</ul></li>
</ul>
<hr />
<h3 id="testing-the-perception-of-29-features-is-not-trivial">Testing
the perception of 29 features is <em>not</em> trivial</h3>
<ul>
<li><p>Each feature requires many stimuli to be generated</p></li>
<li><p>Each stimulus might need several repetitions</p></li>
<li><p>The task is deeply boring</p></li>
<li><p>You’ll run out of participant endurance long before you run out
of features</p></li>
<li><p>So, instead of asking humans to evaluate all 29, let’s
use…</p></li>
</ul>
<hr />
<h2 id="machine-speech-perception">Machine Speech Perception</h2>
<table style="width:6%;">
<colgroup>
<col style="width: 5%" />
</colgroup>
<tbody>
<tr class="odd">
<td>### The Basic Idea</td>
</tr>
<tr class="even">
<td>Human speech perception is just classifying sounds based on
acoustical features</td>
</tr>
<tr class="odd">
<td>* <strong>Computers can do that too!</strong></td>
</tr>
<tr class="even">
<td>* Give the acoustic feature information to a classifier and ask for
oral vs. nasal judgements</td>
</tr>
<tr class="odd">
<td>* Greater accuracy means a feature or grouping is more useful and
informative!</td>
</tr>
</tbody>
</table>
<h3 id="the-plan">The Plan</h3>
<ul>
<li><p>1: Collect a corpus of oral and nasal words, and measure each
feature</p></li>
<li><p>2: Give each feature to a Machine Learning Algorithm
individually</p>
<ul>
<li>The most informative features should be the most accurate</li>
</ul></li>
<li><p>3: Find the best group of features</p>
<ul>
<li>Find the balance between “few features” and “good accuracy”</li>
</ul></li>
<li><p>4: Test <em>those</em> features with expensive and difficult
humans</p></li>
</ul>
<hr />
<h3 id="labeling-and-training">Labeling and Training</h3>
<ul>
<li><p>I recorded 12 English speakers making words with oral and
nasal(ized) vowels</p>
<ul>
<li><p>“Oral” vowels were in CVC contexts, and “Nasal” were in
CVN/NVC/NVN contexts</p></li>
<li><p>This resulted in 3823 words</p></li>
</ul></li>
<li><p>Then, I measured each of the 29 features at two timepoints per
vowel</p>
<ul>
<li>All measurement was done automatically by Praat Script</li>
</ul></li>
<li><p>Then I handed them to a Support Vector Machine as training
data</p></li>
</ul>
<hr />
<h3 id="support-vector-machines">Support Vector Machines</h3>
<ul>
<li><p>A very common, very accurate machine learning algorithm</p></li>
<li><p>Look at all the data in an multi-dimensional space</p>
<ul>
<li>As many dimensions as features</li>
</ul></li>
<li><p>Try to find a line or hyperplane that optimally separates the
classes</p></li>
<li><p>Classification is just seeing where the new data is relative to
that line</p></li>
</ul>
<hr />
<h2 id="so-how-does-it-perform-with-nasality"><strong>So how does it
perform with nasality?</strong></h2>
<hr />
<h3 id="single-feature-testing">Single-Feature testing</h3>
<ul>
<li><p>Are any features good enough <em>on their own</em> to allow
recognition of nasal vowels?</p></li>
<li><p>29 separate models (one per feature) classifying datapoints as
“oral” or “nasal”</p></li>
<li><p>Each model outputs accuracy figures, which we can
compare</p></li>
</ul>
<hr />
<h3 id="single-feature-findings">Single-feature findings</h3>
<ul>
<li><p>F1’s Bandwidth is the most useful and informative feature</p>
<ul>
<li>67.6% SVM accuracy</li>
</ul></li>
<li><p>A1-P0, a measure of relative spectral prominence, gets second
place</p>
<ul>
<li>64.7% SVM accuracy</li>
</ul></li>
<li><p>The worst feature performed at 51.23% accuracy</p></li>
<li><p><em>None of the features are good enough on their
own!</em></p></li>
</ul>
<hr />
<h2 id="what-group-of-features-provides-the-best-information">What
<em>group of features</em> provides the best information?</h2>
<hr />
<h3 id="multi-feature-modeling">Multi-feature modeling</h3>
<ul>
<li><p>Tested 10 <em>a priori</em> feature groupings</p>
<ul>
<li>Selected from various outputs of the machine learning and
statistics</li>
</ul></li>
<li><p>Compared the accuracy <em>in light of the number of
features</em></p>
<ul>
<li>The winning model gets the best performance from the fewest
features</li>
</ul></li>
</ul>
<hr />
<h3 id="multi-feature-results">Multi-feature Results</h3>
<ul>
<li><p>SVMs with all features worked best (29 features)</p>
<ul>
<li>84.7% accuracy</li>
</ul></li>
<li><p>Formant Frequency and Bandwidth, Spectral Tilt, A1-P0, and Vowel
Duration was the best subgroup (5 features)</p>
<ul>
<li>82.2% accuracy</li>
</ul></li>
<li><p><strong>We only lose 2.5% accuracy when we reduce our feature set
by 69%!</strong></p>
<ul>
<li>That’s a promising grouping!</li>
</ul></li>
</ul>
<hr />
<h3 id="overall-machine-learning-results">Overall Machine Learning
Results</h3>
<ul>
<li><p><strong>Formant Bandwidth</strong> was the most useful single
feature for English (62.5% accuracy)</p></li>
<li><p>… and we’ve got a multi-feature grouping with very good accuracy
(82.2% accuracy)!</p>
<ul>
<li>Formant Width, Formant Frequency, Spectal Tilt, A1-P0, and
Duration</li>
</ul></li>
<li><p><strong>So, let’s test those five features with actual
humans!</strong></p></li>
</ul>
<hr />
<h1 id="human-perception">Human Perception</h1>
<hr />
<h3 id="methods">Methods</h3>
<ul>
<li><p>English listeners can use vowel nasality to identify missing
nasal consonants</p>
<ul>
<li>ba_ could be “bad” or “ban”</li>
</ul></li>
<li><p><strong>Let’s add or remove features from vowels to see what
indicates “nasality”!</strong></p></li>
<li><p>If adding or removing a feature changes perception, or makes them
react more slowly, it’s important!</p></li>
</ul>
<hr />
<h3 id="the-modifications">The Modifications</h3>
<p>Use signal processing to simulate the oral-to-nasal change (or vice
versa) in…</p>
<ul>
<li><p>1: A1-P0 (or vice versa)</p></li>
<li><p>2: Duration</p></li>
<li><p>3: Spectral Tilt</p></li>
<li><p>4: Formant Bandwidth and Frequency</p>
<ul>
<li>Combined</li>
</ul></li>
<li><p>5: Modify <em>all five features at once!</em></p></li>
</ul>
<hr />
<h3 id="the-experiment">The Experiment</h3>
<ul>
<li><p>Recruited 42 normal-hearing Native English speakers from a
department subject pool</p></li>
<li><p>Each listened to 400 words with different modifications</p></li>
<li><p>Analyzed both confusion and reaction time associated with
stimulus changes</p></li>
</ul>
<hr />
<center>
<table>
<tr>
<th>
<h1>
bad
</h1>
</th>
<th>
<h1>
ban
</h1>
</th>
</tr>
</table>
</center>
<audio controls>
<source src="phonmedia/diss_hazel_BAD_nfor_ex_c.wav" type="audio/wav">
</audio>
<hr />
<center>
<table>
<tr>
<th>
<h1>
bomb
</h1>
</th>
<th>
<h1>
bob
</h1>
</th>
</tr>
</table>
</center>
<audio controls>
<source src="phonmedia/diss_molly_BOMB_ofor_ex_c.wav" type="audio/wav">
</audio>
<hr />
<h3 id="human-perception-summary">Human Perception Summary</h3>
<ul>
<li><p>Only <strong>formant modification</strong> had a significant
effect on perception</p></li>
<li><p>Formant modification caused listeners to respond more
slowly</p></li>
<li><p>Formant modification made oral vowels sound “nasal”</p></li>
<li><p>F1’s bandwidth is probably the cue</p>
<ul>
<li>This makes sense acoustically, and Hawkins and Stevens (1985) also
points in that direction</li>
</ul></li>
</ul>
<hr />
<h3 id="score-one-for-the-machine">Score one for the Machine!</h3>
<ul>
<li><p>The machine learning models predicted F1’s bandwidth as the most
useful feature…</p></li>
<li><p>… and the humans agreed!</p></li>
<li><h3 id="how-similar-are-the-svms-and-the-humans">How similar
<em>are</em> the SVMs and the humans?</h3></li>
</ul>
<hr />
<h1 id="humans-vs.-machines">Humans vs. Machines</h1>
<p><img class="r-stretch" src="img/terminator.png"></p>
<hr />
<p><em>Let’s give the computer the same experimental task as the humans,
using the same altered stimuli, and see how they compare!</em></p>
<hr />
<h3 id="testing-humans-vs.-machines">Testing Humans vs. Machines</h3>
<ul>
<li><p>1: Train an SVM on all of the English Data</p></li>
<li><p>2: Extract acoustic features from the stimuli used in the
experiment</p></li>
<li><p>3: Test those SVMs using the experimental stimuli data</p>
<ul>
<li>Again classifying “oral” or “nasal”</li>
</ul></li>
<li><p>4: Compare the by-condition confusion results to the
humans</p></li>
</ul>
<hr />
<h3 id="confusion-by-condition">Confusion by Condition</h3>
<p><img src="img/diss_stimml_human_vs_machine_ex_mod.png"></p>
<hr />
<h3 id="humans-vs.-machines-summary">Humans vs. Machines Summary</h3>
<ul>
<li><p>Humans and machines <em>did</em> show similar patterns</p>
<ul>
<li>Modifications that were difficult for humans were difficult for
SVMs</li>
</ul></li>
<li><p>Humans are still more accurate overall</p></li>
</ul>
<hr />
<h3 id="the-svms-didnt-model-the-humans-exactly">The SVMs didn’t model
the humans exactly!</h3>
<ul>
<li><p>SVMs predicted gradient usefulness of the features</p>
<ul>
<li>Humans based their decisions entirely on F1’s Bandwidth</li>
</ul></li>
<li><p>SVMs showed greater accuracy when all features were available</p>
<ul>
<li>Humans weren’t meaningfully affected by the additional three
features</li>
</ul></li>
<li><p>So, SVMs can show relative informativeness of features</p>
<ul>
<li><h2 id="but-they-cant-show-what-humans-actually-do-use">… but they
can’t show what humans actually do use</h2></li>
</ul></li>
</ul>
<h2 id="part-1-conclusion">Part 1: Conclusion</h2>
<ul>
<li><p>The SVM studies very effectively narrowed the field</p></li>
<li><p>The SVM studies and the humans both agreed on the best
feature</p></li>
<li><p>Trained SVMs were able to perform the same experiment, with
similar results</p></li>
<li><p><strong>Modeling human speech perception using machine learning
is helpful!</strong></p></li>
</ul>
<hr />
<h1 id="part-2-machine-learning-and-speech-production">Part 2: Machine
Learning and Speech Production</h1>
<hr />
<h2 id="pause-postures">Pause Postures</h2>
<ul>
<li><p>Articulation during pauses has been studied previously</p>
<ul>
<li>Gick et al. 2005</li>
<li>Wilson and Gick 2013</li>
<li>Ramanarayananan et al. 2013</li>
<li>Usually in the context of “articulatory setting”</li>
</ul></li>
<li><p>Pause postures <em>per se</em> first described by Katsika (2014)
in Greek</p></li>
<li><p>Specific configurations of the articulators at strong prosodic
boundaries</p>
<ul>
<li>Moving articulator(s) to an separate and different target during a
pause</li>
</ul></li>
</ul>
<hr />
<h3 id="pause-postures-in-lip-movement">Pause Postures in Lip
Movement</h3>
<p><img class="r-stretch" src="pausepostures/no_F3_1_s3_biBU_B08_331_LA_clean.png"></p>
<hr />
<h3 id="pause-postures-in-lip-movement-1">Pause Postures in Lip
Movement</h3>
<p><img class="r-stretch" src="pausepostures/yes_F3_1_s6_biBU_B06_237_LA_clean.png"></p>
<hr />
<h3 id="pause-postures-1">Pause postures</h3>
<ul>
<li><p>These postures seem to violate our usual tendency towards economy
of effort</p></li>
<li><p>Do these pause postures occur in English?</p>
<ul>
<li>If so, under what conditions?</li>
</ul></li>
<li><p>Myself, Jelena Krivokapic, Ben Parrell, and Jiseung Kim are
working to find this out.</p></li>
<li><p>But this is very new research</p>
<ul>
<li>So first, we need to know…</li>
</ul></li>
<li><p><strong><em>Can we reproducibly detect, measure, and label these
pause postures?</em></strong></p></li>
</ul>
<hr />
<h3 id="electromagnetic-articulography-ema">Electromagnetic
Articulography (EMA)</h3>
<p>Carstens AG501 System</p>
<p><img class="r-stretch" src="phonmedia/ema_will.jpg"></p>
<hr />
<h3 id="ema-the-basics">EMA: The Basics</h3>
<p>Small wired sensors are glued to the articulators</p>
<p><img class="r-stretch"  src="phonmedia/emasensor.jpg">
<!-- .element: class="fragment" --></p>
<hr />
<h3 id="ema-sensor-placement-tongue">EMA Sensor Placement (Tongue)</h3>
<p><img class="r-stretch" src="phonmedia/ema_will_4.jpg"></p>
<hr />
<h3 id="ema-sensor-placement-lips">EMA Sensor Placement (Lips)</h3>
<p><img class="r-stretch"  src="phonmedia/ema_lips.jpg"></p>
<table style="width:6%;">
<colgroup>
<col style="width: 5%" />
</colgroup>
<tbody>
<tr class="odd">
<td>### EMA: The Basics</td>
</tr>
<tr class="even">
<td>- Machine pulses different magnetic fields from different coils
oriented around the head</td>
</tr>
<tr class="odd">
<td>- Sensors get strength of each of these fields via electromagnetic
induction</td>
</tr>
<tr class="even">
<td>- The relative strengths of each field are used to calculate the
sensor’s position in space</td>
</tr>
<tr class="odd">
<td>- Accurate to 0.1mm, at up to 1250 measurements per second</td>
</tr>
</tbody>
</table>
<h3 id="ema-data">EMA Data</h3>
<p><img class="r-stretch" src="phonmedia/M4_14_mview.jpg"></p>
<hr />
<h3 id="the-data">The Data</h3>
<ul>
<li>EMA Data collected from 7 speakers at USC (by Jelena Krivokapic and
Ben Parrell)
<ul>
<li>1891 curves total</li>
</ul></li>
<li>Sentences like…
<ul>
<li>“I don’t know about MIma. Mini does, though.”</li>
<li>“Does she know about MIma? Mini discovered MIma!”</li>
<li>“I know about biBU, mini, and the rest of the gang.”</li>
</ul></li>
<li>Analyzed at University of Michigan and UC San Diego</li>
</ul>
<hr />
<h3 id="the-main-problem">The main problem</h3>
<ul>
<li><p>Pause postures are newly discovered</p></li>
<li><p>They’re based on unusual patterns of curvature</p></li>
<li><p>Nobody has directly measured these before</p></li>
<li><p><strong>Can we reliably identify and measure pause postures at
all?</strong></p></li>
</ul>
<hr />
<h3 id="why-measuring-them-might-be-hard">Why measuring them might be
hard</h3>
<ul>
<li><p>These articulatory data are incredibly rich</p></li>
<li><p>Pause postures are seen in changes <em>over time</em></p></li>
<li><p>Speakers differ in articulation and absolute positions</p></li>
<li><p>Relatively fewer tokens seem to show pause postures at all</p>
<ul>
<li>Only 30% of current data seem to contain PPs</li>
</ul></li>
<li><p>There are clear “Yes” and “No” tokens, but also “Maybe”</p></li>
</ul>
<hr />
<h3 id="some-pause-postures-are-quite-clear">Some pause postures are
quite clear</h3>
<p><img class="r-stretch"  src="pausepostures/yes_F3_1_s3_MIma_B05_212_LA.png"></p>
<hr />
<h3 id="some-pause-postures-are-quite-clear-1">Some pause postures are
quite clear</h3>
<p><img class="r-stretch"  src="pausepostures/yes_F3_1_s6_biBU_B06_237_LA.png"></p>
<hr />
<h3 id="some-clearly-lack-pause-postures">Some clearly lack Pause
Postures</h3>
<p><img class="r-stretch"  src="pausepostures/no_F3_1_s3_biBU_B08_331_LA.png"></p>
<table style="width:6%;">
<colgroup>
<col style="width: 5%" />
</colgroup>
<thead>
<tr class="header">
<th>### Some clearly lack Pause Postures</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>### Some tokens are less certain</td>
</tr>
<tr class="even">
<td><img class="r-stretch"  src="pausepostures/maybe_F4_1_s12_miMA_B01_35_LA.png"></td>
</tr>
</tbody>
</table>
<h3 id="some-tokens-are-less-certain">Some tokens are less certain</h3>
<p><img class="r-stretch"  src="pausepostures/maybe_M4_1_s1_miMA_B08_317_LA.png"></p>
<hr />
<h3 id="our-questions">Our questions</h3>
<ul>
<li><p>Are there measureable, reproducible patterns associated with
pause postures in these data?</p></li>
<li><p>Can we empirically capture the gradience and uncertainty of these
pause postures?</p></li>
<li><p>Can we identify pause postures without human
intervention?</p></li>
</ul>
<hr />
<h2 id="methods-1">Methods</h2>
<ul>
<li><p>Human annotator marks pause boundaries</p>
<ul>
<li>End of prior gesture to start of following C</li>
</ul></li>
<li><p>Human annotator classifies each as “Yes” or “No” Pause Posture
based on Lip Aperture</p>
<ul>
<li>With secondary marking as “Yes”, “Maybe”, “Unlikely”, and “No”</li>
</ul></li>
<li><p>Train SVM Classifiers to find PPs using the annotator’s Yes/No
judgement</p></li>
<li><p>Test on new data to gauge accuracy</p></li>
</ul>
<hr />
<h3 id="machine-learning-can-address-our-questions">Machine Learning can
address our questions</h3>
<ul>
<li><p>Is the pattern measureable?</p>
<ul>
<li><strong>If the SVM can find PPs based on mathematical features, then
YES!</strong></li>
</ul></li>
<li><p>Can we capture the gradience of these pause posture?</p>
<ul>
<li><strong>If the SVM can differentiate “Yes”, “Maybe”, “Unlikely” and
“No” tokens, then YES!</strong></li>
</ul></li>
<li><p>Can we identify pause postures without human intervention?</p>
<ul>
<li><strong>If the SVM shows high agreement with the human, then
YES!</strong></li>
</ul></li>
</ul>
<hr />
<h3 id="choosing-the-features">Choosing the features</h3>
<ul>
<li>Simple features like means or straight lines don’t work
<ul>
<li>The data are curved, and scales differ for each speaker</li>
</ul></li>
<li>Even more complex features like “area under the curve” or “deviation
from straight line” fail
<ul>
<li>Lots of tokens deviate from a straight line, but this is more
specific</li>
</ul></li>
<li>Instead, we’re interested in specifically timed changes in lip
trajectory
<ul>
<li>These specific movements represent not just noise or interpolation,
but gesture</li>
</ul></li>
<li>… but how do we describe specific types of curvature?</li>
</ul>
<hr />
<h2 id="functional-principal-component-analysis">Functional Principal
Component Analysis</h2>
<ul>
<li><p>“Feed all the curves in, then observe the dominant
<em>patterns</em> of variation”</p>
<ul>
<li>Each pattern is called a “component”</li>
</ul></li>
<li><p>Each component is <em>orthogonal</em> to the others, representing
an indepenent type of difference</p></li>
<li><p>Each pause is given a score for each component</p>
<p><!-- - "How strongly is this component of the overall difference present in this curve?"--></p></li>
<li><p>We’ll do two Principal Component Analyses</p></li>
</ul>
<hr />
<h3 id="pca-1-trajectory-modeling">PCA #1: Trajectory Modeling</h3>
<ul>
<li>Model the trajectory itself</li>
</ul>
<p><img class="r-stretch"  src="pausepostures/yes_M4_1_s8_MIma_B01_14_LA.png"></p>
<hr />
<h3 id="pca-2-difference-modeling">PCA #2: Difference Modeling</h3>
<ul>
<li>Model the difference between the trajectory and the
interpolation</li>
</ul>
<p><img class="r-stretch"  src="pausepostures/yes_M4_1_s8_MIma_B01_14_LA.png"></p>
<hr />
<h3 id="components-for-the-trajectory-pca">Components for the Trajectory
PCA</h3>
<p><img class="r-stretch"  src="pausepostures/RawPCA_LA_ed.png"></p>
<hr />
<h3 id="components-for-the-difference-pca">Components for the Difference
PCA</h3>
<p><img class="r-stretch"  src="pausepostures/DiffPC_LA_ed.png"></p>
<hr />
<h3 id="the-pca-scores">The PCA Scores</h3>
<ul>
<li><p>Each token has 12 scores, six from trajectory models, six from
difference models</p></li>
<li><p>High or low scores represent the presence of a specific shape and
timing of curvature in a given pause</p></li>
</ul>
<hr />
<h2 id="the-model">The Model</h2>
<ul>
<li><p>Randomly split the data into 80% for training, 20% for
testing</p>
<ul>
<li>This is very conservative, but still gives a reasonable estimation
of accuracy</li>
</ul></li>
<li><p>The SVM is trained using these curvature-based measures</p>
<ul>
<li>Twelve PC scores per pause, along with the “Yes” or “No” Pause
Posture judgement</li>
</ul></li>
<li><p>Class weights are adjusted to compensate for the rarity of pause
postures</p>
<ul>
<li>“Give more weight to the pause posture tokens, as they’re
important”</li>
</ul></li>
<li><p>Returns classification accuracy and Cohen’s Kappa to measure
human-to-computer agreement</p>
<ul>
<li>Kappa measures agreement with the human <em>accounting for
chance</em></li>
</ul></li>
</ul>
<hr />
<h3 id="the-worlds-dumbest-model">The World’s Dumbest Model</h3>
<p>“Ignore the data, guess that every item is not a PP.”</p>
<center>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg th{font-family:Arial, sans-serif;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg .tg-d78e{background-color:#9aff99;text-align:center;vertical-align:top}
.tg .tg-cmwg{background-color:#ffccc9;text-align:center;vertical-align:top}
.tg .tg-baqh{text-align:center;vertical-align:top}
.tg .tg-amwm{font-weight:bold;text-align:center;vertical-align:top}
.tg .tg-9hbo{font-weight:bold;vertical-align:top}
</style>
<table class="tg">
<tr>
<th class="tg-baqh">
</th>
<th class="tg-amwm">
ACTUAL NO
</th>
<th class="tg-amwm">
ACTUAL YES
</th>
</tr>
<tr>
<td class="tg-amwm">
PREDICTED NO
</td>
<td class="tg-d78e">
268
</td>
<td class="tg-cmwg">
107
</td>
</tr>
<tr>
<td class="tg-amwm">
PREDICTED YES
</td>
<td class="tg-cmwg">
0
</td>
<td class="tg-d78e">
0
</td>
</tr>
<tr>
<td class="tg-amwm">
Prediction Accuracy
</td>
<td class="tg-baqh" colspan="2">
71.4%
</td>
</tr>
<tr>
<td class="tg-9hbo">
Cohen’s Kappa
</td>
<td class="tg-baqh" colspan="2">
0
</td>
</tr>
</table>
</center>
<hr />
<h3 id="pca-1-only-model-trajectory-modeling">PCA #1 ONLY Model:
Trajectory Modeling</h3>
<p>Only using the 6 PCs per item that are based on actual
trajectories</p>
<center>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg th{font-family:Arial, sans-serif;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg .tg-d78e{background-color:#9aff99;text-align:center;vertical-align:top}
.tg .tg-cmwg{background-color:#ffccc9;text-align:center;vertical-align:top}
.tg .tg-baqh{text-align:center;vertical-align:top}
.tg .tg-amwm{font-weight:bold;text-align:center;vertical-align:top}
.tg .tg-9hbo{font-weight:bold;vertical-align:top}
</style>
<table class="tg">
<tr>
<th class="tg-baqh">
</th>
<th class="tg-amwm">
ACTUAL NO
</th>
<th class="tg-amwm">
ACTUAL YES
</th>
</tr>
<tr>
<td class="tg-amwm">
PREDICTED NO
</td>
<td class="tg-d78e">
254
</td>
<td class="tg-cmwg">
8
</td>
</tr>
<tr>
<td class="tg-amwm">
PREDICTED YES
</td>
<td class="tg-cmwg">
14
</td>
<td class="tg-d78e">
99
</td>
</tr>
<tr>
<td class="tg-amwm">
Prediction Accuracy
</td>
<td class="tg-baqh" colspan="2">
94.1%
</td>
</tr>
<tr>
<td class="tg-9hbo">
Cohen’s Kappa
</td>
<td class="tg-baqh" colspan="2">
0.85
</td>
</tr>
</table>
</center>
<p><!-- .element: class="fragment" --></p>
<hr />
<h3 id="pca-1-and-2-12-feature-model">PCA #1 AND #2: 12 Feature
Model</h3>
<p>6 PCs per item based on actual trajectories, 6 PCs based on
differences</p>
<center>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg .tg-d78e{background-color:#9aff99;text-align:center;vertical-align:top}
.tg .tg-cmwg{background-color:#ffccc9;text-align:center;vertical-align:top}
.tg .tg-baqh{text-align:center;vertical-align:top}
.tg .tg-amwm{font-weight:bold;text-align:center;vertical-align:top}
.tg .tg-9hbo{font-weight:bold;vertical-align:top}
</style>
<table class="tg">
<tr>
<th class="tg-baqh">
</th>
<th class="tg-amwm">
ACTUAL NO
</th>
<th class="tg-amwm">
ACTUAL YES
</th>
</tr>
<tr>
<td class="tg-amwm">
PREDICTED NO
</td>
<td class="tg-d78e">
258
</td>
<td class="tg-cmwg">
7
</td>
</tr>
<tr>
<td class="tg-amwm">
PREDICTED YES
</td>
<td class="tg-cmwg">
10
</td>
<td class="tg-d78e">
100
</td>
</tr>
<tr>
<td class="tg-amwm">
Prediction Accuracy
</td>
<td class="tg-baqh" colspan="2">
95.4%
</td>
</tr>
<tr>
<td class="tg-9hbo">
Cohen’s Kappa
</td>
<td class="tg-baqh" colspan="2">
0.889
</td>
</tr>
</table>
</center>
<p><!-- .element: class="fragment" --></p>
<hr />
<h2 id="lets-return-to-our-questions">Let’s return to our questions</h2>
<hr />
<h3 id="are-there-measureable-reproducible-patterns-in-these-data">Are
there measureable, reproducible patterns in these data?</h3>
<ul>
<li><p>The SVM finds the same patterns as the annotator, with good
accuracy, using only curvature measurements</p></li>
<li><p>Pause postures are most readily identifiable using PC2 and PC3
from PCA #1 (Trajectory), and PC1 from PCA #2 (Difference)</p>
<ul>
<li>This wouldn’t be possible to check with opaque ML algorithms!</li>
</ul></li>
<li><p>Using those three features <strong>alone</strong> offers 92%
accuracy, with a kappa of 0.832</p></li>
</ul>
<hr />
<p><img class="r-stretch" src="pausepostures/RawPCA_LA_ed.png">
<img class="r-stretch" src="pausepostures/DiffPC_LA_ed.png"></p>
<hr />
<h3
id="are-there-measureable-reproducible-patterns-associated-with-pause-postures-in-these-data">Are
there measureable, reproducible patterns associated with pause postures
in these data?</h3>
<p>The SVM finds the same patterns as the annotator, with good accuracy,
using only curvature measurements</p>
<p>Pause postures are readily identifiable using PC2 and PC3 from PCA #1
(Trajectory), and PC1 from PCA #2 (Difference)</p>
<p>Using those three features <strong>alone</strong> offers 92%
accuracy, with a kappa of 0.832</p>
<ul>
<li><h2 id="yes">Yes!</h2></li>
</ul>
<hr />
<h3 id="can-we-empirically-capture-degree-of-pause-posture">Can we
empirically capture “degree” of pause posture?</h3>
<ul>
<li><p>We had “Yes”“,”Maybe”, “Unlikely”, “No” judgements too</p>
<ul>
<li>The SVM only saw “Yes” and “No”</li>
</ul></li>
<li><p>The SVM returns a “probability” judgement for each item</p></li>
<li><p>Let’s see how that aligns with our human’s judgements</p></li>
</ul>
<hr />
<p><img class="r-stretch" src="pausepostures/maybe_vs_svm.png"></p>
<hr />
<h3 id="can-we-empirically-capture-degree-of-pause-posture-1">Can we
empirically capture “degree” of pause posture?</h3>
<ul>
<li><p>SVM probability judgements reflect human judgements
nicely</p></li>
<li><p>So, the gradience is now directly measurable</p></li>
<li><h2 id="yes-1">Yes!</h2></li>
</ul>
<hr />
<h3 id="can-we-identify-pause-postures-without-human-intervention">Can
we identify pause postures without human intervention?</h3>
<ul>
<li><p>Our best-performing model finds PPs in novel data with 96.5%
accuracy</p></li>
<li><p>Out of 375 unknown items, it made 17 mistakes</p>
<ul>
<li><p>It’s slightly more prone to false positives</p></li>
<li><p>This could be viewed as a feature or a bug</p></li>
</ul></li>
<li><p>This is not ideal, but respectable</p>
<ul>
<li>… and human-to-machine agreement (0.889) is an acceptable IAA in the
field</li>
</ul></li>
<li><p>So, depending on your needs…</p></li>
<li><h2 id="maybe">Maybe!</h2></li>
</ul>
<hr />
<h3 id="so-pause-postures-are-empirically-findable-in-the-data">So,
Pause Postures are empirically findable in the data</h3>
<ul>
<li><p>We can now characterize their nature more clearly</p>
<ul>
<li>… and we can measure their degree reliably</li>
</ul></li>
<li><p>Our major methodological concern is cleared up</p></li>
<li><p><strong>Allowing us to actually do the work!</strong></p></li>
</ul>
<hr />
<h3 id="turns-out-pause-postures-do-exist-in-american-english">Turns out
Pause Postures <em>do</em> exist in American English</h3>
<ul>
<li><p>… and they appear to be related to upcoming sentence
planning!</p></li>
<li><p>(c.f. our forthcoming paper in the Journal of
Phonetics!)</p></li>
</ul>
<hr />
<h2 id="part-2-conclusion">Part 2: Conclusion</h2>
<ul>
<li><p>Machine learning was able to very effectively model annotator
judgements</p></li>
<li><p>These sorts of curve-based phenomena can be effectively captured
with functional PCA</p></li>
<li><p>SVM probability judgements can address the gradiency of the
task</p></li>
<li><p>Automatic annotation of pause postures is imperfect, but
promising</p></li>
</ul>
<hr />
<h1 id="final-conclusions">Final Conclusions</h1>
<hr />
<h3
id="transparent-machine-learning-is-a-valuable-tool-in-linguistic-research">Transparent
Machine learning is a valuable tool in Linguistic research</h3>
<ul>
<li><p>Great for identifying subtle differences among classes or
items</p>
<ul>
<li>Detecting multi-feature differences in vowel nasality, or
differences in curvature which seem like pause postures</li>
</ul></li>
<li><p>Great for working with curves and nuances, where conventional
statistical models often struggle</p>
<ul>
<li>Using fPCA with machine learning helped identify a very nuanced
pattern of curvature</li>
</ul></li>
</ul>
<hr />
<h3
id="transparent-machine-learning-is-a-valuable-tool-in-linguistic-research-continued">Transparent
Machine learning is a valuable tool in Linguistic research
(Continued)</h3>
<ul>
<li><p>Great for evaluating feature meaningfulness, particularly with
large sets of possible features</p>
<ul>
<li>Narrowing the field of features in both cases</li>
</ul></li>
<li><p>Great for roughly simulating human decisions, perception, or
annotation</p>
<ul>
<li><p>Predicting human listeners’ perceptions (in Part 1)</p></li>
<li><p>Predicting human annotators’ judgements (in Part 2)</p></li>
</ul></li>
</ul>
<hr />
<h3 id="there-is-much-left-to-do-with-machine-learning-and-speech">There
is much left to do with Machine Learning and Speech</h3>
<ul>
<li><p>Using Machine Learning to explore the relationship between
articulation, acoustics, and speaker variation in nasality</p></li>
<li><p>Determining the most informative cues for voicing type perception
and measurement in the presence of other phenomena</p></li>
<li><p>Studying articulation and acoustics with regard to a feature’s
functional and informational load</p></li>
</ul>
<hr />
<h2
id="any-hypothesis-about-human-language-needs-to-be-tested-with-human-speakers">Any
hypothesis about human language needs to be tested with human
speakers</h2>
<ul>
<li>… but sometimes, it’s a good idea to trust the machines!</li>
</ul>
<hr />
<h3 id="just-be-careful">(Just be careful)</h3>
<p><img src="img/hal_eye.jpg"> <!-- .element: class="fragment" --></p>
<hr />
<h2 id="acknowledgements">Acknowledgements</h2>
<p>My collaborators in Part 2, Jelena Krivokapic, Ben Parrell, and
Jiseung Kim</p>
<p>University of Michigan Phonetics Laboratory (and Phondi group)</p>
<p>NSF Grant BCS-1348150 to Patrice Speeter Beddor and Andries W.
Coetzee</p>
<p>NSF Grant 1551513 to Goldstein, Katsika, Krivokapic, Nam,
Saltzman</p>
<p>NIH Grant DC003172 to Dani Byrd, and DC002717 to Doug Whalen</p>
<p>The University of Colorado at Boulder and Rebecca Scarborough</p>
<p>The speakers and listeners who participated in the studies</p>
<p>The great many electrons inconvenienced in the process of building
these SVMs</p>
<hr />
<h3 id="references">References</h3>
<p><small></p>
<p>[Gick et al., 2005] Gick, B., Wilson, I., Koch, K., and Cook, C.
(2005). Language-specific articulatory settings: Evidence from
inter-utterance rest position. Phonetica, 61(4):220–233.</p>
<p>[Hawkins and Stevens, 1985] Hawkins, S. and Stevens, K. N. (1985).
Acoustic and perceptual correlates of the non-nasal–nasal distinction
for vowels. The Journal of the Acoustical Society of America,
77(4):1560– 1575.</p>
<p>[Katsika et al., 2014] Katsika, A., Krivokapi ́c, J., Mooshammer, C.,
Tiede, M., and Goldstein, L. (2014). The coordination of boundary tones
and its interaction with prominence. Journal of phonetics, 44:62–82.</p>
<p>[Ramanarayanan et al., 2013] Ramanarayanan, V., Goldstein, L., Byrd,
D., and Narayanan, S. S. (2013). An investigation of articulatory
setting using real-time magnetic resonance imaging. The Journal of the
Acoustical Society of America, 134(1):510–519.</p>
<p>[Shaw and Kawahara 2017], Shaw, J., Kawaha, S. (2017) Modelling
articulatory dynamics in the frequency domain. A Poster Presented at
Workshop on Dynamic Modeling in Phonetics and Phonology 2017, Hosted by
the Chicago Linguistic Society.</p>
<p>[Styler, 2015] Styler, W. (2015). On the Acoustical and Perceptual
Features of Vowel Nasality. PhD thesis, University of Colorado at
Boulder.</p>
<p>[Wilson and Gick, 2014] Wilson, I. and Gick, B. (2014). Bilinguals
use language-specific articulatory set- tings. Journal of Speech,
Language, and Hearing Research, 57(2):361–373.</p>
<p></small></p>
<hr />
</body>
</html>
