<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title></title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <style type="text/css">
  /*
   * I add this to html files generated with pandoc.
   * Originally from https://gist.github.com/killercup/5917178
   */

  html {
      overflow-y: scroll;
      -webkit-text-size-adjust: 100%;
      -ms-text-size-adjust: 100%;
  }

  body {
      color: #444;
      font-family: "Source Sans 3", Helvetica-Neue, Helvetica, Sans;
      line-height: 1.5;
      padding: 0.5em;
      margin: auto;
      max-width: 55em;
      background: #fefefe;
  }

  a {
      color: #2171b5;
      text-decoration: underline;
  }

  tr:nth-child(even) {background: #F8F8F8}
  tr:nth-child(odd) {background: #FFF}

  a:visited {
      color: #2171b5;
      text-decoration: none;
  }

  a:focus {
      outline: thin dotted;
  }

  *::-moz-selection {
      background: rgba(255, 255, 0, 0.3);
      color: #000;
  }

  *::selection {
      background: rgba(255, 255, 0, 0.3);
      color: #000;
  }

  a::-moz-selection {
      background: rgba(255, 255, 0, 0.3);
      color: #0645ad;
  }

  a::selection {
      background: rgba(255, 255, 0, 0.3);
      color: #0645ad;
  }

  p {
      margin: 0.75em 0;
  }

  img {
      max-width: 60%;
      max-height:400px;
  }

  video {
      max-width: 60%;
  }


  h1, h2, h3, h4, h5, h6 {
      color: #111;
      line-height: 80%;
      margin-top: 1em;
      margin-bottom: 0.5em;
      font-weight: normal;
  }

  h1, h2, h3, h4, h5, h6 {
      font-weight: bold;
  }

  h1 {
      font-size: 2em;
      line-height: 1.25;
      color:  #084594;

  }

  h1.title {
      margin-top:0.2em;
      font-size: 2em;
      line-height: 1.25;
  }

  h2 {
      font-size: 1.5em;
      line-height: 1.6em;
          color:  #084594;
      padding-bottom: 3px;

  }

  h3 {
      font-size: 1.2em;
      line-height: 1.6em;
  }


  h4 {
      font-size: 1.2em;
      line-height: 1.4em;
  }

  h5 {
      font-size: 1em;
  }

  h6 {
      font-size: 0.9em;
  }

  blockquote {
      color: #666666;
      margin: 0;
      padding-left: 3em;
      border-left: 0.5em #EEE solid;
  }

  hr {
      display: block;
      height: 2px;
      border: 0;
      border-top: 1px solid #aaa;
      border-bottom: 1px solid #eee;
      margin: 0.5em 0;
      padding: 0;
  }

  pre, code, kbd, samp {
      color: #000;
      font-family: monospace, monospace;
      _font-family: 'courier new', monospace;
      font-size: 0.98em;
  }

  pre {
      white-space: pre;
      white-space: pre-wrap;
      word-wrap: break-word;
  }

  .answer {
      color:#CC0033;
      font-style:italic;
  }

  b, strong {
      font-weight: bold;
  }

  dfn {
      font-style: italic;
  }

  ins {
      background: #ff9;
      color: #000;
      text-decoration: none;
  }

  mark {
      background: #ff0;
      color: #000;
      font-style: italic;
      font-weight: bold;
  }

  sub, sup {
      font-size: 75%;
      line-height: 0;
      position: relative;
      vertical-align: baseline;
  }

  sup {
      top: -0.5em;
  }

  sub {
      bottom: -0.25em;
  }

  ul, ol {
      margin: 0.5em 0;
      padding: 0em 0em 0em 1em;
  }

  ul img {
      list-style-type: none;
  }

  li p:last-child {
      margin-bottom: 0;
  }

  hr {
      border-top:none;
      height:0px;
      clear:both;
  }

  ul ul, ol ol {
      margin: .3em 0;
  }

  dl {
      margin-bottom: 1em;
  }

  dt {
      font-weight: bold;
      margin-bottom: .8em;
  }

  dd {
      margin: 0 0 .8em 2em;
  }

  dd:last-child {
      margin-bottom: 0;
  }

  img {
      border: 0;
      -ms-interpolation-mode: bicubic;
      vertical-align: middle;
  }

  figure {
      display: block;
      text-align: center;
      margin: 1em 0;
  }

  figure img {
      border: none;
      margin: 0 auto;
  }

  figcaption {
      font-size: 0.8em;
      font-style: italic;
      margin: 0 0 .8em;
  }

  table {
      margin-bottom: 2em;
      border-bottom: 1px solid #ddd;
      border-right: 1px solid #ddd;
      border-spacing: 0;
      border-collapse: collapse;
  }

  table th {
      padding: .2em 1em;
      background-color: #eee;
      border-top: 1px solid #ddd;
      border-left: 1px solid #ddd;
  }

  table td {
      padding: .2em 1em;
      border-top: 1px solid #ddd;
      border-left: 1px solid #ddd;
      vertical-align: top;
  }

  .author {
      font-size: 1.2em;
      text-align: center;
  }

  @media only screen and (min-width: 480px) {
      body {
  	font-size: 14px;
      }
  }
  @media only screen and (min-width: 768px) {
      body {
  	font-size: 16px;
      }
  }
  @media print {
      * {
  	background: transparent !important;
  	color: black !important;
  	filter: none !important;
  	-ms-filter: none !important;
      }

      body {
  	font-size: 12pt;
  	max-width: 100%;
      }

      a, a:visited {
  	text-decoration: underline;
      }

      hr {
  	height: 1px;
  	border: 0;
  	border-bottom: 1px solid black;
      }

      a[href]:after {
  	content: " (" attr(href) ")";
      }

      abbr[title]:after {
  	content: " (" attr(title) ")";
      }

      .ir a:after, a[href^="javascript:"]:after, a[href^="#"]:after {
  	content: "";
      }

      pre, blockquote {
  	border: 1px solid #999;
  	padding-right: 1em;
  	page-break-inside: avoid;
      }

      tr, img {
  	page-break-inside: avoid;
      }

      img {
  	max-width: 40% !important;
      max-height: 300px !important;
      }

      @page :left {
  	margin: 15mm 20mm 15mm 10mm;
      }

      @page :right {
  	margin: 15mm 10mm 15mm 20mm;
      }

      p, h2, h3 {
  	orphans: 3;
  	widows: 3;
      }

      h2, h3 {
  	page-break-after: avoid;
      }
  }


  ldata {
  	font-size: 0.7em;
  	margin-bottom: 0em;
  	color:#808080;
  	font-style:italic;
  }

  danger {
  	color:#FF0000;
  	font-weight:bold;
  }

  correct {
  	color:#39C900;
  	font-weight:bold;
  }

  clg{
      color:#39C900;
  	font-weight:bold;
  }

  clr{
  	color:#FF0000;
  	font-weight:bold;
  }

  clb{
  	color:#0000CC;
  	font-weight:bold;
  }

  clp{
  	color:#6600FF;
  	font-weight:bold;
  }

  clk{
  	color:#708cef;
  	font-weight:bold;
  }

  clo{
  	color:#CC6600;
  	font-weight:bold;
  }

  sc{
          font-variant: small-caps;
  }

  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="deep-learning-and-speech-data">Deep Learning and Speech
Data</h1>
<h3 id="will-styler---lign-167">Will Styler - LIGN 167</h3>
<p><a href="http://savethevowels.org/talks/deep_learning_speech.html"
class="uri">http://savethevowels.org/talks/deep_learning_speech.html</a></p>
<hr />
<h1 id="why-work-with-speech-at-all">Why work with speech at all?</h1>
<hr />
<h3 id="human-language-is-mostly-speech-based">Human language is mostly
speech-based</h3>
<ul>
<li><p>The vast majority of human languages are spoken</p></li>
<li><p>Many human/computer interactions work better by voice</p></li>
<li><p>We can speak faster and more readily than we can type</p></li>
<li><p><strong>We want our systems to be able to work with spoken
language too!</strong></p></li>
</ul>
<hr />
<h3 id="what-are-common-speech-tasks">What are common speech tasks?</h3>
<ul>
<li><p>Automatic Speech Recognition (ASR)</p>
<ul>
<li>Spoken language to orthography</li>
</ul></li>
<li><p>Speech Synthesis or Text-to-speech (TTS)</p>
<ul>
<li>Orthography to spoken language</li>
</ul></li>
<li><p>Voice or language recognition</p>
<ul>
<li>“I don’t care what they’re saying, but who/what is it?”</li>
</ul></li>
<li><p>Real-time spoken translation</p>
<ul>
<li>“Which NLP problems do you want to have?” “Yes.”</li>
</ul></li>
</ul>
<hr />
<h3 id="these-tasks-are-hard">These tasks are hard</h3>
<ul>
<li><p>ASR and TTS are fraught with thousands of complexities in the
tasks</p></li>
<li><p>The datasets are very expensive to get and store and
annotate</p></li>
<li><p>Speech is <em>amazingly</em> complicated</p></li>
<li><p>… but we want to be good at them, <em>so badly</em></p></li>
</ul>
<hr />
<h3
id="but-theyre-fundamentally-similar-to-many-other-deep-learning-tasks">…
but they’re fundamentally similar to many other deep learning tasks</h3>
<ul>
<li><p>Taking acoustic data as input and classifying it</p>
<ul>
<li>ASR or Voice/Language Recognition</li>
</ul></li>
<li><p>Taking written words as input and generating appropriate spoken
data</p>
<ul>
<li>Text-to-speech</li>
</ul></li>
</ul>
<hr />
<h3 id="deep-learning-for-speech-is-a-rapidly-changing-field">Deep
Learning for speech is a rapidly changing field</h3>
<ul>
<li><p>Most speech work was done with HMMs for a long time</p></li>
<li><p>Now deep neural network approaches are taking over</p></li>
<li><p>Apple is <a
href="https://www.theverge.com/2019/6/3/18650906/siri-new-voice-ios-13-iphone-homepod-neutral-text-to-speech-technology-natural-wwdc-2019">announcing
Neural TTS in keynotes</a>!</p></li>
<li><p>As a result…</p></li>
</ul>
<hr />
<h3 id="getting-into-implementation-is-very-hard">Getting into
implementation is very hard</h3>
<ul>
<li><p>Companies are <em>really</em> guarded about their speech
processing algos</p></li>
<li><p>The state-of-the-art is changing every day</p></li>
<li><p><strong>We’re going to focus on the basic issues involved with
speech classification</strong></p>
<ul>
<li>… rather than diving deep on an algorithm which will be outdated by
the end of the talk</li>
</ul></li>
</ul>
<hr />
<h3 id="todays-plan">Today’s Plan</h3>
<ul>
<li><p>What is the nature of speech?</p></li>
<li><p>How do we discuss and display sound?</p></li>
<li><p>What is the nature of the speech signal?</p></li>
<li><p>How can we turn speech into features?</p></li>
<li><p>What is the training data?</p></li>
</ul>
<hr />
<h1 id="what-is-the-nature-of-speech">What is the nature of speech?</h1>
<hr />
<h3 id="the-speech-process">The Speech Process</h3>
<ul>
<li><p>Flapping bits of meat inside your head while blowing out
air</p></li>
<li><p>This creates vibrations in the air you’re expelling</p></li>
<li><p>The ear picks these up, and inteprets them as speech.</p></li>
<li><p>This process is studied in the Linguistic subfield of
<strong>Phonetics</strong></p></li>
</ul>
<hr />
<h3 id="the-lungs">The Lungs</h3>
<p><img class="r-stretch" src="phonmedia/lungs.jpg"></p>
<hr />
<h3 id="flapping-bits-of-meat-articulation">Flapping bits of meat
(“articulation”)</h3>
<p><img class="r-stretch" src="phonmedia/nasalsagittal.jpg"></p>
<hr />
<h3 id="simplified-a-bit">Simplified a bit…</h3>
<p><img class="r-stretch" src="phonmedia/sagittal_simple.jpg"></p>
<hr />
<h3 id="lets-do-an-experiment">Let’s do an experiment</h3>
<hr />
<blockquote>
<p>The North Wind and the Sun were disputing which was the stronger,
when a traveler came along wrapped in a warm cloak.</p>
</blockquote>
<hr />
<h3 id="speech-is-absolutely-insane">Speech is absolutely insane</h3>
<ul>
<li><p>It’s a series of fluid and overlapping gestures</p></li>
<li><p>It’s amazingly complex</p></li>
<li><p>… and it’s nothing like we think it is</p></li>
</ul>
<hr />
<h3 id="how-do-we-wrap-our-heads-around-it">How do we wrap our heads
around it?</h3>
<ul>
<li><p>First, we break speech into ‘segments’ or ‘phones’</p></li>
<li><p>Then, we figure out how to describe those phones and their
properties</p></li>
<li><p>This lets us <em>transcribe</em> what was said, rather than what
words were said</p></li>
<li><p>But first you need to realize that…</p></li>
</ul>
<hr />
<h2 id="your-writing-system-is-a-trainwreck">Your writing system is a
trainwreck</h2>
<ul>
<li><img class="r-stretch" src="humorimg/trainwreck.png"></li>
</ul>
<hr />
<h3 id="your-writing-system-is-lying-to-you">Your writing system is
lying to you</h3>
<ul>
<li><p>Every minute of every day</p>
<ul>
<li><p>“They thoroughly and roughly wrought the boughs in the borough,
through and through”</p></li>
<li><p>C doesn’t exist</p></li>
<li><p>TH is neither a t nor an h, and represents two different
sounds</p></li>
<li><p>We have 15 vowels</p></li>
</ul></li>
<li><p>… and if you start thinking about letters, you’re going to start
struggling</p></li>
<li><p>Consider your writing system with the same skepticism you would
normally reserve for a guy with a broken bottle walking towards you in a
dark alley.</p></li>
</ul>
<hr />
<h3 id="for-more-on-this-lign-110">For more on this, LIGN 110!</h3>
<hr />
<h3
id="we-use-different-writing-systems-to-capture-the-sounds-being-made">We
use different writing systems to capture the sounds being made</h3>
<ul>
<li><p>The International Phonetic Alphabet was developed by
Linguists</p>
<ul>
<li>ðə ɪntəɹ’næʃɪnəl fə’nɛtɪk ’ælfəbət wʌz də’vɛləpt baj ’lɪŋgwɪsts</li>
</ul></li>
<li><p>ARPABET uses two character combinations to encode the sounds of
<em>English</em></p>
<ul>
<li>AAA R P AX B EH T / Y UW Z IH Z / T UW / K EH R IH K T ER …</li>
</ul></li>
<li><p><strong>Often, TTS and ASR use these alphabets as a
‘go-between’</strong></p>
<ul>
<li>They’re used in resources like <a
href="http://www.speech.cs.cmu.edu/cgi-bin/cmudict">CMUDict</a></li>
</ul></li>
</ul>
<hr />
<h3 id="but-we-can-think-about-speech-as-a-sequence-of-phones">… but we
can think about speech as a sequence of ‘phones’</h3>
<ul>
<li><p>Individual speech sounds</p>
<ul>
<li>A series of articulatory targets…</li>
</ul></li>
<li><p>With hard-to-identify boundaries between them</p>
<ul>
<li>Adjacent sounds affect each other</li>
</ul></li>
<li><p>Which is broadcasted to the world acoustically</p></li>
</ul>
<hr />
<h1 id="what-is-sound">What is Sound?</h1>
<hr />
<p><img class="r-stretch" src="phonmedia/sound_diagram.jpg"></p>
<hr />
<p><img class="r-stretch" src="phonmedia/slinky_wave.jpg"></p>
<hr />
<h3 id="sound-is-compression-and-rarefaction-in-a-medium">Sound is
compression and rarefaction in a medium</h3>
<ul>
<li>Sound needs something to travel in (like air or water)</li>
</ul>
<hr />
<h3 id="yes-your-childhood-is-a-lie">(Yes, your childhood is a lie)</h3>
<p><img class="r-stretch" src="humorimg/star_wars_battle.jpg"></p>
<hr />
<h3
id="thinking-of-sound-as-waves-of-air-compression-is-helpful">Thinking
of sound as waves of air compression is helpful</h3>
<ul>
<li><p>Why does clapping cause a sound, but waving your hand through the
air doesn’t?</p></li>
<li><p>Why are gunshots loud?</p></li>
</ul>
<hr />
<h3 id="were-good-at-hearing-sound">We’re good at hearing sound</h3>
<ul>
<li>… but we need to visualize it</li>
</ul>
<hr />
<h3 id="visualizing-sound">Visualizing Sound</h3>
<ul>
<li><p>Waveforms</p></li>
<li><p>Spectrograms</p></li>
</ul>
<hr />
<h2 id="waveform">Waveform</h2>
<p>A horizontal cut through the wave showing the peaks and troughs over
time</p>
<ul>
<li>The height/strength of the wave is called its “amplitude”</li>
</ul>
<p><img class="r-stretch" src="phonmedia/sound_diagram.jpg"></p>
<hr />
<p><img class="r-stretch" src="phonmedia/noisewaveform.png"><br></p>
<audio controls>
<source src="phonmedia/noise.wav" type="audio/wav">
</audio>
<hr />
<h3 id="lets-look-at-the-sounds-in-this-room-right-now">Let’s look at
the sounds in this room right now</h3>
<hr />
<h3 id="waveforms-are-well-and-good">Waveforms are well and good</h3>
<ul>
<li>… and you can tell a lot from a waveform</li>
</ul>
<hr />
<p><img class="r-stretch" src="phonmedia/waveform_patpadbadspat.jpg"></p>
<hr />
<p><img class="r-stretch" src="phonmedia/noisewaveform.png"></p>
<hr />
<h3 id="but-well-need-better-information-to-process-speech">… but we’ll
need better information to process speech</h3>
<hr />
<h2 id="frequency">Frequency</h2>
<p>The speed with which a wave oscillates</p>
<ul>
<li>Measured in Hertz (Hz), Cycles per second</li>
</ul>
<p><img class="r-stretch" src="phonmedia/sound_diagram.jpg"></p>
<hr />
<h3 id="hz---waveform">100 Hz - Waveform</h3>
<audio controls src="phonmedia/soundw.wav">
</audio>
<p><img class="r-stretch" src="phonmedia/soundw.jpg"></p>
<hr />
<h3 id="hz---waveform-1">200Hz - Waveform</h3>
<audio controls src="phonmedia/200Hz.wav">
</audio>
<p><img class="r-stretch" src="phonmedia/200Hz.jpg"></p>
<hr />
<h3 id="voice-pitch">Voice Pitch</h3>
<ul>
<li><p>Changing the “fundamental frequency” of your voice changes the
perceived “pitch” of your voice</p></li>
<li><p>Higher frequency of vocal fold vibration == “higher
pitch”</p></li>
<li><p><em>Intonation is all about this frequency!</em></p></li>
</ul>
<hr />
<h3 id="frequency-is-important">Frequency is important</h3>
<ul>
<li><p>Different phenomena produce sounds at different
frequencies</p></li>
<li><p>Most things produce sounds with a mix of different frequencies,
each at different amplitudes</p></li>
<li><p>Speech has <em>many</em> components at different
frequencies</p></li>
<li><p>Each of those frequencies has a different power</p></li>
</ul>
<hr />
<h3 id="how-do-we-visualize-this">How do we visualize this?</h3>
<ul>
<li>Spectra only show one ‘moment’ of the signal</li>
</ul>
<hr />
<h3 id="noise---waveform">“Noise” - Waveform</h3>
<p><img width="70%" src="phonmedia/noisewaveform.jpg"><br></p>
<audio controls src="phonmedia/noise.wav">
</audio>
<hr />
<h2 id="spectrogram">Spectrogram</h2>
<p>Displays signal strength by frequency, over time</p>
<hr />
<h3 id="noise---waveform-1">“Noise” - Waveform</h3>
<p><img width="70%" src="phonmedia/noisewaveform.jpg"><br></p>
<audio controls src="phonmedia/noise.wav">
</audio>
<hr />
<h3 id="noise---spectrogram">“Noise” - Spectrogram</h3>
<p><img width="70%" src="phonmedia/noisebbspectrogram.jpg"><br></p>
<audio controls src="phonmedia/noise.wav">
</audio>
<hr />
<h3 id="lets-have-a-bit-of-spectrogram-fun">Let’s have a bit of
spectrogram fun</h3>
<hr />
<h3 id="you-can-have-fun-on-your-own">You can have fun on your own</h3>
<ul>
<li><p>SpectrumView on iOS</p></li>
<li><p>https://musiclab.chromeexperiments.com/Spectrogram/</p></li>
<li><p>Praat (http://praat.org)</p></li>
</ul>
<hr />
<h1 id="fundamentals-of-speech-acoustics">Fundamentals of Speech
Acoustics</h1>
<hr />
<h3 id="voicing">Voicing</h3>
<p><img width="70%" src="phonmedia/noisebbspectrogram.jpg"><br></p>
<audio controls src="phonmedia/noise.wav">
</audio>
<hr />
<h3
id="spectrograms-show-us-many-evenly-spaced-vertical-lines">Spectrograms
show us many evenly-spaced vertical lines</h3>
<ul>
<li><p>These are individual glottal pulses</p></li>
<li><p>Higher pitched voices will have…?</p>
<ul>
<li>More tightly spaced lines!</li>
</ul></li>
</ul>
<hr />
<h3 id="resonances-in-the-mouth">Resonances in the mouth</h3>
<p><img width="70%" src="phonmedia/noisebbspectrogram.jpg"><br></p>
<audio controls src="phonmedia/noise.wav">
</audio>
<hr />
<h3
id="different-vowels-have-different-resonances-in-the-mouth">Different
vowels have different resonances in the mouth</h3>
<ul>
<li><p>Resonances vary depending on the tongue’s position</p>
<ul>
<li>… as well as the size and shape of the talker’s head</li>
</ul></li>
<li><p><em>Different resonances from the same speaker mean different
vowels</em></p></li>
</ul>
<hr />
<p><img class="big" src="phonmedia/vowelformants.gif"></p>
<p><small>Different American English vowels, as spoken by a male
speaker</small></p>
<hr />
<h3 id="different-speakers-have-different-resonances">Different speakers
have different resonances</h3>
<ul>
<li><p>This is a fundamental problem in ASR and speech
perception</p></li>
<li><p>Enough data can help, but this is a <em>major</em> issue</p></li>
</ul>
<hr />
<h3 id="other-speech-sounds-have-their-own-acoustics">Other speech
sounds have their own acoustics</h3>
<hr />
<h3 id="l-r-w-j-act-a-lot-like-vowels">/l r w j/ act a lot like
vowels</h3>
<p><img class="wide" src="phonmedia/sonorant_acoustics.jpg"></p>
<hr />
<h3 id="nasals-sounds-look-like-quiet-vowels">Nasals sounds look like
quiet vowels</h3>
<p><img class="wide" src="phonmedia/nasalcons_acoustics.jpg"></p>
<hr />
<h3 id="fricative-consonants-have-little-black-clouds">Fricative
consonants have little black clouds</h3>
<ul>
<li>… and the cloud is higher frequency as you get closer to the
mouth</li>
</ul>
<p><img class="wide" src="phonmedia/fricative_acoustics.jpg"></p>
<hr />
<h3 id="for-stop-consonants-the-signal-stops">For stop consonants, the
signal… stops</h3>
<p><img class="wide" src="phonmedia/stop_voiceless_acoustics.jpg"></p>
<hr />
<h2
id="patterns-of-frequency-and-amplitude-changes-are-indicative-of-sounds-and-words">Patterns
of frequency and amplitude changes are indicative of sounds and
words</h2>
<hr />
<h3 id="cats">Cats</h3>
<p><img class="wide" src="phonmedia/animals/cats.jpg"></p>
<hr />
<h3 id="owls">Owls</h3>
<p><img class="wide" src="phonmedia/animals/owls.jpg"></p>
<hr />
<h3 id="chickadees">Chickadees</h3>
<p><img class="wide" src="phonmedia/animals/chickadees.jpg"></p>
<hr />
<h3 id="koalas">Koalas</h3>
<p><img class="wide" src="phonmedia/animals/koalas.jpg"></p>
<hr />
<h3 id="sparrows">Sparrows</h3>
<p><img class="wide" src="phonmedia/animals/sparrows.jpg"></p>
<hr />
<h3 id="there-is-often-no-one-to-one-mapping">There is often no
one-to-one mapping</h3>
<ul>
<li><p>The expression of a given gesture can have many acoustic
consequences</p></li>
<li><p>Different speakers have different realizations of each
sound</p></li>
<li><p>Different phones sound different in different contexts</p></li>
<li><p>… but it all has to happen from this signal</p></li>
</ul>
<hr />
<h3
id="representing-sound-as-frequency-power-and-time-is-the-basis-of-speech-technology">Representing
sound as frequency, power and time is the basis of speech
technology</h3>
<ul>
<li><p>If we don’t know what words sound like, we can’t teach computers
what they sound like</p></li>
<li><p>Similar patterns are easy to confuse for humans and
computers</p></li>
<li><p>This lets us understand a bit more about how speech technology
might work</p></li>
</ul>
<hr />
<p>… but first, we need to ask an important question</p>
<hr />
<h1 id="how-do-we-turn-speech-sounds-into-features">How do we turn
speech sounds into features?</h1>
<hr />
<h3 id="weve-got-a-fundamental-problem-to-start">We’ve got a fundamental
problem, to start</h3>
<hr />
<h3 id="computers-dont-do-waves">Computers don’t do waves</h3>
<p><img class="r-stretch" src="phonmedia/sampling_raw.jpg"></p>
<p>010001110010101000100101101010101010</p>
<hr />
<h3 id="sound-is-analog-computers-are-digital">Sound is analog,
computers are digital</h3>
<ul>
<li>How do we deal with that?</li>
</ul>
<hr />
<h3 id="quantization-sampling">Quantization (‘Sampling’)</h3>
<p><img class="wide" src="phonmedia/sampling_wave.jpg"></p>
<hr />
<h3 id="quantization-sampling-1">Quantization (‘Sampling’)</h3>
<p><img class="wide" src="phonmedia/sampling_quantized.jpg"></p>
<hr />
<h3 id="quantization-sampling-2">Quantization (‘Sampling’)</h3>
<p><img class="wide" src="phonmedia/sampling_measures.jpg"></p>
<hr />
<h3 id="analog-to-digital-conversion">Analog-to-digital conversion</h3>
<ul>
<li><p>Sample the wave many times per second</p></li>
<li><p>Record the amplitude at each sample</p></li>
<li><p>The resulting series of measurements will faithfully capture the
signal</p></li>
</ul>
<hr />
<h3 id="relevant-parameters">Relevant Parameters</h3>
<ul>
<li><p>The <strong>Bit Depth</strong> describes how many bits of
information encode amplitude</p>
<ul>
<li>16 bit audio is the norm</li>
</ul></li>
<li><p>The <strong>Sampling Rate</strong> describes how many samples per
second we take</p>
<ul>
<li>44,100 Hz is the norm, and captures everything you need for
speech</li>
</ul></li>
</ul>
<hr />
<h3 id="ad-conversion-now-yields-a-signal-that-the-computer-can-read">AD
Conversion now yields a signal that the computer can read</h3>
<ul>
<li>… but what are the features?</li>
</ul>
<hr />
<h3 id="putting-in-the-waveform-itself-is-a-possibility">Putting in the
waveform itself is a possibility</h3>
<ul>
<li><p>It’s cheap and easy</p></li>
<li><p><a
href="https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/">Wave2Vec</a>
is showing amazing results doing just this!</p>
<ul>
<li>This uses transformers to work directly on the waveform and identify
‘latent speech units’</li>
</ul></li>
<li><p>This is a very, very tantalizing possibility</p></li>
</ul>
<hr />
<h3 id="but-we-might-want-more-information">… but we might want more
information!</h3>
<ul>
<li><p>Important parts of the signal live only in frequency band
info</p></li>
<li><p>Many approaches try to give it all the information we
can</p></li>
<li><p>Not the same features that linguists usually use</p></li>
</ul>
<hr />
<h3 id="we-dont-need-transparent-or-parsimonious-features">We don’t need
transparent or parsimonious features</h3>
<ul>
<li><p>Things like vowel features and pitch and other details are a pain
to extract</p></li>
<li><p>We’re plugging it into a black box</p></li>
<li><p>We’re happy to plug in hundreds of features, if need be</p></li>
<li><p>We’d just as soon turn that sound into a boring matrix</p></li>
</ul>
<hr />
<h3 id="lets-get-that-algorithm-a-matrix">Let’s get that algorithm a
Matrix</h3>
<ul>
<li>Algorithms love Matrices</li>
</ul>
<hr />
<h2 id="mel-frequency-cepstral-coefficients-mfccs">Mel-Frequency
Cepstral Coefficients (MFCCs)</h2>
<hr />
<h3 id="were-not-going-deep-here">We’re not going deep here</h3>
<ul>
<li><p>This is a lot of signal processing</p></li>
<li><p>We’re going to teach the idea, not the practice</p></li>
</ul>
<hr />
<h3 id="mfccs">MFCCs</h3>
<p><img class="wide" src="phonmedia/mfcc.jpg"></p>
<hr />
<h3 id="mfcc-process">MFCC Process</h3>
<ul>
<li><p>1: Create a spectrogram (effectively)</p></li>
<li><p>2: Extract the most useful bands for speech (in Mels)</p></li>
<li><p>3: Look at the frequencies of this banded signal (repeating the
Fourier Transform process)</p></li>
<li><p>4: Simplify this into a smaller number of coefficients using
Discrete Cosine Transform (DCT)</p>
<ul>
<li>Usually 12 or 13</li>
</ul></li>
</ul>
<hr />
<h3 id="mfcc-input">MFCC Input</h3>
<p><img class="r-stretch" src="phonmedia/noisewaveform.jpg"></p>
<hr />
<h3 id="mfcc-output">MFCC Output</h3>
<p><img class="wide" src="phonmedia/noise_mfcc.jpg"></p>
<hr />
<h3 id="so-the-sound-becomes-a-matrix-of-features">So, the sound becomes
a matrix of features</h3>
<ul>
<li><p>Many columns (representing time during the signal)</p></li>
<li><p>N columns (usually 13) with coefficients which tell us the
spectral shape</p></li>
<li><p>It’s black-boxy, but we don’t care.</p></li>
<li><p>We’ve created a Matrix</p></li>
</ul>
<hr />
<p><img class="r-stretch" src="humorimg/whoa_neo.jpg"></p>
<hr />
<h3 id="now-weve-got-a-matrix-representing-the-sound">Now we’ve got a
matrix representing the sound</h3>
<ul>
<li><p>MFCCs captures frequency information, according to our perceptual
needs</p></li>
<li><p>Wav2Vec (and equivalents) go straight to vectors</p></li>
</ul>
<hr />
<h3 id="its-neural-network-time">It’s Neural Network time!</h3>
<p><img class="r-stretch" src="img/neuralnetwork.jpg"></p>
<hr />
<h1 id="what-is-the-learning-task-like-for-asr-and-tts">What is the
learning task like for ASR and TTS?</h1>
<hr />
<p>First, one major question…</p>
<ul>
<li><h3 id="what-units-of-speech-are-we-working-with">What units of
speech are we working with?</h3></li>
</ul>
<hr />
<h3 id="whats-the-desired-data-labeling">What’s the desired data
labeling?</h3>
<ul>
<li><p>We need to give the NN labeled data</p></li>
<li><p>[Chunk of Sound] == [Labeled Linguistic Info]</p>
<ul>
<li>(x Many many many many tokens)</li>
</ul></li>
<li><p>What level do we want to recognize and generate at?</p></li>
</ul>
<hr />
<h3 id="possible-levels-of-labeling">Possible levels of labeling</h3>
<ul>
<li><p>Sentences?</p></li>
<li><p>Words?</p></li>
<li><p>Phones?</p></li>
<li><p>Diphones?</p></li>
</ul>
<hr />
<h3 id="sentences">Sentences</h3>
<ul>
<li>Why are sentences a bad idea?</li>
</ul>
<hr />
<h3 id="words">Words</h3>
<p><img class="r-stretch" src="phonmedia/noisebbspectrogram.jpg"></p>
<p>“Noise”</p>
<hr />
<h3 id="what-are-the-pros-and-cons-of-words">What are the pros and cons
of words?</h3>
<hr />
<h3 id="phones">Phones</h3>
<p><img src="phonmedia/noise_phones.jpg"></p>
<hr />
<h3 id="diphones">Diphones</h3>
<p><img src="phonmedia/noise_diphones.jpg"></p>
<hr />
<h3 id="what-are-the-pros-and-cons-of-phones-and-diphones">What are the
pros and cons of phones and diphones?</h3>
<hr />
<h3 id="in-practice-many-systems-use-diphones">In practice, many systems
use diphones</h3>
<ul>
<li><p><a href="https://cmusphinx.github.io/">CMU’s Sphynx
does</a></p></li>
<li><p>As do many others</p></li>
<li><p>Triphones are often a possibility</p></li>
<li><p>Some go straight to entire words</p></li>
<li><p>Speech recognition systems are often kept secret</p></li>
</ul>
<hr />
<h3 id="so-we-can-now-train-a-system">So, we can now train a system</h3>
<ul>
<li><p>Capture sounds and annotate them as diphones</p></li>
<li><p>Vectorize them and feed them into a neural network as training
data</p></li>
<li><p>We can do speech recognition, text-to-speech, and more!</p></li>
</ul>
<hr />
<h3 id="using-neural-networks-for-asr">Using Neural Networks for
ASR</h3>
<ul>
<li><p>Feed the vectorized sound data in and get the most likely diphone
sequence back</p>
<ul>
<li>… or go straight to words, if you feel dangerous!</li>
</ul></li>
</ul>
<hr />
<h3 id="why-is-asr-hard">Why is ASR hard?</h3>
<ul>
<li><p>ASR requires good dictionaries</p>
<ul>
<li>“Bashira yeeted the Mel Frequency Cepstral Coefficients into the
RNN”</li>
</ul></li>
<li><p>ASR requires some context awareness</p>
<ul>
<li>“Robb took a wok from the Chinese restaurant”</li>
</ul></li>
<li><p>Dialect is always a thing</p>
<ul>
<li>“English” is a convenient lie</li>
</ul></li>
<li><p>… and 99 other problems</p></li>
</ul>
<hr />
<h3 id="using-neural-networks-for-tts">Using Neural Networks for
TTS</h3>
<ul>
<li><p>Feed the diphone sequence in, get back a likely acoustic
signal</p>
<ul>
<li>This will generate a voice which matches (roughly) the input
training voice</li>
</ul></li>
<li><p>Style transfer is possible too!</p>
<ul>
<li><p>Training the model on a generic voice</p></li>
<li><p>Then learning the variation associated with another as a style
embedding</p></li>
<li><p>Then applying the variation to the model</p></li>
</ul></li>
</ul>
<hr />
<h3 id="neural-network-text-to-speech-style-transfer-examples">Neural
Network Text-to-Speech Style Transfer Examples</h3>
<audio controls src="comp/tts_squidward_ling.wav">
</audio>
<audio controls src="comp/tts_arnie_ling.wav">
</audio>
<audio controls src="comp/tts_snape_ling.wav">
</audio>
<audio controls src="comp/tts_clarkson_ling.wav">
</audio>
<audio controls src="comp/tts_gilbert_ling.wav">
</audio>
<audio controls src="comp/tts_optimus_ling.wav">
</audio>
<audio controls src="comp/tts_mario_ling.wav">
</audio>
<hr />
<h3 id="text-to-speech-is-hard">Text-to-Speech is hard!</h3>
<ul>
<li><p>Text-to-speech requires you to understand how humans talk</p>
<ul>
<li>“The NSA and NASA printed 1200 t-shirts for area code 303”</li>
</ul></li>
<li><p>… and the prosody is really hard</p>
<ul>
<li>“Let’s eat, Grandpa”</li>
</ul></li>
<li><p>… and 99 other problems</p></li>
</ul>
<hr />
<h3
id="we-talk-a-lot-more-about-the-linguistic-difficulties-with-these-tasks-in-lign-6-language-and-computers">We
talk a lot more about the linguistic difficulties with these tasks in
LIGN 6 ‘Language and Computers’</h3>
<p><img class="r-stretch" src="img/plug.jpg"></p>
<hr />
<h3
id="and-well-talk-a-lot-more-about-processing-speech-outside-of-neural-networks-in-lign-168-computational-speech-processing">…
and we’ll talk a lot more about processing speech outside of Neural
Networks in LIGN 168 ‘Computational Speech Processing’</h3>
<p><img class="r-stretch" src="img/plug.jpg"></p>
<hr />
<h3 id="wrapping-up">Wrapping up</h3>
<ul>
<li><p>Speech is movement of the articulators in the airstream</p></li>
<li><p>This creates sounds which vary in frequency and amplitude over
time</p></li>
<li><p>This signal can be analyzed as a matrix of opaque cepstral
features</p></li>
<li><p>… and then fed into a neural network with linguistic
annotations</p></li>
<li><p>To generate and classify human speech</p></li>
<li><p>So…</p></li>
</ul>
<hr />
<h3 id="deep-learning-works-for-speech-too">Deep Learning works for
Speech too!</h3>
<ul>
<li><p>It’s never going to be easy</p></li>
<li><p>It’s never going to be cheap</p></li>
<li><p>… but it’ll work</p></li>
<li><p>And it’ll get you that much closer to actual human
interaction!</p></li>
</ul>
<hr />
<p><img class="r-stretch" src="img/c3p0.jpg"></p>
<hr />
<p><huge>Thank you!</huge></p>
<p><a href="http://savethevowels.org/talks/deep_learning_speech.html"
class="uri">http://savethevowels.org/talks/deep_learning_speech.html</a></p>
</body>
</html>
