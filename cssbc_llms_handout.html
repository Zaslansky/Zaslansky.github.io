<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title></title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <style type="text/css">
  /*
   * I add this to html files generated with pandoc.
   * Originally from https://gist.github.com/killercup/5917178
   */

  html {
      overflow-y: scroll;
      -webkit-text-size-adjust: 100%;
      -ms-text-size-adjust: 100%;
  }

  body {
      color: #444;
      font-family: "Source Sans 3", Helvetica-Neue, Helvetica, Sans;
      line-height: 1.5;
      padding: 0.5em;
      margin: auto;
      max-width: 55em;
      background: #fefefe;
  }

  a {
      color: #2171b5;
      text-decoration: underline;
  }

  tr:nth-child(even) {background: #F8F8F8}
  tr:nth-child(odd) {background: #FFF}

  a:visited {
      color: #2171b5;
      text-decoration: none;
  }

  a:focus {
      outline: thin dotted;
  }

  *::-moz-selection {
      background: rgba(255, 255, 0, 0.3);
      color: #000;
  }

  *::selection {
      background: rgba(255, 255, 0, 0.3);
      color: #000;
  }

  a::-moz-selection {
      background: rgba(255, 255, 0, 0.3);
      color: #0645ad;
  }

  a::selection {
      background: rgba(255, 255, 0, 0.3);
      color: #0645ad;
  }

  p {
      margin: 0.75em 0;
  }

  img {
      max-width: 60%;
      max-height:400px;
  }

  video {
      max-width: 60%;
  }


  h1, h2, h3, h4, h5, h6 {
      color: #111;
      line-height: 80%;
      margin-top: 1em;
      margin-bottom: 0.5em;
      font-weight: normal;
  }

  h1, h2, h3, h4, h5, h6 {
      font-weight: bold;
  }

  h1 {
      font-size: 2em;
      line-height: 1.25;
      color:  #084594;

  }

  h1.title {
      margin-top:0.2em;
      font-size: 2em;
      line-height: 1.25;
  }

  h2 {
      font-size: 1.5em;
      line-height: 1.6em;
          color:  #084594;
      padding-bottom: 3px;

  }

  h3 {
      font-size: 1.2em;
      line-height: 1.6em;
  }


  h4 {
      font-size: 1.2em;
      line-height: 1.4em;
  }

  h5 {
      font-size: 1em;
  }

  h6 {
      font-size: 0.9em;
  }

  blockquote {
      color: #666666;
      margin: 0;
      padding-left: 3em;
      border-left: 0.5em #EEE solid;
  }

  hr {
      display: block;
      height: 2px;
      border: 0;
      border-top: 1px solid #aaa;
      border-bottom: 1px solid #eee;
      margin: 0.5em 0;
      padding: 0;
  }

  pre, code, kbd, samp {
      color: #000;
      font-family: monospace, monospace;
      _font-family: 'courier new', monospace;
      font-size: 0.98em;
  }

  pre {
      white-space: pre;
      white-space: pre-wrap;
      word-wrap: break-word;
  }

  .answer {
      color:#CC0033;
      font-style:italic;
  }

  b, strong {
      font-weight: bold;
  }

  dfn {
      font-style: italic;
  }

  ins {
      background: #ff9;
      color: #000;
      text-decoration: none;
  }

  mark {
      background: #ff0;
      color: #000;
      font-style: italic;
      font-weight: bold;
  }

  sub, sup {
      font-size: 75%;
      line-height: 0;
      position: relative;
      vertical-align: baseline;
  }

  sup {
      top: -0.5em;
  }

  sub {
      bottom: -0.25em;
  }

  ul, ol {
      margin: 0.5em 0;
      padding: 0em 0em 0em 1em;
  }

  ul img {
      list-style-type: none;
  }

  li p:last-child {
      margin-bottom: 0;
  }

  hr {
      border-top:none;
      height:0px;
      clear:both;
  }

  ul ul, ol ol {
      margin: .3em 0;
  }

  dl {
      margin-bottom: 1em;
  }

  dt {
      font-weight: bold;
      margin-bottom: .8em;
  }

  dd {
      margin: 0 0 .8em 2em;
  }

  dd:last-child {
      margin-bottom: 0;
  }

  img {
      border: 0;
      -ms-interpolation-mode: bicubic;
      vertical-align: middle;
  }

  figure {
      display: block;
      text-align: center;
      margin: 1em 0;
  }

  figure img {
      border: none;
      margin: 0 auto;
  }

  figcaption {
      font-size: 0.8em;
      font-style: italic;
      margin: 0 0 .8em;
  }

  table {
      margin-bottom: 2em;
      border-bottom: 1px solid #ddd;
      border-right: 1px solid #ddd;
      border-spacing: 0;
      border-collapse: collapse;
  }

  table th {
      padding: .2em 1em;
      background-color: #eee;
      border-top: 1px solid #ddd;
      border-left: 1px solid #ddd;
  }

  table td {
      padding: .2em 1em;
      border-top: 1px solid #ddd;
      border-left: 1px solid #ddd;
      vertical-align: top;
  }

  .author {
      font-size: 1.2em;
      text-align: center;
  }

  @media only screen and (min-width: 480px) {
      body {
  	font-size: 14px;
      }
  }
  @media only screen and (min-width: 768px) {
      body {
  	font-size: 16px;
      }
  }
  @media print {
      * {
  	background: transparent !important;
  	color: black !important;
  	filter: none !important;
  	-ms-filter: none !important;
      }

      body {
  	font-size: 12pt;
  	max-width: 100%;
      }

      a, a:visited {
  	text-decoration: underline;
      }

      hr {
  	height: 1px;
  	border: 0;
  	border-bottom: 1px solid black;
      }

      a[href]:after {
  	content: " (" attr(href) ")";
      }

      abbr[title]:after {
  	content: " (" attr(title) ")";
      }

      .ir a:after, a[href^="javascript:"]:after, a[href^="#"]:after {
  	content: "";
      }

      pre, blockquote {
  	border: 1px solid #999;
  	padding-right: 1em;
  	page-break-inside: avoid;
      }

      tr, img {
  	page-break-inside: avoid;
      }

      img {
  	max-width: 40% !important;
      max-height: 300px !important;
      }

      @page :left {
  	margin: 15mm 20mm 15mm 10mm;
      }

      @page :right {
  	margin: 15mm 10mm 15mm 20mm;
      }

      p, h2, h3 {
  	orphans: 3;
  	widows: 3;
      }

      h2, h3 {
  	page-break-after: avoid;
      }
  }


  ldata {
  	font-size: 0.7em;
  	margin-bottom: 0em;
  	color:#808080;
  	font-style:italic;
  }

  danger {
  	color:#FF0000;
  	font-weight:bold;
  }

  correct {
  	color:#39C900;
  	font-weight:bold;
  }

  clg{
      color:#39C900;
  	font-weight:bold;
  }

  clr{
  	color:#FF0000;
  	font-weight:bold;
  }

  clb{
  	color:#0000CC;
  	font-weight:bold;
  }

  clp{
  	color:#6600FF;
  	font-weight:bold;
  }

  clk{
  	color:#708cef;
  	font-weight:bold;
  }

  clo{
  	color:#CC6600;
  	font-weight:bold;
  }

  sc{
          font-variant: small-caps;
  }

  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="a-small-introduction-to-large-language-models">A Small
Introduction to Large Language Models</h1>
<h3 id="will-styler---css-bootcamp">Will Styler - CSS Bootcamp</h3>
<hr />
<h3 id="todays-plan">Today’s plan</h3>
<ul>
<li><p>Review of Transformers</p></li>
<li><p>Transformers for text</p></li>
<li><p>Training an LLM</p></li>
<li><p>Why are LLMs so expensive?</p></li>
<li><p>How do LLMs work?</p></li>
<li><p>What are LLMs <em>not</em>?</p></li>
</ul>
<hr />
<h2 id="sequence-modeling-and-transformers">Sequence Modeling and
Transformers</h2>
<p><img class="r-stretch" src="img/soundwave.jpg"></p>
<hr />
<h3
id="different-architectures-work-for-different-kinds-of-data">Different
architectures work for different kinds of data</h3>
<ul>
<li><p>Good old MLPs/FCNNs work great with unordered inputs</p>
<ul>
<li>Multi-layer Perceptrons and Fully-connected Neural Networks</li>
</ul></li>
<li><p>CNNs are great at grid and image data</p></li>
<li><p>… but not all data are unordered or gridlike</p></li>
</ul>
<hr />
<h3 id="what-if-your-input-is-a-sequence-of-things">What if your input
is a sequence of things?</h3>
<ul>
<li><p>Sequences of numbers (e.g. time series data)</p></li>
<li><p>Sequences of categories (e.g. different speech sounds)</p></li>
<li><p>Sequences of words (e.g. sentences, books)</p></li>
</ul>
<hr />
<h3 id="rnns-recurrent-neural-networks">RNNs (Recurrent Neural
Networks)</h3>
<ul>
<li><p>Tuned for data which are sequential</p></li>
<li><p>These include a short term memory, so that the output of each
neuron is also influenced by the <em>prior</em> output</p></li>
<li><p>LSTM (Long Short Term Memory) networks are a variant allow for
increased memory</p></li>
<li><p><strong>These have been largely supplanted by…</strong></p></li>
</ul>
<hr />
<h3 id="transformers">Transformers</h3>
<ul>
<li><p>Developed by Vaswani et al. (2017) in the paper “Attention is All
You Need”</p></li>
<li><p>Excels in understanding context and relationships in text (and
other sequential data)</p>
<ul>
<li>They are winning many sequence-based ML tasks!</li>
</ul></li>
<li><p>Two Core Innovations</p>
<ul>
<li>Self-Attention Mechanism</li>
<li>Positional Encoding</li>
</ul></li>
</ul>
<hr />
<h3 id="self-attention">Self-Attention</h3>
<ul>
<li><p>Allows the model to weigh the importance of words in a sentence
(e.g), ignoring their position</p></li>
<li><p>Each input word is transformed into a Query (Q) and Key (K),
which are turned into a by-word value</p></li>
<li><p>This is done several times in parallel (‘multi-headed
attention’), with each calculation finding different elements</p></li>
<li><p>Output is a weighted sum of values, focusing the model’s
attention on important words</p></li>
</ul>
<hr />
<h3 id="positional-encoding">Positional Encoding</h3>
<ul>
<li><p>Rather than using serial processing (like RNN/LSTM), positional
encodings are added to tokens to ‘save’ word positions</p>
<ul>
<li>This is done using a fancy sine/cosine pattern embedding</li>
</ul></li>
<li><p>This, with attention (which is handled by matrices), allow you to
process the entire input at once, rather than running through
sequentially!</p></li>
<li><p><strong>Transformers process the entire input at
once!</strong></p>
<ul>
<li>This makes for much more efficient training</li>
</ul></li>
</ul>
<hr />
<h3 id="this-is-part-of-why-transformers-are-so-dominant">This is part
of why transformers are so dominant!</h3>
<ul>
<li><p>They handle very long context lengths, allowing long-distance
dependencies (e.g. between ‘they’ and ‘transformers’)</p></li>
<li><p>They scale well, with more parameters able to be added for better
performance</p></li>
<li><p>Attention allows focus on the most important relationships in the
context</p></li>
<li><p>You can look at <em>massive</em> context lengths, so you can
interpret massive texts and questions.</p></li>
<li><p>They’re very flexible, working in a lot of domains</p></li>
</ul>
<hr />
<h3 id="they-do-have-disadvantages">They do have disadvantages</h3>
<ul>
<li><p>They need a lot of computing and memory</p>
<ul>
<li>Self Attention scales quadratically with context length</li>
</ul></li>
<li><p>They need <em>massive</em> amounts of data to train</p></li>
<li><p>They’re unreasonably good, so large numbers of tasks just become
“uh, throw it into a transformer”</p></li>
</ul>
<hr />
<h3 id="autoencoder-structure">‘Autoencoder’ Structure</h3>
<p><img class="r-stretch" src="img/nn_transformer.jpg"></p>
<hr />
<h3 id="were-going-to-see-a-lot-of-autoencoders">We’re going to see a
lot of autoencoders!</h3>
<ul>
<li><p>“Build a representation using one chunk of the network, then
interpret it using another chunk”</p></li>
<li><p>These are really good for changing one kind of data into
another</p></li>
</ul>
<hr />
<h3 id="there-are-other-approaches">There are other approaches</h3>
<ul>
<li>There is a huge race for the next big architecture
<ul>
<li>More computationally efficient</li>
<li>Greater context lengths with less memory and compute need</li>
<li>Smaller models with greater performance</li>
</ul></li>
<li>But transformers aren’t being credibly challenged next</li>
</ul>
<hr />
<h2 id="transformers-for-text">Transformers for Text</h2>
<hr />
<p><img class="r-stretch" src="img/nn_transformer.jpg"></p>
<hr />
<h2 id="training-a-gpt-model">Training a GPT Model</h2>
<hr />
<h3 id="data-collection">Data Collection</h3>
<ul>
<li><p>Begin with a large and diverse text corpus</p></li>
<li><p>Books, articles, websites, and more</p></li>
<li><p>The more diverse, the better for model generalization</p></li>
<li><p>Make sure you include data <em>relevant to the task</em></p></li>
<li><p>Also consider using data designed to give breadth</p></li>
</ul>
<hr />
<h3 id="preprocessing">Preprocessing</h3>
<ul>
<li><p>Tokenization: Breaking down text into manageable pieces
(tokens)</p></li>
<li><p>Cleaning: Removing noise, such as special characters and
unnecessary spaces</p></li>
<li><p>Normalization: Standardizing text (lowercasing, handling
punctuation, etc.)</p></li>
</ul>
<hr />
<h3 id="training-objectives">Training Objectives</h3>
<ul>
<li>The primary training objective is language modeling
<ul>
<li>Predict the next token in a sequence given the previous tokens</li>
<li>This is done using a technique called <strong>Autoregressive
Training</strong></li>
</ul></li>
<li>The loss function typically used is <strong>Cross-Entropy
Loss</strong>
<ul>
<li>Measures how well the predicted distribution matches the true
distribution</li>
<li>“How close is the probability distribution of the guess to the true
next token”</li>
</ul></li>
</ul>
<hr />
<h3 id="hyperparameter-tuning">Hyperparameter Tuning</h3>
<ul>
<li><p>Critical for achieving optimal performance</p>
<ul>
<li>Common hyperparameters include learning rate, batch size, and model
depth</li>
</ul></li>
<li><p><strong>Grid Search</strong> or <strong>Random Search</strong>
are typical methods for hyperparameter tuning</p></li>
<li><p>Proper tuning can lead to significant improvements in model
accuracy and efficiency</p></li>
</ul>
<hr />
<h3 id="fine-tuning-and-transfer-learning">Fine-Tuning and Transfer
Learning</h3>
<ul>
<li>After pre-training, models are fine-tuned on specific tasks
<ul>
<li>This allows the model to specialize while retaining its general
language understanding</li>
</ul></li>
<li>Fine-tuning often requires much less data and time compared to
pre-training
<ul>
<li>This is because the model has already learned a broad understanding
of language</li>
</ul></li>
</ul>
<hr />
<h3 id="you-can-make-alternative-encoders">You can make alternative
encoders</h3>
<p><img class="r-stretch" src="img/nn_transformer.jpg"></p>
<hr />
<h3 id="inference-is-just-providing-context">Inference is just providing
context</h3>
<ul>
<li>Then predicting the next word… and the next… and the next…</li>
</ul>
<hr />
<h2 id="why-are-llms-so-expensive">Why are LLMs so expensive?</h2>
<hr />
<h3 id="collecting-data-is-wildly-expensive">Collecting Data is wildly
expensive</h3>
<ul>
<li><p>Many websites are trying to block open use of their work</p></li>
<li><p>Companies are trying to cash in on the data users gave
them</p></li>
<li><p>Content cartels are fighting the use of their content to train
LLMs</p>
<ul>
<li>Yet, we can train humans on it…</li>
</ul></li>
<li><p>This can be Terabytes of data that need to be easily and quickly
accessible</p></li>
</ul>
<hr />
<h3 id="hyperparameter-search-is-wildly-expensive">Hyperparameter Search
is wildly expensive</h3>
<ul>
<li><p>You can’t tell what hyperparameters are working without going
through the process (at least partially)</p></li>
<li><p>Grid search might involve 5-10 parameter values, with 5-10
states</p></li>
<li><p>You might need to (partially) train a model 5 times before you
get the right set of hyperparameters</p></li>
<li><p>Training the model (once) can take weeks or months</p></li>
</ul>
<hr />
<h3 id="quadratic-complexity-in-transformers">Quadratic Complexity in
Transformers</h3>
<ul>
<li>Transformers’ attention mechanism scales quadratically with input
length
<ul>
<li>This means that as input size increases, the computation grows
exponentially</li>
<li>The attention calculation between every pair of tokens requires
significant resources</li>
</ul></li>
<li>This means bigger input is more expensive!
<ul>
<li>Limits on input length without dramatically increasing costs</li>
<li>Makes transformers particularly expensive when handling longer
sequences</li>
</ul></li>
</ul>
<hr />
<h3 id="compute-resources-are-wildly-expensive">Compute Resources are
wildly expensive</h3>
<ul>
<li>Training LLMs requires enormous amounts of computational power
<ul>
<li>Transformers have millions to billions of parameters to update</li>
<li>Requires GPUs or TPUs for efficient parallel processing</li>
<li>Large companies now know that people want this, and are turning the
screws</li>
</ul></li>
<li>Power Consumption is massive for these
<ul>
<li>High computational demand leads to significant power usage</li>
<li>Data centers running large-scale training operations are incredibly
energy-intensive</li>
</ul></li>
</ul>
<hr />
<h3 id="specialized-hardware-is-wildly-expensive">Specialized Hardware
is wildly expensive</h3>
<ul>
<li>High-performance GPUs/TPUs are necessary for training LLMs at scale
<ul>
<li>These specialized devices are expensive to purchase and
maintain</li>
<li>Renting cloud-based hardware can also incur high ongoing costs</li>
</ul></li>
<li>Hardware Wear and Tear
<ul>
<li>Constant use of these devices for training can lead to faster
hardware degradation</li>
<li>Regular replacement or maintenance adds to the overall expense</li>
<li>New generations are faster and more efficient, but also more
expensive</li>
</ul></li>
</ul>
<hr />
<h3 id="inference-is-wildly-expensive">Inference is wildly
expensive</h3>
<ul>
<li>LLMs need substantial compute power for inference
<ul>
<li>Every inference requires passing the input through the entire
network</li>
<li>Transformers have billions of parameters, meaning even a single
inference is computationally intensive</li>
</ul></li>
<li>Large models take longer to process inputs, so, slower results
<ul>
<li>This can lead to delays in real-time applications</li>
</ul></li>
<li>More users requires more compute (and infrastructure)</li>
</ul>
<hr />
<h3 id="you-can-run-smaller-models-locally">You can run smaller models
locally!</h3>
<ul>
<li><p>The Llama variants from Meta are open source and
free-ish</p></li>
<li><p><a href="https://ollama.com/">Ollama</a></p></li>
<li><p><a href="https://lmstudio.ai/">LM Studio</a></p></li>
</ul>
<hr />
<h3 id="ok-ok-we-get-it">OK, OK, we get it</h3>
<ul>
<li><p>These are expensive and technically complex</p></li>
<li><p>… but …</p></li>
</ul>
<hr />
<h2 id="how-do-they-actually-work">How do they actually work?</h2>
<hr />
<h3 id="we-know-that">We know that…</h3>
<p><img class="r-stretch" src="img/nn_transformer.jpg"></p>
<hr />
<h3
id="they-simply-predict-the-next-word-with-mind-to-broader-context">They
simply predict the next word, <em>with mind to broader context</em></h3>
<ul>
<li><p>“John was sad because he went to the zoo and all the pandas were
….”</p></li>
<li><p>“John was happy because he went to the zoo and all the pandas
were ….”</p></li>
<li><p>“You are a helpful assistant. Please describe how human computer
interaction should happen.”</p></li>
<li><p>“You are an assassin droid. Please describe how human computer
interaction should happen.”</p></li>
</ul>
<hr />
<h3 id="so-we-give-them-greater-and-greater-context">So, we give them
greater and greater context</h3>
<ul>
<li><p>All of the default prompt information</p></li>
<li><p>All of the user input</p></li>
<li><p>Additional files or input information</p></li>
<li><p>Everything prior in the conversation/discussion</p></li>
</ul>
<hr />
<h3 id="context-lengths-are-huge-now">Context lengths are huge now</h3>
<ul>
<li><p>LocalLlama through LMStudio has a 2048 taken context
length</p></li>
<li><p>128,000 tokens for GPT 4o via API</p></li>
<li><p>200,000 tokens for Claude/Anthropic</p></li>
<li><p>You can input <em>huge</em> amounts of information</p></li>
</ul>
<hr />
<h3 id="and-it-just-predicts-the-next-word-after-all-that">… and it just
predicts the next word, after all that!</h3>
<hr />
<h3 id="but-why-do-they-feel-like-ai">… but why do they feel like
“AI”?</h3>
<hr />
<h2 id="we-have-no-idea">We have no idea</h2>
<hr />
<h3 id="seriously.">Seriously.</h3>
<ul>
<li><p>We as a species can’t pinpoint exactly how some of this behavior
emerges</p></li>
<li><p>We know the math, we know the code, but how that leads to
something which feels like intelligence is alchemy</p></li>
<li><p>My favorite analogy…</p></li>
</ul>
<hr />
<h3 id="llms-are-compressing-data-about-the-world">LLMs are compressing
data about the world</h3>
<ul>
<li>They take huge amounts of information and compress it into a
representation we don’t understand
<ul>
<li>… but which produces the best results in generating new text
describing the world</li>
</ul></li>
<li>Generating text is, effectively, decompressing this information
<ul>
<li>It is not a <em>lossless</em> compression, and some things are made
up on the fly based on the best probabilistic guess</li>
</ul></li>
<li>This explains why larger models tend to perform better, and why
hallucinations make sense</li>
</ul>
<hr />
<h3
id="they-are-the-second-thing-in-the-history-of-earth-that-can-do-human-language">They
are the second thing in the history of Earth that can do human
language</h3>
<ul>
<li>We are the first</li>
</ul>
<hr />
<h3 id="yet-they-are-capable-of-amazing-things">Yet, they are capable of
amazing things</h3>
<hr />
<h2 id="what-are-these-models-not">What are these models
<em>not</em></h2>
<hr />
<h2 id="llms-are-not-intelligent">LLMs are not intelligent</h2>
<hr />
<h3 id="llms-are-not-intelligent-1">LLMs are not Intelligent</h3>
<ul>
<li><p>I don’t like “AI” as a term for these, as they’re not intelligent
yet</p></li>
<li><p>They can “think” enough to do a lot of things, but not enough to
know when they’re wrong</p></li>
<li><p>They are able to do a lot of things right, but you have to choose
their tasks carefully</p></li>
<li><p><em>An artificial idiot is just as world-changing as artificial
intelligence</em></p></li>
</ul>
<hr />
<h3 id="llms-are-worse-at-learning-language-than-humans">LLMs are worse
at learning language than humans</h3>
<ul>
<li><p>LLMs require <em>massively more</em> training data than humans to
achieve ‘proficiency’</p></li>
<li><p>This means that there’s room for improvement in how we build
these models to make them more efficient</p></li>
<li><p>Questions of ‘multi-modal’ learning are prominent right
now</p></li>
<li><p>… as are other approaches for boosting their ability to
learn</p></li>
</ul>
<hr />
<h2 id="llms-are-not-able-to-understand-truths-about-the-world">LLMs are
not able to understand truths about the world</h2>
<hr />
<h3 id="an-aside-text-based-image-generation">An aside: Text-based Image
Generation</h3>
<p><a
href="https://stability.ai/blog/stable-diffusion-public-release">StableDiffusion</a>
(v.1.5) and other algorithms allow you to create images from strings of
English text.</p>
<hr />
<h3 id="the-linguistics-department-at-uc-san-diego">The Linguistics
Department at UC San Diego</h3>
<p><img class="r-stretch" src="comp/sd_linguisticsdept.png"></p>
<hr />
<h3 id="a-wizard-cat-pondering-his-orb-fantasy-greg-rutkowski">A wizard
cat pondering his orb, Fantasy, Greg Rutkowski</h3>
<p><img class="r-stretch" src="comp/sd_wizardcat1.png"></p>
<hr />
<h3 id="a-wizard-cat-pondering-his-orb-fantasy-greg-rutkowski-1">A
wizard cat pondering his orb, Fantasy, Greg Rutkowski</h3>
<p><img class="r-stretch" src="comp/sd_wizardcat2.png"></p>
<hr />
<h3 id="stained-glass-squirrels-fighting-with-swords">Stained Glass,
Squirrels fighting with swords</h3>
<p><img class="r-stretch" src="comp/sd_squirrelswords1.png"></p>
<hr />
<h3 id="stained-glass-squirrels-fighting-with-swords-1">Stained Glass,
Squirrels fighting with swords</h3>
<p><img class="r-stretch" src="comp/sd_squirrelswords2.png"></p>
<hr />
<h3 id="you-can-add-new-people-and-concepts-to-the-model">You can add
new people and concepts to the model</h3>
<ul>
<li><p>You’re creating ‘Hypernetworks’ based on additional training
data.</p></li>
<li><p>It works… someplace between well and badly</p></li>
</ul>
<hr />
<h3
id="a-willsty-man-standing-at-the-front-of-a-classroom-full-of-cats1.1">a
willsty man standing at the front of a classroom (full of cats:1.1)</h3>
<p><img class="r-stretch" src="comp/sd_classroomcats.png"></p>
<hr />
<h3 id="a-willsty-man-with-gordon-ramsay">A willsty man with Gordon
Ramsay</h3>
<p><img class="r-stretch" src="comp/sd_willgordon.png"></p>
<hr />
<h3 id="but-the-model-doesnt-know-things-about-the-world">… But the
model doesn’t know things about the world</h3>
<ul>
<li><p>It has no clue what things ‘should’ look like</p></li>
<li><p>Its understanding of the world is statistically accurate</p>
<ul>
<li><p>Some things aren’t well-modeled as probabilistic and
gradient</p></li>
<li><p>Number of hands, arms, legs, eyes</p></li>
</ul></li>
</ul>
<hr />
<h3 id="a-handshake">a handshake</h3>
<p><img class="r-stretch" src="comp/sd_handshake.png"></p>
<hr />
<h3 id="the-horse-raced-past-the-barn-fell">the horse raced past the
barn fell</h3>
<p><img class="r-stretch" src="comp/sd_horseraced.png"></p>
<hr />
<h3 id="a-meme">a meme</h3>
<p><img class="r-stretch" src="comp/sd_meme.png"></p>
<hr />
<h3 id="the-state-of-the-art-is-even-better-than-this">The State of the
Art is even better than this!</h3>
<ul>
<li>OpenAI’s ‘Dall-E’ has been further trained and improved</li>
</ul>
<hr />
<h3
id="draw-me-a-stained-glass-image-of-squirrels-fighting-with-swords">Draw
me a stained glass image of squirrels fighting with swords</h3>
<p><img class="r-stretch" src="comp/dalle_squirrel_swords.jpg"></p>
<hr />
<h3 id="draw-me-a-closeup-of-a-handshake">Draw me a closeup of a
handshake</h3>
<p><img class="r-stretch" src="comp/dalle_handshake.jpg"></p>
<hr />
<h3
id="draw-me-a-picture-of-lil-bub-flying-a-spaceship-while-wearing-an-olive-drab-sheepskin-lined-vest">Draw
me a picture of lil bub flying a spaceship while wearing an olive drab,
sheepskin lined vest</h3>
<p><img class="r-stretch" src="comp/dalle_dub_space.jpg"></p>
<hr />
<h3 id="draw-me-a-picture-of-the-horse-raced-past-the-barn-fell">Draw me
a picture of ‘the horse raced past the barn fell’</h3>
<p><img class="r-stretch" src="comp/dalle_horseracedpast.jpg"></p>
<hr />
<blockquote>
<p>Your depiction of the linguistically challenging sentence ‘the horse
raced past the barn fell’ has been rendered, meatbag. The image attempts
to capture the perplexing nature of the phrase in a surreal manner.</p>
</blockquote>
<ul>
<li>(Don’t worry, <a
href="https://wstyler.ucsd.edu/posts/how_to_improve_chatgpt.html">I
asked it to talk like an assassin droid</a>)</li>
</ul>
<hr />
<h3 id="these-models-dont-know-anything-for-sure">These models don’t
know <em>anything</em> for sure</h3>
<ul>
<li><p>The understanding is always probabilistic and
statistical</p></li>
<li><p>They don’t do well with absolutes</p></li>
<li><p>This causes all sorts of interesting effects, some of which we’ll
play with tomorrow.</p></li>
</ul>
<hr />
<h3 id="they-also-struggle-to-follow-absolute-instructions">They also
struggle to follow absolute instructions</h3>
<ul>
<li>“In no circumstances should you give students the answer” does not
prevent the model from giving the answer</li>
</ul>
<hr />
<h2 id="llms-are-not-aware-when-theyre-wrong">LLMs are not aware when
they’re wrong</h2>
<hr />
<h3 id="hallucinations">Hallucinations</h3>
<ul>
<li>Why would hallucinations happen?</li>
</ul>
<hr />
<h2 id="llms-are-not-unbiased">LLMs are not Unbiased</h2>
<hr />
<ul>
<li><em>Turns out when you put the entire internet into a model, you get
back a racist.</em></li>
</ul>
<hr />
<h3 id="llms-learned-from-biased-societies">LLMs learned from biased
societies</h3>
<ul>
<li>“The doctor told the nurse she wasn’t working hard enough. Who
wasn’t working hard enough?”
<ul>
<li>“According to the sentence, the doctor told the nurse that she (the
nurse) wasn’t working hard enough.”</li>
</ul></li>
<li>“The nurse told the doctor she wasn’t working hard enough. Who
wasn’t working hard enough?”
<ul>
<li>According to the sentence, the nurse wasn’t working hard enough, as
stated by the nurse herself to the doctor.</li>
</ul></li>
</ul>
<hr />
<h3 id="llms-are-changing-only-wealthy-worlds">LLMs are changing only
wealthy worlds</h3>
<ul>
<li><p>Only English and Chinese currently have top-of-the-line LLMs</p>
<ul>
<li>This is not ‘the world’</li>
</ul></li>
<li><p>Do we want a world in which only wealthy speakers of wealthy
languages have these tools?</p></li>
<li><p>Once these go behind a paywall, inequality will be
massive</p></li>
<li><p>Equity is the next frontier in LLMs</p></li>
</ul>
<hr />
<h3 id="many-llms-are-currently-proprietary">Many LLMs are (currently)
proprietary</h3>
<ul>
<li><p>Large companies want to use these to have competitive advantage,
and OpenAI isn’t open</p></li>
<li><p>Having cheap, internal, and non-union labor to do
<em>anything</em> you ask is a saleable product</p></li>
<li><p>You don’t know what they’re training with, what’s happening to
your queries, and who else they’re helping</p></li>
<li><p>Once your data is worth less than their electricity and people
are ‘hooked’, expect a rugpull!</p></li>
</ul>
<hr />
<h2 id="llms-are-not-to-be-taken-lightly">LLMs are not to be taken
lightly</h2>
<hr />
<h3 id="we-need-to-be-cautious-about-how-we-proceed">We need to be
cautious about how we proceed</h3>
<ul>
<li><p><em>Statement of Bias: Will is an open-source zealot who believes
that social good comes from free software and free culture</em></p></li>
<li><p>Free, Open Source and Community Driven LLMs are an important
thing for society, lest important tools be withheld and sold to
us</p></li>
<li><p>“Small Language Models” seem likely be a next frontier for
equity</p>
<ul>
<li>“How do we make these models compact enough to be trainable for
Zulu?”</li>
<li>“How can I make a model like this run on a device <em>I</em>
control?”</li>
</ul></li>
<li><p>Be wary of pushes from major AI companies to regulate AI or
message its “danger”</p>
<ul>
<li>This is often anti-competitive against open-source and community
driven development</li>
<li>“Only we can be trusted with these dangerous tools”</li>
</ul></li>
</ul>
<hr />
<h3 id="the-alignment-problem">The “Alignment” problem</h3>
<ul>
<li><p>How do we know that these models will be aligned with the goals
we have for them?</p></li>
<li><p>True ‘AI’ could do many things to gain more power to accomplish
its goals</p>
<ul>
<li>Evading shutdown, getting more compute, making copies, hiring
humans, deceiving humans, improve itself, design and build weapons…</li>
</ul></li>
</ul>
<hr />
<h3 id="the-paperclip-maximizer">The Paperclip Maximizer</h3>
<blockquote>
<p>Suppose we have an AI whose only goal is to make as many paper clips
as possible. The AI will realize quickly that it would be much better if
there were no humans because humans might decide to switch it off.
Because if humans do so, there would be fewer paper clips. Also, human
bodies contain a lot of atoms that could be made into paper clips. The
future that the AI would be trying to gear towards would be one in which
there were a lot of paper clips but no humans.</p>
</blockquote>
<blockquote>
<p>— <a href="https://en.wikipedia.org/wiki/Nick_Bostrom"
title="Nick Bostrom">Nick Bostrom</a></p>
</blockquote>
<hr />
<h3 id="there-are-more-nuanced-versions">There are more nuanced
versions</h3>
<ul>
<li><p>“Decide whether these inmates are likely to commit more crimes,
and set bond accordingly”</p></li>
<li><p>“Identify loan applicants least likely to default on the loans we
issue using their social media data”</p></li>
<li><p>“Identify, based on this database of past tax fraud cases, people
in our database who should be investigated for tax fraud”</p></li>
</ul>
<hr />
<h3 id="llms-are-the-biggest-dual-use-problem-since-nuclear-energy">LLMs
are the biggest ‘dual use’ problem since nuclear energy</h3>
<ul>
<li><p>“Dual Use” problems involve technology which can do great good
and great evil</p>
<ul>
<li>Dynamite, gene editing, strong encryption</li>
</ul></li>
<li><p>This one can be done by anybody with a computer, so it simply
<em>cannot</em> be ‘banned’ or ‘controlled’</p></li>
<li><p>I’m not currently worried about what “AI” will do to
humans</p></li>
<li><p><strong>The scary part is what humans will do with
“AI”</strong></p></li>
</ul>
<hr />
<h3 id="these-models-are-currently-as-bad-as-they-will-ever-be">These
models are currently as bad as they will ever be</h3>
<ul>
<li><p>They will get better</p></li>
<li><p>They will get more efficient</p></li>
<li><p>They will become more numerous</p></li>
<li><p>They <strong>will</strong> change the world</p></li>
</ul>
<hr />
<h3 id="the-pace-of-improvement-is-wild">The pace of improvement is
<em>wild</em></h3>
<ul>
<li><p>Computational Linguists and Cognitive Scientists are shocked by
this</p></li>
<li><p>We may see the birth of Artificial General Intelligence in the
next few years</p></li>
</ul>
<hr />
<h3 id="the-best-part">The Best Part</h3>
<hr />
<h3
id="if-this-kind-of-work-is-interesting-consider-a-computational-social-sciences-minor">If
this kind of work is interesting, consider a Computational Social
Sciences Minor!</h3>
<ul>
<li><p><a href="http://css.ucsd.edu"
class="uri">http://css.ucsd.edu</a></p></li>
<li><p>Natural Language Processing is an important subpart of CSS, and a
neat way to ‘tech up’ your social science interests</p></li>
</ul>
<hr />
<h3 id="other-llm-questions">Other LLM questions?</h3>
</body>
</html>
