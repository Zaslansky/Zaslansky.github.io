<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title></title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <style type="text/css">
  /*
   * I add this to html files generated with pandoc.
   * Originally from https://gist.github.com/killercup/5917178
   */

  html {
      overflow-y: scroll;
      -webkit-text-size-adjust: 100%;
      -ms-text-size-adjust: 100%;
  }

  body {
      color: #444;
      font-family: "Source Sans 3", Helvetica-Neue, Helvetica, Sans;
      line-height: 1.5;
      padding: 0.5em;
      margin: auto;
      max-width: 55em;
      background: #fefefe;
  }

  a {
      color: #2171b5;
      text-decoration: underline;
  }

  tr:nth-child(even) {background: #F8F8F8}
  tr:nth-child(odd) {background: #FFF}

  a:visited {
      color: #2171b5;
      text-decoration: none;
  }

  a:focus {
      outline: thin dotted;
  }

  *::-moz-selection {
      background: rgba(255, 255, 0, 0.3);
      color: #000;
  }

  *::selection {
      background: rgba(255, 255, 0, 0.3);
      color: #000;
  }

  a::-moz-selection {
      background: rgba(255, 255, 0, 0.3);
      color: #0645ad;
  }

  a::selection {
      background: rgba(255, 255, 0, 0.3);
      color: #0645ad;
  }

  p {
      margin: 0.75em 0;
  }

  img {
      max-width: 60%;
      max-height:400px;
  }

  video {
      max-width: 60%;
  }


  h1, h2, h3, h4, h5, h6 {
      color: #111;
      line-height: 80%;
      margin-top: 1em;
      margin-bottom: 0.5em;
      font-weight: normal;
  }

  h1, h2, h3, h4, h5, h6 {
      font-weight: bold;
  }

  h1 {
      font-size: 2em;
      line-height: 1.25;
      color:  #084594;

  }

  h1.title {
      margin-top:0.2em;
      font-size: 2em;
      line-height: 1.25;
  }

  h2 {
      font-size: 1.5em;
      line-height: 1.6em;
          color:  #084594;
      padding-bottom: 3px;

  }

  h3 {
      font-size: 1.2em;
      line-height: 1.6em;
  }


  h4 {
      font-size: 1.2em;
      line-height: 1.4em;
  }

  h5 {
      font-size: 1em;
  }

  h6 {
      font-size: 0.9em;
  }

  blockquote {
      color: #666666;
      margin: 0;
      padding-left: 3em;
      border-left: 0.5em #EEE solid;
  }

  hr {
      display: block;
      height: 2px;
      border: 0;
      border-top: 1px solid #aaa;
      border-bottom: 1px solid #eee;
      margin: 0.5em 0;
      padding: 0;
  }

  pre, code, kbd, samp {
      color: #000;
      font-family: monospace, monospace;
      _font-family: 'courier new', monospace;
      font-size: 0.98em;
  }

  pre {
      white-space: pre;
      white-space: pre-wrap;
      word-wrap: break-word;
  }

  .answer {
      color:#CC0033;
      font-style:italic;
  }

  b, strong {
      font-weight: bold;
  }

  dfn {
      font-style: italic;
  }

  ins {
      background: #ff9;
      color: #000;
      text-decoration: none;
  }

  mark {
      background: #ff0;
      color: #000;
      font-style: italic;
      font-weight: bold;
  }

  sub, sup {
      font-size: 75%;
      line-height: 0;
      position: relative;
      vertical-align: baseline;
  }

  sup {
      top: -0.5em;
  }

  sub {
      bottom: -0.25em;
  }

  ul, ol {
      margin: 0.5em 0;
      padding: 0em 0em 0em 1em;
  }

  ul img {
      list-style-type: none;
  }

  li p:last-child {
      margin-bottom: 0;
  }

  hr {
      border-top:none;
      height:0px;
      clear:both;
  }

  ul ul, ol ol {
      margin: .3em 0;
  }

  dl {
      margin-bottom: 1em;
  }

  dt {
      font-weight: bold;
      margin-bottom: .8em;
  }

  dd {
      margin: 0 0 .8em 2em;
  }

  dd:last-child {
      margin-bottom: 0;
  }

  img {
      border: 0;
      -ms-interpolation-mode: bicubic;
      vertical-align: middle;
  }

  figure {
      display: block;
      text-align: center;
      margin: 1em 0;
  }

  figure img {
      border: none;
      margin: 0 auto;
  }

  figcaption {
      font-size: 0.8em;
      font-style: italic;
      margin: 0 0 .8em;
  }

  table {
      margin-bottom: 2em;
      border-bottom: 1px solid #ddd;
      border-right: 1px solid #ddd;
      border-spacing: 0;
      border-collapse: collapse;
  }

  table th {
      padding: .2em 1em;
      background-color: #eee;
      border-top: 1px solid #ddd;
      border-left: 1px solid #ddd;
  }

  table td {
      padding: .2em 1em;
      border-top: 1px solid #ddd;
      border-left: 1px solid #ddd;
      vertical-align: top;
  }

  .author {
      font-size: 1.2em;
      text-align: center;
  }

  @media only screen and (min-width: 480px) {
      body {
  	font-size: 14px;
      }
  }
  @media only screen and (min-width: 768px) {
      body {
  	font-size: 16px;
      }
  }
  @media print {
      * {
  	background: transparent !important;
  	color: black !important;
  	filter: none !important;
  	-ms-filter: none !important;
      }

      body {
  	font-size: 12pt;
  	max-width: 100%;
      }

      a, a:visited {
  	text-decoration: underline;
      }

      hr {
  	height: 1px;
  	border: 0;
  	border-bottom: 1px solid black;
      }

      a[href]:after {
  	content: " (" attr(href) ")";
      }

      abbr[title]:after {
  	content: " (" attr(title) ")";
      }

      .ir a:after, a[href^="javascript:"]:after, a[href^="#"]:after {
  	content: "";
      }

      pre, blockquote {
  	border: 1px solid #999;
  	padding-right: 1em;
  	page-break-inside: avoid;
      }

      tr, img {
  	page-break-inside: avoid;
      }

      img {
  	max-width: 40% !important;
      max-height: 300px !important;
      }

      @page :left {
  	margin: 15mm 20mm 15mm 10mm;
      }

      @page :right {
  	margin: 15mm 10mm 15mm 20mm;
      }

      p, h2, h3 {
  	orphans: 3;
  	widows: 3;
      }

      h2, h3 {
  	page-break-after: avoid;
      }
  }


  ldata {
  	font-size: 0.7em;
  	margin-bottom: 0em;
  	color:#808080;
  	font-style:italic;
  }

  danger {
  	color:#FF0000;
  	font-weight:bold;
  }

  correct {
  	color:#39C900;
  	font-weight:bold;
  }

  clg{
      color:#39C900;
  	font-weight:bold;
  }

  clr{
  	color:#FF0000;
  	font-weight:bold;
  }

  clb{
  	color:#0000CC;
  	font-weight:bold;
  }

  clp{
  	color:#6600FF;
  	font-weight:bold;
  }

  clk{
  	color:#708cef;
  	font-weight:bold;
  }

  clo{
  	color:#CC6600;
  	font-weight:bold;
  }

  sc{
          font-variant: small-caps;
  }

  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h3
id="i-know-the-project-is-due-today-but-im-just-getting-started-and-i-have-questions">“I
know the project is due today but I’m just getting started and I have
questions…”</h3>
<p><img class="r-stretch" src="humorimg/done_cormac.jpg"></p>
<hr />
<h1 id="automatic-part-of-speech-tagging">Automatic Part-of-Speech
Tagging</h1>
<h3 id="will-styler---lign-6">Will Styler - LIGN 6</h3>
<hr />
<h3 id="todays-plan">Today’s Plan</h3>
<ul>
<li><p>Why computers can’t POS tag like humans</p></li>
<li><p>Creating a corpus for POS-tagging use</p></li>
<li><p>Part-of-Speech Ambiguity</p></li>
<li><p>How does HMM-based POS tagging work?</p></li>
<li><p>POS Tagging is hard</p></li>
</ul>
<hr />
<h3 id="weve-talked-about-parts-of-speech-already">We’ve talked about
parts of speech already</h3>
<hr />
<h3 id="lexical-categories">Lexical Categories</h3>
<ul>
<li><strong>Nouns</strong>: bike, car, cat, dog, tofu, dude, bling</li>
<li><strong>Verbs:</strong> go, eat, talk, walk, yeet</li>
<li><strong>Adjectives:</strong> lit, sweet, hot, cool, awesome</li>
<li><strong>Adverbs:</strong> well, fast, slowly, easily</li>
<li><strong>Pre/postpositions:</strong> with, from, on, in</li>
<li><strong>Determiners:</strong> the, a, that, this, those</li>
<li><strong>Pronouns:</strong> she, he, him, her, it, I, you, they</li>
<li><strong>Conjunctions:</strong> and, or, whenever, while</li>
<li><strong>Numeral:</strong> one, twice, third</li>
<li><strong>Interjection:</strong> ouch, tsk, damnit!</li>
</ul>
<hr />
<h3 id="but-these-are-linguistic-human-categories">… but these are
linguistic, human categories</h3>
<ul>
<li><p>We understand the functional distinction between an adverb and a
preposition</p></li>
<li><p>We can talk within a certain language, but understand when the
rules change</p></li>
<li><p>We know the <em>semantics</em> of a given word</p>
<ul>
<li>We know that a pipe is an object, and that it has a function that
could be verby</li>
</ul></li>
</ul>
<hr />
<h3 id="we-also-gave-you-tests-to-use">We also gave you ‘tests’ to
use</h3>
<ul>
<li><p>“Can you make it plural? If so, it’s a noun!”</p></li>
<li><p>“Can you inflect it? If so, it’s a verb!”</p></li>
<li><p>“If you can use a comparative construction, it’s probably an
adjective!”</p></li>
<li><p>“Pronouns can substitute for noun phrases”</p></li>
<li><p>“Is this a relationship a squirrel can have with a tree? Then
probably Preposition!”</p></li>
</ul>
<hr />
<h3 id="but-a-computer-cant-use-any-of-these-tests">… but a computer
can’t use <em>any</em> of these tests</h3>
<ul>
<li><p>“Sure, rotates is the plural of ‘rotate’, so it’s a
noun”</p></li>
<li><p>“I treed, therefore, tree is a verb”</p></li>
<li><p>“This slide is computerer than the last one”</p></li>
<li><p>“What the heck is a noun phrase, anyways?”</p></li>
<li><p>“Squirrel? Tree? Huh?”</p></li>
</ul>
<hr />
<h3
id="so-we-cant-teach-computers-to-do-pos-tagging-in-the-same-way-that-we-teach-humans-to">So,
we can’t teach computers to do POS tagging in the same way that we teach
humans to!</h3>
<hr />
<h1 id="preparing-for-pos-tagging">Preparing for POS Tagging</h1>
<hr />
<h3 id="before-we-can-automate-it-we-need-to-do-it-with-humans">Before
we can automate it, we need to do it with humans</h3>
<ul>
<li>This is always going to be the case</li>
</ul>
<hr />
<h3 id="determining-the-best-tagset">Determining the best tagset</h3>
<ul>
<li><p>This is partly language specific</p>
<ul>
<li><p>What POS categories exist</p></li>
<li><p>What additional detail would be helpful in prediction</p></li>
</ul></li>
<li><p>Partly based on what corpora are available</p>
<ul>
<li>Use this tagset, or annotate 12 million words?</li>
</ul></li>
</ul>
<hr />
<h3 id="for-english">For English…</h3>
<p><img class="r-stretch" src="comp/penn_tagset.jpg"></p>
<p>(Table from Jurafsky and Martin ‘Speech and Language Processing’
3e)</p>
<hr />
<h3 id="annotating-a-corpus-for-pos-tags">Annotating a corpus for POS
tags</h3>
<ul>
<li><p>Teach some annotators the POS-tagging system</p></li>
<li><p>Run a sample POS-tagging system to get suggestions</p></li>
<li><p>Have the annotators hand correct them</p></li>
</ul>
<hr />
<blockquote>
<p>On/IN an/DT exceptionally/RB hot/JJ evening/NN early/RB in/IN
July/NNP a/DT young/JJ man/NN came/VBD out/RP of/IN the/DT garret/NN
in/IN which/WDT he/PRP lodged/VBN and/CC walked/VBD slowly/RB ,/, as/RB
though/IN in/IN hesitation/NN ,/, towards/IN a/DT bridge/NN ./.</p>
</blockquote>
<p><img class="r-stretch" src="comp/penn_tagset.jpg"></p>
<hr />
<p>All example tagging from today comes from <a
href="http://nlp.stanford.edu:8080/parser/index.jsp">the Stanford
Parser</a></p>
<hr />
<h3 id="there-are-many-tagged-corpora-already-out-there">There are many
tagged corpora already out there</h3>
<ul>
<li><p>You don’t need to do this.</p></li>
<li><p>Which is good.</p></li>
<li><p>POS tagging is <em>super boring</em></p></li>
</ul>
<hr />
<h3 id="once-you-have-a-tagset-and-a-corpus-you-can-use">Once you have a
tagset and a corpus, you can use…</h3>
<hr />
<h1 id="automatic-pos-tagging">Automatic POS Tagging</h1>
<hr />
<h2 id="pos-ambiguity">POS Ambiguity</h2>
<p>How much uncertainty there is about the part of speech of a given
word</p>
<hr />
<h3 id="some-words-are-certain-in-terms-of-pos">Some words are
<em>certain</em> in terms of POS</h3>
<ul>
<li><p>‘Funniest’</p></li>
<li><p>‘hesitantly’</p></li>
<li><p>‘Sharon’</p></li>
<li><p>Around 85% of words are <em>unambiguous</em> in terms of POS</p>
<ul>
<li>… but around 65% of <em>tokens</em> in running text are ambiguous
:(</li>
</ul></li>
</ul>
<hr />
<h3 id="some-words-are-only-a-bit-ambiguous-in-pos">Some words are only
a bit ambiguous in POS</h3>
<ul>
<li><p>‘in’</p></li>
<li><p>‘a’</p></li>
<li><p>‘between’</p></li>
<li><p>‘Marshall’</p></li>
<li><p>‘Demonstrated’</p></li>
</ul>
<hr />
<h3 id="some-words-are-very-ambiguous-in-pos">Some words are very
ambiguous in POS</h3>
<ul>
<li><p>‘sink’</p></li>
<li><p>‘that’</p></li>
<li><p>‘lift’</p></li>
<li><p>‘will’</p></li>
</ul>
<hr />
<h3 id="some-words-have-many-parts-of-speech">Some words have many parts
of speech</h3>
<ul>
<li>earnings growth took a back/JJ seat</li>
<li>a small building in the back/NN</li>
<li>a clear majority of senators back/VBP the bill</li>
<li>Dave began to back/VB toward the door</li>
<li>enable the country to buy back/RP about debt</li>
<li>I was twenty-one back/RB then</li>
</ul>
<hr />
<h3 id="pos-tagging-is-about-resolving-this-ambiguity">POS tagging is
about resolving this ambiguity</h3>
<hr />
<h3 id="the-stupid-approach-most-frequent-tag">The Stupid Approach:
‘Most Frequent Tag’</h3>
<ul>
<li><p>“Let the tag of word X be the most likely tag of word X in our
corpus”</p></li>
<li><p>Tagging is just a lookup table</p>
<ul>
<li><p>‘fly’ is most frequently a verb</p></li>
<li><p>Therefore, every instance of ‘fly’ is a verb</p></li>
</ul></li>
<li><p>This provides a ‘baseline’ performance</p>
<ul>
<li>“If we take the dumbest possible approach, what performance do we
get?”</li>
</ul></li>
</ul>
<hr />
<h3 id="most-frequent-tag-accuracy">Most Frequent Tag Accuracy</h3>
<ul>
<li><p>Accuracy here is ‘percentage of tags correctly labeled’</p></li>
<li><p>Most Frequent Tag gets 92% accuracy on WSJ data!</p></li>
<li><p>If we want to use something more complicated, you have to do
better than this.</p>
<ul>
<li>If you can’t beat the dumbest approach, you’ve got a problem</li>
</ul></li>
</ul>
<hr />
<h3 id="slightly-more-intelligent-word-form-features">Slightly more
intelligent: Word form features</h3>
<ul>
<li><p>Capitalization</p>
<ul>
<li>‘I showed Will my will, prepared by Green.’</li>
</ul></li>
<li><p>Prefixes and suffixes are helpful.</p>
<ul>
<li><p>‘Ungerplinked’</p></li>
<li><p>‘Flabertibly’</p></li>
<li><p>‘Skwerking’</p></li>
</ul></li>
<li><p>X-Y constructions are usually adjectives</p>
<ul>
<li><p>“New-found”</p></li>
<li><p>“46-year”</p></li>
<li><p>“Under-utilized”</p></li>
</ul></li>
</ul>
<hr />
<h3 id="but-words-come-in-sequences.-we-should-use-that">… but words
come in sequences. We should use that!</h3>
<hr />
<h1 id="hmm-based-pos-tagging">HMM-based POS Tagging</h1>
<hr />
<h2 id="hidden-markov-model">Hidden Markov Model</h2>
<p>A machine learning process which models a series of
<strong>observations</strong>, with the assumption that there’s some
‘hidden’ <strong>state</strong> which helps to predict the
observations</p>
<hr />
<h3 id="one-major-assumption-of-hmms">One major assumption of HMMs</h3>
<ul>
<li><p><strong>The probability of the current state is based ONLY on the
previous state</strong></p></li>
<li><p>The model does not have long term ‘memory’</p></li>
<li><p>The model cannot look ahead</p></li>
<li><p>This is a left-to-right walk through the data</p></li>
</ul>
<hr />
<h3 id="hmms-for-pos-tagging">HMMs for POS Tagging</h3>
<ul>
<li><p><strong>Observations:</strong> The series of words in the
text</p></li>
<li><p><strong>States:</strong> The parts of speech of those
words</p></li>
<li><p>‘Look at the sequence of words, to help predict which part of
speech corresponds to this word’</p></li>
</ul>
<hr />
<h3 id="how-do-we-use-hmms-for-pos-tagging">How do we use HMMs for
POS-tagging</h3>
<ul>
<li><p>1: Calculate the probabilities of parts-of-speech (and sequences)
from a corpus</p></li>
<li><p>2: Tokenize the input data</p></li>
<li><p>3: Using the input, decide the most likely sequence of
parts-of-speech</p></li>
</ul>
<hr />
<h3 id="we-need-to-know-two-types-of-probabilities">We need to know two
types of probabilities</h3>
<ul>
<li><p><strong>Observation probability:</strong> The probability that a
word has a given tag</p>
<ul>
<li>e.g. “How likely is”will” to be a modal verb?”</li>
</ul></li>
<li><p><strong>Transition Probability:</strong> The probability of one
POS, given the prior one</p>
<ul>
<li>e.g. “How likely is a modal verb following a pronoun?”</li>
</ul></li>
</ul>
<hr />
<h3 id="to-get-observation-probabilities">To get observation
probabilities…</h3>
<ul>
<li><p>Count the number of instances of “will” in the corpus</p></li>
<li><p>Count the number of times that it’s a modal verb</p></li>
<li><p>Count the number of times it’s a noun</p></li>
<li><p>Count the number of times it’s a proper noun</p></li>
<li><p>… and so on …</p></li>
<li><p>Turn these numbers into P(modal|will) (and so on)</p></li>
</ul>
<hr />
<h3
id="observation-probability-gets-at-the-idea-of-pos-ambiguity">Observation
probability gets at the idea of ‘POS Ambiguity’</h3>
<ul>
<li><p>Words that have little ambiguity will have high probabilities for
one category</p></li>
<li><p>Words that have lots of ambiguity may have nearly equivalent
probabilties across several categories</p></li>
</ul>
<hr />
<h3 id="to-get-transition-probabilities">To get Transition
probabilities…</h3>
<ul>
<li><p>Count the number of instances of modal in the corpus</p></li>
<li><p>Count the number of times modal follows pronoun</p></li>
<li><p>Count the number of times modal follows noun</p></li>
<li><p>Count the number of times modal follows verb</p></li>
<li><p>… and so on …</p></li>
<li><p>Turn these numbers into P(modal|Previous pronoun) (and so
on)</p></li>
</ul>
<hr />
<h3
id="transition-probabilities-get-at-the-idea-that-syntax-involves-sequences-of-word-types">Transition
probabilities get at the idea that syntax involves sequences of word
types</h3>
<ul>
<li><p>How likely is a Determiner to be followed by a Noun?</p>
<ul>
<li>REALLY likely</li>
</ul></li>
<li><p>How likely is a preposition to be followed by a determiner?</p>
<ul>
<li>Reasonably likely</li>
</ul></li>
<li><p>How likely is a preposition to be followed by a proper noun?</p>
<ul>
<li>Likely-ish</li>
</ul></li>
<li><p>How likely is a modal verb (e.g. ‘will’) to be followed by a
Noun?</p>
<ul>
<li>Really unlikely</li>
</ul></li>
</ul>
<hr />
<h3 id="now-we-know-the-probabilities">Now we know the
probabilities!</h3>
<ul>
<li><p>Then we tokenize</p></li>
<li><p>Then…</p></li>
</ul>
<hr />
<h3 id="we-decode-the-hmm">We decode the HMM</h3>
<ul>
<li><p>“Given this sequence of words, what’s the most likely sequence of
POS tags”</p></li>
<li><p>This uses the Viterbi Algorithm</p>
<ul>
<li>Which we’re not going into!</li>
</ul></li>
</ul>
<hr />
<h3 id="hmm-decoding-the-basic-idea">HMM Decoding: The Basic Idea</h3>
<ul>
<li><p>We know the probability of a given state (POS tag) given each
word</p></li>
<li><p>We know the probability of a given state (POS tag) given the
prior state (POS tag)</p></li>
<li><p>We can calculate the most probable state for each word in light
of those two facts</p></li>
<li><p><strong>What is the most likely string of states that gets us
through the entire sentence?</strong></p></li>
</ul>
<hr />
<p><img class="r-stretch" src="comp/pos_hmm_decoding.jpg"></p>
<hr />
<h3 id="so-we-have-the-most-likely-set-of-pos-tags">So, we have the
<em>most likely</em> set of POS tags</h3>
<ul>
<li><p>Both with respect to individual words’ probabilities</p></li>
<li><p>… and with respect to the likely sequence of tags</p></li>
<li><p>This gives us the best of both worlds!</p></li>
<li><p>Cool!</p></li>
</ul>
<hr />
<h3 id="one-consequence-of-hmm-based-tagging">One consequence of
HMM-based tagging</h3>
<ul>
<li>Word order matters!</li>
</ul>
<hr />
<blockquote>
<p>the/DT three/CD cute/JJ cats/NNS made/VBN will/MD sit/VB back/RP
in/IN awe/NN</p>
</blockquote>
<ul>
<li><blockquote>
<p>sit/VB cute/JJ three/CD awe/NN the/DT will/NN back/RB made/VBN in/IN
cats/NNS</p>
</blockquote></li>
<li><p>‘will’ goes from modal to noun</p></li>
<li><p>‘back’ goes from particle to adverb</p></li>
</ul>
<hr />
<h3 id="how-does-hmm-based-pos-tagging-perform">How does HMM-based POS
tagging perform?</h3>
<ul>
<li><p>Baseline (“Most Frequent Class”): ~92% accuracy</p></li>
<li><p>Hidden Markov Model POS Tagging: ~97% accuracy</p></li>
<li><p><strong>That’s pretty good!</strong></p></li>
<li><p>This is one of the ‘flagship’ tasks for HMMs</p>
<ul>
<li><p>Other approaches exist</p></li>
<li><p>Neural Networks didn’t win, for once!</p></li>
<li><p>(Well, OK, they might win by a few decimal places)</p></li>
</ul></li>
</ul>
<hr />
<p>… Why only 97% accuracy?</p>
<hr />
<h1 id="pos-tagging-is-hard">POS Tagging is hard</h1>
<hr />
<h2 id="use-mention-distinctions">Use-mention distinctions</h2>
<hr />
<h3 id="not-all-words-are-being-used-when-being-used">Not all words are
being used, when being used</h3>
<ul>
<li><p>You can have words that show up in uninformative
contexts</p></li>
<li><p>Words that are being mentioned, rather than used, are hard to
POS-tag</p></li>
</ul>
<hr />
<h3 id="she-said-bear-was-her-favorite-word.">‘She said ’bear’ was her
favorite word.’</h3>
<ul>
<li><blockquote>
<p>She/PRP said/VBD `/`` bear/NN ‘/’’ was/VBD her/PRP$ favorite/JJ
word/NN ./.</p>
</blockquote></li>
</ul>
<hr />
<h3 id="roger-texted-me-back">‘Roger texted me ’back’’</h3>
<ul>
<li><blockquote>
<p>Roger/NNP texted/VBD me/PRP `/`` back/VBP ‘/’’</p>
</blockquote></li>
</ul>
<hr />
<h3 id="i-bought-the-the-pianist-dvd">‘I bought the The Pianist
DVD’</h3>
<ul>
<li><blockquote>
<p>I/PRP bought/VBD the/DT The/NNP Pianist/NNP DVD/NN</p>
</blockquote></li>
<li><blockquote>
<p>I/PRP bought/VBD the/DT the/DT pianist/NN DVD/NN</p>
</blockquote></li>
</ul>
<hr />
<h2 id="ambiguous-sentences">Ambiguous Sentences</h2>
<hr />
<h3 id="some-sentences-are-actually-ambiguous-in-pos-tagging">Some
sentences are actually ambiguous in POS tagging</h3>
<ul>
<li>Not all ambiguities of POS are resolveable by humans</li>
</ul>
<hr />
<h3 id="maria-was-entertaining-last-night">‘Maria was entertaining last
night’</h3>
<ul>
<li><blockquote>
<p>Maria/NNP was/VBD entertaining/JJ last/JJ night/NN</p>
</blockquote></li>
</ul>
<hr />
<h3 id="i-saw-the-official-take-from-the-store.">‘I saw the official
take from the store.’</h3>
<ul>
<li><blockquote>
<p>I/PRP saw/VBD the/DT official/NN take/VBP from/IN the/DT store/NN
./.</p>
</blockquote></li>
</ul>
<hr />
<h3 id="you-should-ask-a-smith.">‘You should ask a Smith.’</h3>
<ul>
<li><blockquote>
<p>You/PRP should/MD ask/VB a/DT Smith/NNP ./.</p>
</blockquote></li>
</ul>
<hr />
<h3 id="i-hate-bridging-gaps.">‘I hate bridging gaps.’</h3>
<ul>
<li><blockquote>
<p>I/PRP hate/VBP bridging/VBG gaps/NNS ./.</p>
</blockquote></li>
</ul>
<hr />
<h2 id="rare-or-unknown-words">Rare or Unknown words</h2>
<hr />
<h3 id="rare-or-unknown-words-1">Rare or unknown Words</h3>
<ul>
<li><p>Capitalization and Morphology are the best tools</p></li>
<li><p>You can rely mostly on the transitional probability within the
model</p>
<ul>
<li>“Well, I know that the last thing was a modal ‘will’, so
‘gerfleeble’ is probably a verb!”</li>
</ul></li>
</ul>
<hr />
<h3 id="yeet">‘yeet’</h3>
<ul>
<li><blockquote>
<p>yeet/NN</p>
</blockquote></li>
</ul>
<hr />
<h3 id="yeeting">‘yeeting’</h3>
<ul>
<li><blockquote>
<p>yeeting/NN</p>
</blockquote></li>
</ul>
<hr />
<h3 id="yeeted">‘yeeted’</h3>
<ul>
<li><blockquote>
<p>yeeted/JJ</p>
</blockquote></li>
</ul>
<hr />
<h3 id="i-yeet-when-i-throw-empty-cans">‘I yeet when I throw empty
cans’</h3>
<ul>
<li><blockquote>
<p>I/PRP yeet/VBP when/WRB I/PRP throw/VBP empty/JJ cans/NNS</p>
</blockquote></li>
</ul>
<hr />
<h3 id="lit">‘lit’</h3>
<ul>
<li><blockquote>
<p>lit/UH</p>
</blockquote></li>
</ul>
<hr />
<h3 id="that-phonetics-lab-meeting-was-lit">‘That phonetics lab meeting
was lit’</h3>
<ul>
<li><blockquote>
<p>That/DT phonetics/NNS lab/NN meeting/NN was/VBD lit/JJ</p>
</blockquote></li>
</ul>
<hr />
<h3 id="im-studying-english-lit">‘I’m studying English Lit’</h3>
<ul>
<li><blockquote>
<p>I/PRP ’m/VBP studying/VBG English/NNP Lit/NNP</p>
</blockquote></li>
</ul>
<hr />
<h3 id="they-lit-the-beacon-of-amon-din-to-summon-the-rohirrim">‘They
lit the beacon of Amon Din to summon the Rohirrim’</h3>
<ul>
<li><blockquote>
<p>They/PRP lit/VBD the/DT beacon/NN of/IN Amon/NNP Din/NNP to/TO
summon/VB the/DT Rohirrim/NNP</p>
</blockquote></li>
</ul>
<hr />
<h2 id="homonyms">Homonyms</h2>
<hr />
<h3 id="homonyms-are-always-a-problem">Homonyms are (always) a
problem</h3>
<ul>
<li>Is ‘saw’ a past tense verb, a noun, or a present tense verb?</li>
</ul>
<hr />
<h3 id="i-saw-the-sign">‘I saw the sign’</h3>
<ul>
<li><blockquote>
<p>I/PRP saw/VBD the/DT sign/NN</p>
</blockquote></li>
</ul>
<hr />
<h3
id="i-saw-the-sign-whenever-i-need-to-test-the-cutting-feel-of-a-new-blade">‘I
saw the sign whenever I need to test the cutting feel of a new
blade’</h3>
<ul>
<li><blockquote>
<p>I/PRP saw/VBD the/DT sign/NN whenever/WRB I/PRP need/VBP to/TO
test/VB the/DT cutting/VBG feel/NN of/IN a/DT new/JJ blade/NN</p>
</blockquote></li>
</ul>
<hr />
<h3 id="i-bought-a-saw">‘I bought a saw’</h3>
<ul>
<li><blockquote>
<p>I/PRP bought/VBD a/DT saw/NN</p>
</blockquote></li>
</ul>
<hr />
<h1 id="pos-tagging-is-crucial">POS Tagging is crucial</h1>
<hr />
<h3 id="pos-tagging-is-very-helpful">POS Tagging is very helpful</h3>
<ul>
<li><p>Helps disambiguate word senses</p></li>
<li><p>Helps identify verbs vs. the things the verbs are acting
on</p></li>
<li><p>Provides the basis for syntactic parsing!</p></li>
</ul>
<hr />
<h3 id="wrapping-up">Wrapping up</h3>
<ul>
<li><p>Computers can’t use meaning or language intuitions to
POS-tag</p></li>
<li><p>POS-tagged data is valuable</p></li>
<li><p>Words can be more or less ambiguous in terms of POS tags</p></li>
<li><p>HMMs work great for POS Tagging</p></li>
<li><p>But POS tagging is still hard!</p></li>
</ul>
<hr />
<h3 id="for-next-time">For Next Time</h3>
<ul>
<li>We’ll talk about syntactic parsing</li>
</ul>
<hr />
<p><huge>Thank you!</huge></p>
</body>
</html>
