<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title></title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <style type="text/css">
  /*
   * I add this to html files generated with pandoc.
   * Originally from https://gist.github.com/killercup/5917178
   */

  html {
      overflow-y: scroll;
      -webkit-text-size-adjust: 100%;
      -ms-text-size-adjust: 100%;
  }

  body {
      color: #444;
      font-family: "Source Sans 3", Helvetica-Neue, Helvetica, Sans;
      line-height: 1.5;
      padding: 0.5em;
      margin: auto;
      max-width: 55em;
      background: #fefefe;
  }

  a {
      color: #2171b5;
      text-decoration: underline;
  }

  tr:nth-child(even) {background: #F8F8F8}
  tr:nth-child(odd) {background: #FFF}

  a:visited {
      color: #2171b5;
      text-decoration: none;
  }

  a:focus {
      outline: thin dotted;
  }

  *::-moz-selection {
      background: rgba(255, 255, 0, 0.3);
      color: #000;
  }

  *::selection {
      background: rgba(255, 255, 0, 0.3);
      color: #000;
  }

  a::-moz-selection {
      background: rgba(255, 255, 0, 0.3);
      color: #0645ad;
  }

  a::selection {
      background: rgba(255, 255, 0, 0.3);
      color: #0645ad;
  }

  p {
      margin: 0.75em 0;
  }

  img {
      max-width: 60%;
      max-height:400px;
  }

  video {
      max-width: 60%;
  }


  h1, h2, h3, h4, h5, h6 {
      color: #111;
      line-height: 80%;
      margin-top: 1em;
      margin-bottom: 0.5em;
      font-weight: normal;
  }

  h1, h2, h3, h4, h5, h6 {
      font-weight: bold;
  }

  h1 {
      font-size: 2em;
      line-height: 1.25;
      color:  #084594;

  }

  h1.title {
      margin-top:0.2em;
      font-size: 2em;
      line-height: 1.25;
  }

  h2 {
      font-size: 1.5em;
      line-height: 1.6em;
          color:  #084594;
      padding-bottom: 3px;

  }

  h3 {
      font-size: 1.2em;
      line-height: 1.6em;
  }


  h4 {
      font-size: 1.2em;
      line-height: 1.4em;
  }

  h5 {
      font-size: 1em;
  }

  h6 {
      font-size: 0.9em;
  }

  blockquote {
      color: #666666;
      margin: 0;
      padding-left: 3em;
      border-left: 0.5em #EEE solid;
  }

  hr {
      display: block;
      height: 2px;
      border: 0;
      border-top: 1px solid #aaa;
      border-bottom: 1px solid #eee;
      margin: 0.5em 0;
      padding: 0;
  }

  pre, code, kbd, samp {
      color: #000;
      font-family: monospace, monospace;
      _font-family: 'courier new', monospace;
      font-size: 0.98em;
  }

  pre {
      white-space: pre;
      white-space: pre-wrap;
      word-wrap: break-word;
  }

  .answer {
      color:#CC0033;
      font-style:italic;
  }

  b, strong {
      font-weight: bold;
  }

  dfn {
      font-style: italic;
  }

  ins {
      background: #ff9;
      color: #000;
      text-decoration: none;
  }

  mark {
      background: #ff0;
      color: #000;
      font-style: italic;
      font-weight: bold;
  }

  sub, sup {
      font-size: 75%;
      line-height: 0;
      position: relative;
      vertical-align: baseline;
  }

  sup {
      top: -0.5em;
  }

  sub {
      bottom: -0.25em;
  }

  ul, ol {
      margin: 0.5em 0;
      padding: 0em 0em 0em 1em;
  }

  ul img {
      list-style-type: none;
  }

  li p:last-child {
      margin-bottom: 0;
  }

  hr {
      border-top:none;
      height:0px;
      clear:both;
  }

  ul ul, ol ol {
      margin: .3em 0;
  }

  dl {
      margin-bottom: 1em;
  }

  dt {
      font-weight: bold;
      margin-bottom: .8em;
  }

  dd {
      margin: 0 0 .8em 2em;
  }

  dd:last-child {
      margin-bottom: 0;
  }

  img {
      border: 0;
      -ms-interpolation-mode: bicubic;
      vertical-align: middle;
  }

  figure {
      display: block;
      text-align: center;
      margin: 1em 0;
  }

  figure img {
      border: none;
      margin: 0 auto;
  }

  figcaption {
      font-size: 0.8em;
      font-style: italic;
      margin: 0 0 .8em;
  }

  table {
      margin-bottom: 2em;
      border-bottom: 1px solid #ddd;
      border-right: 1px solid #ddd;
      border-spacing: 0;
      border-collapse: collapse;
  }

  table th {
      padding: .2em 1em;
      background-color: #eee;
      border-top: 1px solid #ddd;
      border-left: 1px solid #ddd;
  }

  table td {
      padding: .2em 1em;
      border-top: 1px solid #ddd;
      border-left: 1px solid #ddd;
      vertical-align: top;
  }

  .author {
      font-size: 1.2em;
      text-align: center;
  }

  @media only screen and (min-width: 480px) {
      body {
  	font-size: 14px;
      }
  }
  @media only screen and (min-width: 768px) {
      body {
  	font-size: 16px;
      }
  }
  @media print {
      * {
  	background: transparent !important;
  	color: black !important;
  	filter: none !important;
  	-ms-filter: none !important;
      }

      body {
  	font-size: 12pt;
  	max-width: 100%;
      }

      a, a:visited {
  	text-decoration: underline;
      }

      hr {
  	height: 1px;
  	border: 0;
  	border-bottom: 1px solid black;
      }

      a[href]:after {
  	content: " (" attr(href) ")";
      }

      abbr[title]:after {
  	content: " (" attr(title) ")";
      }

      .ir a:after, a[href^="javascript:"]:after, a[href^="#"]:after {
  	content: "";
      }

      pre, blockquote {
  	border: 1px solid #999;
  	padding-right: 1em;
  	page-break-inside: avoid;
      }

      tr, img {
  	page-break-inside: avoid;
      }

      img {
  	max-width: 40% !important;
      max-height: 300px !important;
      }

      @page :left {
  	margin: 15mm 20mm 15mm 10mm;
      }

      @page :right {
  	margin: 15mm 10mm 15mm 20mm;
      }

      p, h2, h3 {
  	orphans: 3;
  	widows: 3;
      }

      h2, h3 {
  	page-break-after: avoid;
      }
  }


  ldata {
  	font-size: 0.7em;
  	margin-bottom: 0em;
  	color:#808080;
  	font-style:italic;
  }

  danger {
  	color:#FF0000;
  	font-weight:bold;
  }

  correct {
  	color:#39C900;
  	font-weight:bold;
  }

  clg{
      color:#39C900;
  	font-weight:bold;
  }

  clr{
  	color:#FF0000;
  	font-weight:bold;
  }

  clb{
  	color:#0000CC;
  	font-weight:bold;
  }

  clp{
  	color:#6600FF;
  	font-weight:bold;
  }

  clk{
  	color:#708cef;
  	font-weight:bold;
  }

  clo{
  	color:#CC6600;
  	font-weight:bold;
  }

  sc{
          font-variant: small-caps;
  }

  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="chalk-talk">Chalk Talk</h1>
<h2 id="will-styler">Will Styler</h2>
<p>UC San Diego</p>
<hr />
<h3 id="the-linguistic-signal-is-incredibly-complex">The linguistic
signal is incredibly complex</h3>
<ul>
<li><p>Natural language text is complex</p></li>
<li><p>Speech is complex</p></li>
<li><p>It reflects and describes a complex world</p></li>
<li><p>… yet, somehow, we’re able to consistenly get relevant
information from person to person using language</p></li>
</ul>
<hr />
<h3 id="the-signal-must-be-carrying-contrast">The signal must be
carrying contrast</h3>
<ul>
<li><p>Some feature (or group of features) allows us to recover the
difference between ..</p></li>
<li><p>Multiple phonemes</p></li>
<li><p>Different temporal orderings</p></li>
<li><p>Many possible social identities</p></li>
<li><p>Two closely related meanings</p></li>
<li><p><strong>There are many possible features which could be
signalling any given contrast</strong></p></li>
</ul>
<hr />
<h3 id="my-research-has-focused-on-this-problem">My research has focused
on this problem</h3>
<ul>
<li><p>“I’m drowning in possible features that could signal what I’m
studying.”</p></li>
<li><p>“<strong>How can we figure out which ones humans are actually
using?</strong>”</p>
<ul>
<li>(Without requiring 15 years and a series of improbably large
grants)</li>
</ul></li>
</ul>
<hr />
<h3 id="there-are-many-approaches-to-this-problem">There are many
approaches to this problem</h3>
<ul>
<li><p>15 years and a series of improbably large grants (to test them
all with humans)</p></li>
<li><p>Educated guesswork to narrow the field</p></li>
<li><p>Conventional Statistical analysis</p>
<ul>
<li>… and my personal favorite</li>
</ul></li>
<li><p><strong>Machine Learning, <em>then</em> humans!</strong></p></li>
</ul>
<hr />
<h3 id="todays-case-study-vowel-nasality">Today’s Case Study: Vowel
Nasality</h3>
<ul>
<li>Vowel Nasality is the opening of the Velopharyngeal port during the
vowel <!-- .element: class="fragment" --></li>
</ul>
<center>
<!-- .element: class="fragment" -->
<table>
<tr>
<th>
‘Cat’<br>[kæt]
</th>
<th>
‘Can’t’<br>[kæ̃nt]
</th>
</tr>
</table>
</center>
<ul>
<li>What are the acoustic cues used for perceiving vowel nasality in
English? <!-- .element: class="fragment" --></li>
</ul>
<hr />
<h2 id="the-problem">The Problem</h2>
<p><img src="phonmedia/styler2017_figure1.jpg">
<!-- .element: class="fragment" --></p>
<p><small>Figure from Styler 2017</small>
<!-- .element: class="fragment" --></p>
<hr />
<h3 id="feature-selection-is-half-the-battle">Feature selection is half
the battle</h3>
<ul>
<li><p>In a perfect world, you would test all possible features as
potential cues</p></li>
<li><p>Some <em>a priori</em> choices must be made to reduce the feature
space</p></li>
<li><p>… but every time you exclude a feature, you run the risk of
excluding <em>the</em> feature</p></li>
</ul>
<hr />
<h3 id="potential-cues-for-evaluation">29 Potential Cues for
evaluation</h3>
<ul>
<li><p>All spectral or temporal features in the signal</p></li>
<li><p>Some absolute, some relative</p></li>
<li><p>Features like…</p>
<ul>
<li>Formant Frequencies and Bandwidths</li>
<li>Spectral Relationships (like A1-P0 or A3-P0)</li>
<li>Nasal Peaks and Zeroes</li>
<li>Spectral Tilt</li>
<li>Vowel Duration</li>
<li>… and more!</li>
</ul></li>
</ul>
<hr />
<h3 id="the-other-problem">The <em>other</em> problem</h3>
<hr />
<h3 id="humans-are-troublesome">Humans are troublesome</h3>
<ul>
<li><p>Human responses aren’t very transparent</p></li>
<li><p>Subtle changes often produce subtle differences in
response</p></li>
<li><p>Observations are not independent</p></li>
<li><p>Participants have different language and knowledge
backgrounds</p></li>
<li><p>Participants have limited endurance</p>
<ul>
<li>Especially with boring tasks</li>
</ul></li>
<li><p>Running human experiments is… non-trivial</p>
<ul>
<li>… and really, really expensive</li>
</ul></li>
</ul>
<hr />
<ul>
<li>So, instead of asking humans to evaluate all 29, let’s use…</li>
</ul>
<hr />
<h2 id="machine-speech-perception">Machine Speech Perception</h2>
<table style="width:6%;">
<colgroup>
<col style="width: 5%" />
</colgroup>
<tbody>
<tr class="odd">
<td>### The Basic Idea</td>
</tr>
<tr class="even">
<td>Human speech perception is just classifying sounds based on
acoustical features</td>
</tr>
<tr class="odd">
<td>* <strong>Computers can do that too!</strong></td>
</tr>
<tr class="even">
<td>* Give the acoustic feature information to a classifier and ask for
oral vs. nasal judgements</td>
</tr>
<tr class="odd">
<td>* Greater accuracy means a feature or grouping is more useful and
informative!</td>
</tr>
</tbody>
</table>
<h3 id="computers-are-not-humans">Computers are not humans</h3>
<p><img src="img/lab_rat.jpg"> <!-- .element: class="fragment" --></p>
<hr />
<h3 id="they-have-some-serious-advantages-for-studying-language">They
have some serious advantages for studying language!</h3>
<ul>
<li><p>Their decisions are easier to quantify than humans’</p></li>
<li><p>They’ll (often) tell you <em>how</em> they made the decision they
did</p></li>
<li><p>They have no knowledge that you don’t give to them</p></li>
<li><p>They make all decisions independently</p></li>
<li><p>They don’t require payment or scheduling</p></li>
<li><p>They’re available 24/7</p></li>
</ul>
<hr />
<h3 id="supervised-machine-classification-101">Supervised Machine
Classification 101</h3>
<ul>
<li><p>Select a large corpus of data, and manually assign each
observation to a group</p></li>
<li><p><strong>Training:</strong> Feed this labeled data into an
algorithm so it can learn the patterns</p></li>
<li><p><strong>Testing:</strong> Give the trained algorithm new data
without labels, and check the accuracy of its classifications</p></li>
<li><p>Better accuracy often indicates more useful information was given
to the classifier!</p></li>
</ul>
<hr />
<h3 id="machine-classification-is-everywhere">Machine Classification is
<em>everywhere</em></h3>
<hr />
<h3 id="my-approach">My approach</h3>
<ul>
<li><ol type="1">
<li>Collect a corpus of oral and nasal words, and measure each
feature</li>
</ol></li>
<li><ol start="2" type="1">
<li>Give each feature to a Machine Learning Algorithm individually</li>
</ol>
<ul>
<li>The most informative features should be the most accurate</li>
</ul></li>
<li><ol start="3" type="1">
<li>Find the best group of features</li>
</ol>
<ul>
<li>Find the balance between “few features” and “good accuracy”</li>
</ul></li>
<li><ol start="4" type="1">
<li>Test <em>those</em> features with expensive and difficult
humans</li>
</ol></li>
</ul>
<hr />
<h3 id="labeling-and-training">Labeling and Training</h3>
<ul>
<li><p>Recorded 12 English speakers making words with oral and
nasal(ized) vowels</p>
<ul>
<li><p>“Oral” vowels were in CVC contexts, and “Nasal” were in
CVN/NVC/NVN contexts</p></li>
<li><p>This resulted in 3823 words</p></li>
</ul></li>
<li><p>Then, I measured each of the 29 features at two timepoints per
vowel</p>
<ul>
<li>All measurement was done automatically by Praat Script</li>
</ul></li>
<li><p>Then I handed them to a Support Vector Machine as training
data</p></li>
</ul>
<hr />
<h3 id="support-vector-machines">Support Vector Machines</h3>
<ul>
<li><p>A very common, very accurate machine learning algorithm</p></li>
<li><p>Look at all the data in an multi-dimensional space</p>
<ul>
<li>As many dimensions as features</li>
</ul></li>
<li><p>Try to find a line or hyperplane that optimally** separates the
classes</p></li>
<li><p>Classification is just seeing where the new data is relative to
that line</p></li>
</ul>
<hr />
<p>(There are other algorithms that can work well, too!)</p>
<hr />
<h2 id="so-how-does-it-perform-with-nasality"><strong>So how does it
perform with nasality?</strong></h2>
<hr />
<h3 id="single-feature-testing">Single-Feature testing</h3>
<ul>
<li><p>Are any features good enough <em>on their own</em> to allow nasal
perception?</p></li>
<li><p>29 separate models (one per feature) classifying datapoints as
“oral” or “nasal”</p></li>
<li><p>Each model outputs accuracy figures, which we can
compare!</p></li>
</ul>
<hr />
<h3 id="single-feature-findings">Single-feature findings</h3>
<ul>
<li><p>F1’s Bandwidth is the most useful and informative feature</p>
<ul>
<li>67.6% SVM accuracy</li>
</ul></li>
<li><p>A1-P0, a measure of relative spectral prominence, gets second
place</p>
<ul>
<li>64.7% SVM accuracy</li>
</ul></li>
<li><p>The worst feature performed at 51.23% accuracy</p></li>
<li><p><em>None of the features are good enough on their
own!</em></p></li>
</ul>
<hr />
<h2 id="what-group-of-features-provides-the-best-information">What
<em>group of features</em> provides the best information?</h2>
<hr />
<h3 id="multi-feature-modeling">Multi-feature modeling</h3>
<ul>
<li>Tested 10 <em>a priori</em> feature groupings
<ul>
<li>Selected from various outputs of the machine learning and
statistics</li>
</ul></li>
<li>Compared the accuracy <em>in light of the number of features</em>
<ul>
<li>The winning model gets the best performance from the fewest
features</li>
</ul></li>
</ul>
<hr />
<h3 id="multi-feature-results">Multi-feature Results</h3>
<ul>
<li><p>SVMs with all features worked best (29 features)</p>
<ul>
<li>84.7% accuracy</li>
</ul></li>
<li><p>Formant Frequency and Bandwidth, Spectral Tilt, A1-P0, and Vowel
Duration was the best subgroup (5 features)</p>
<ul>
<li>82.2% accuracy</li>
</ul></li>
<li><p><strong>We only lose 2.5% accuracy when we reduce our feature set
by 69%!</strong></p></li>
</ul>
<hr />
<h3 id="overall-machine-learning-results">Overall Machine Learning
Results</h3>
<ul>
<li><p><strong>Formant Bandwidth</strong> was the most useful single
feature for English (62.5% accuracy)</p></li>
<li><p>… and we’ve got a multi-feature grouping with very good accuracy
(82.2% accuracy)!</p>
<ul>
<li>Formant Width, Formant Frequency, Spectal Tilt, A1-P0, and
Duration</li>
</ul></li>
<li><p><strong>So, let’s test those five features with actual
humans!</strong></p></li>
</ul>
<hr />
<h1 id="human-perception">Human Perception</h1>
<hr />
<h3 id="methods">Methods</h3>
<ul>
<li><p>English listeners can use vowel nasality to identify missing
nasal consonants</p>
<ul>
<li>ba_ could be “bad” or “ban”</li>
</ul></li>
<li><p><strong>Let’s add or remove features from vowels to see what
indicates “nasality”!</strong></p></li>
<li><p>If adding or removing a feature changes perception, or makes them
react more slowly, it’s important!</p></li>
</ul>
<hr />
<h3 id="the-modifications">The Modifications</h3>
<p>Use signal processing to simulate the oral-to-nasal change (or vice
versa) in…</p>
<ul>
<li><ol type="1">
<li>A1-P0 (or vice versa)</li>
</ol></li>
<li><ol start="2" type="1">
<li>Duration</li>
</ol></li>
<li><ol start="3" type="1">
<li>Spectral Tilt</li>
</ol></li>
<li><ol start="4" type="1">
<li>Formant Bandwidth and Frequency</li>
</ol>
<ul>
<li>Combined</li>
</ul></li>
<li><ol start="5" type="1">
<li>Modify <em>all five features at once!</em></li>
</ol></li>
</ul>
<hr />
<h3 id="the-experiment">The Experiment</h3>
<ul>
<li><p>Recruited 42 normal-hearing Native English speakers from a
department subject pool</p></li>
<li><p>Each listened to 400 words with different modifications</p></li>
<li><p>Analyzed both confusion and reaction time associated with
stimulus changes</p></li>
</ul>
<hr />
<center>
<table>
<tr>
<th>
<h1>
bad
</h1>
</th>
<th>
<h1>
ban
</h1>
</th>
</tr>
</table>
</center>
<audio controls>
<source src="phonmedia/diss_hazel_BAD_nfor_ex_c.wav" type="audio/wav">
</audio>
<hr />
<center>
<table>
<tr>
<th>
<h1>
bomb
</h1>
</th>
<th>
<h1>
bob
</h1>
</th>
</tr>
</table>
</center>
<audio controls>
<source src="phonmedia/diss_molly_BOMB_ofor_ex_c.wav" type="audio/wav">
</audio>
<hr />
<h3 id="human-perception-summary">Human Perception Summary</h3>
<ul>
<li><p>Only <strong>formant modification</strong> had a significant
effect on perception</p></li>
<li><p>Formant modification caused listeners to respond more
slowly</p></li>
<li><p>Formant modification made oral vowels sound “nasal”</p></li>
<li><p>F1’s bandwidth is probably the cue</p>
<ul>
<li>This makes sense acoustically, and Hawkins and Stevens (1985) also
points in that direction</li>
</ul></li>
</ul>
<hr />
<h3 id="score-one-for-the-machine">Score one for the Machine!</h3>
<ul>
<li><p>The machine learning models predicted F1’s bandwidth as the most
useful feature…</p></li>
<li><p>… and the humans agreed!</p></li>
<li><h3 id="how-similar-are-the-svms-and-the-humans">How similar
<em>are</em> the SVMs and the humans?</h3></li>
</ul>
<hr />
<p><em>Let’s give the computer the same experimental task as the humans,
using the same altered stimuli, and see how they compare!</em></p>
<hr />
<h3 id="testing-humans-vs.-machines">Testing Humans vs. Machines</h3>
<ul>
<li><ol type="1">
<li>Train an SVM on all of the English Data</li>
</ol></li>
<li><ol start="2" type="1">
<li>Extract acoustic features from the stimuli used in the
experiment</li>
</ol></li>
<li><ol start="3" type="1">
<li>Test those SVMs using the experimental stimuli data</li>
</ol>
<ul>
<li>Again classifying “oral” or “nasal”</li>
</ul></li>
<li><ol start="4" type="1">
<li>Compare the by-condition confusion results to the humans</li>
</ol></li>
</ul>
<hr />
<h3 id="confusion-by-condition">Confusion by Condition</h3>
<p><img src="img/diss_stimml_human_vs_machine_ex_mod.png"></p>
<hr />
<h3 id="humans-vs.-machines-summary">Humans vs. Machines Summary</h3>
<ul>
<li><p>Humans and machines <em>did</em> show similar patterns</p>
<ul>
<li>Modifications that were difficult for humans were difficult for
SVMs</li>
</ul></li>
<li><p>Humans are still more accurate overall</p></li>
</ul>
<hr />
<h3 id="the-svms-didnt-model-the-humans-exactly">The SVMs didn’t model
the humans exactly!</h3>
<ul>
<li><p>SVMs predicted gradient usefulness of the features</p>
<ul>
<li>Humans based their decisions entirely on F1’s Bandwidth</li>
</ul></li>
<li><p>SVMs showed greater accuracy when all features were available</p>
<ul>
<li>Humans weren’t meaningfully affected by the additional three
features</li>
</ul></li>
<li><p>So, SVMs can show relative informativeness of features</p>
<ul>
<li>… but they can’t show what humans actually do use</li>
</ul></li>
</ul>
<hr />
<h2 id="conclusion">Conclusion</h2>
<hr />
<h3 id="what-did-using-machine-learning-win-us">What did using machine
learning win us?</h3>
<ul>
<li><p>The SVM studies very effectively narrowed the field</p></li>
<li><p>The SVM studies and the humans both agreed on the best
feature</p></li>
<li><p>Trained SVMs were able to perform the same experiment, with
similar results</p></li>
<li><p><strong>Modeling human language using machine learning is
helpful!</strong></p></li>
</ul>
<hr />
<h3 id="machine-learning-is-widely-applicable">Machine learning is
widely applicable</h3>
<ul>
<li><p>Computational Linguistics loves it</p></li>
<li><p>Modeling gestural data to identify discrete gestures rather than
interpolation</p></li>
<li><p>Modeling the time course of speech perception with machine
learning</p></li>
<li><p>Neural networks for finding tongue shapes in ultrasound data
(Jian Zhu, cf. ASA 2018)</p></li>
<li><p>Classifiying athletes as ‘white’ or ‘black’ based on media
portrayal (Kelly Wright)</p></li>
<li><p>… and there are always more rich signals</p></li>
</ul>
<hr />
<h2
id="any-hypothesis-about-human-language-needs-to-be-tested-with-human-speakers">Any
hypothesis about human language needs to be tested with human
speakers</h2>
<ul>
<li>… but sometimes, it’s a good idea to trust the machines!</li>
</ul>
<hr />
<h3 id="just-be-careful">(Just be careful)</h3>
<p><img src="img/hal_eye.jpg"> <!-- .element: class="fragment" --></p>
<hr />
<h1 id="lets-talk">Let’s talk!</h1>
<hr />
<h2 id="acknowledgements">Acknowledgements</h2>
<ul>
<li><p>The speakers and listeners who participated in the study</p></li>
<li><p>The great many electrons inconvenienced in the process of
building these SVMs</p></li>
<li><p>The University of Colorado at Boulder and Dr. Rebecca
Scarborough</p></li>
<li><p>The University of Michigan for the support and training, and the
Michigan Phondi Group</p></li>
</ul>
<hr />
<h3 id="references">References</h3>
<ul>
<li><p>Chen, M. Y. (1997). Acoustic correlates of english and french
nasalized vowels. The Journal of the Acoustical Society of America,
102(4):2350–2370.</p></li>
<li><p>Hawkins, S. and Stevens, K. N. (1985b). Acoustic and perceptual
correlates of the non-nasal–nasal distinction for vowels. The Journal of
the Acoustical Society of America, 77(4):1560–1575.</p></li>
<li><h2
id="styler-w.-2015-on-the-acoustical-and-perceptual-features-of-vowel-nasality.-phd-thesis-university-of-colorado-at-boulder-march-2015.">Styler,
W. (2015) On the Acoustical and Perceptual Features of Vowel Nasality.
PhD thesis, University of Colorado at Boulder, March 2015.</h2></li>
</ul>
<h1 id="additional-information">Additional Information</h1>
<hr />
<h3 id="feature-list">Feature List</h3>
<p><img class="big" src="img/diss_featurelist.png"></p>
<hr />
<h3 id="single-feature-models">Single-Feature Models</h3>
<p><img class="big" src="img/diss_ensvm_only.png"></p>
<hr />
<h3 id="multi-feature-machine-learning-results">Multi-feature Machine
Learning Results</h3>
<p><img src="img/diss_multifeature.png"></p>
<hr />
<h3 id="feature-importance">Feature Importance</h3>
<ul>
<li><img src="img/diss_enfrimportance.png"></li>
</ul>
<hr />
<h3 id="feature-addition-oral-made-nasal-findings">Feature Addition
(oral-made-nasal) Findings</h3>
<p><img src="img/diss_conf.add.sum.png"></p>
<ul>
<li><p><em>Modifying formants (or all together) resulted in more
confusion!</em></p>
<ul>
<li><p>People called oral vowels “nasal” more often with modified
formants</p></li>
<li><p>The pattern of the All-Modified stimuli was statistically
similar.</p></li>
</ul></li>
</ul>
<table style="width:6%;">
<colgroup>
<col style="width: 5%" />
</colgroup>
<thead>
<tr class="header">
<th><img src="img/diss_rt.add.sum.png"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>## Feature Reduction (Nasal-made-Oral) Findings</td>
</tr>
</tbody>
</table>
<p><img src="img/diss_conf.rem.sum.png"></p>
<ul>
<li><p>Confusion wasn’t affected by modificaton for nasal-to-oral
stimuli!</p></li>
<li><p><strong>We never changed “nasal” to “oral” by modifying
features</strong></p></li>
</ul>
<table style="width:6%;">
<colgroup>
<col style="width: 5%" />
</colgroup>
<tbody>
<tr class="odd">
<td><img src="img/diss_rt.rem.sum.png"></td>
</tr>
<tr class="even">
<td>* <em>Modifying formants (or all features) resulted in slower
reaction times!</em></td>
</tr>
<tr class="odd">
<td>* People were slower to call vowels “oral” or “nasal” with modified
formants</td>
</tr>
</tbody>
</table>
</body>
</html>
