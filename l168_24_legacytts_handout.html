<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title></title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <style type="text/css">
  /*
   * I add this to html files generated with pandoc.
   * Originally from https://gist.github.com/killercup/5917178
   */

  html {
      overflow-y: scroll;
      -webkit-text-size-adjust: 100%;
      -ms-text-size-adjust: 100%;
  }

  body {
      color: #444;
      font-family: "Source Sans 3", Helvetica-Neue, Helvetica, Sans;
      line-height: 1.5;
      padding: 0.5em;
      margin: auto;
      max-width: 55em;
      background: #fefefe;
  }

  a {
      color: #2171b5;
      text-decoration: underline;
  }

  tr:nth-child(even) {background: #F8F8F8}
  tr:nth-child(odd) {background: #FFF}

  a:visited {
      color: #2171b5;
      text-decoration: none;
  }

  a:focus {
      outline: thin dotted;
  }

  *::-moz-selection {
      background: rgba(255, 255, 0, 0.3);
      color: #000;
  }

  *::selection {
      background: rgba(255, 255, 0, 0.3);
      color: #000;
  }

  a::-moz-selection {
      background: rgba(255, 255, 0, 0.3);
      color: #0645ad;
  }

  a::selection {
      background: rgba(255, 255, 0, 0.3);
      color: #0645ad;
  }

  p {
      margin: 0.75em 0;
  }

  img {
      max-width: 60%;
      max-height:400px;
  }

  video {
      max-width: 60%;
  }


  h1, h2, h3, h4, h5, h6 {
      color: #111;
      line-height: 80%;
      margin-top: 1em;
      margin-bottom: 0.5em;
      font-weight: normal;
  }

  h1, h2, h3, h4, h5, h6 {
      font-weight: bold;
  }

  h1 {
      font-size: 2em;
      line-height: 1.25;
      color:  #084594;

  }

  h1.title {
      margin-top:0.2em;
      font-size: 2em;
      line-height: 1.25;
  }

  h2 {
      font-size: 1.5em;
      line-height: 1.6em;
          color:  #084594;
      padding-bottom: 3px;

  }

  h3 {
      font-size: 1.2em;
      line-height: 1.6em;
  }


  h4 {
      font-size: 1.2em;
      line-height: 1.4em;
  }

  h5 {
      font-size: 1em;
  }

  h6 {
      font-size: 0.9em;
  }

  blockquote {
      color: #666666;
      margin: 0;
      padding-left: 3em;
      border-left: 0.5em #EEE solid;
  }

  hr {
      display: block;
      height: 2px;
      border: 0;
      border-top: 1px solid #aaa;
      border-bottom: 1px solid #eee;
      margin: 0.5em 0;
      padding: 0;
  }

  pre, code, kbd, samp {
      color: #000;
      font-family: monospace, monospace;
      _font-family: 'courier new', monospace;
      font-size: 0.98em;
  }

  pre {
      white-space: pre;
      white-space: pre-wrap;
      word-wrap: break-word;
  }

  .answer {
      color:#CC0033;
      font-style:italic;
  }

  b, strong {
      font-weight: bold;
  }

  dfn {
      font-style: italic;
  }

  ins {
      background: #ff9;
      color: #000;
      text-decoration: none;
  }

  mark {
      background: #ff0;
      color: #000;
      font-style: italic;
      font-weight: bold;
  }

  sub, sup {
      font-size: 75%;
      line-height: 0;
      position: relative;
      vertical-align: baseline;
  }

  sup {
      top: -0.5em;
  }

  sub {
      bottom: -0.25em;
  }

  ul, ol {
      margin: 0.5em 0;
      padding: 0em 0em 0em 1em;
  }

  ul img {
      list-style-type: none;
  }

  li p:last-child {
      margin-bottom: 0;
  }

  hr {
      border-top:none;
      height:0px;
      clear:both;
  }

  ul ul, ol ol {
      margin: .3em 0;
  }

  dl {
      margin-bottom: 1em;
  }

  dt {
      font-weight: bold;
      margin-bottom: .8em;
  }

  dd {
      margin: 0 0 .8em 2em;
  }

  dd:last-child {
      margin-bottom: 0;
  }

  img {
      border: 0;
      -ms-interpolation-mode: bicubic;
      vertical-align: middle;
  }

  figure {
      display: block;
      text-align: center;
      margin: 1em 0;
  }

  figure img {
      border: none;
      margin: 0 auto;
  }

  figcaption {
      font-size: 0.8em;
      font-style: italic;
      margin: 0 0 .8em;
  }

  table {
      margin-bottom: 2em;
      border-bottom: 1px solid #ddd;
      border-right: 1px solid #ddd;
      border-spacing: 0;
      border-collapse: collapse;
  }

  table th {
      padding: .2em 1em;
      background-color: #eee;
      border-top: 1px solid #ddd;
      border-left: 1px solid #ddd;
  }

  table td {
      padding: .2em 1em;
      border-top: 1px solid #ddd;
      border-left: 1px solid #ddd;
      vertical-align: top;
  }

  .author {
      font-size: 1.2em;
      text-align: center;
  }

  @media only screen and (min-width: 480px) {
      body {
  	font-size: 14px;
      }
  }
  @media only screen and (min-width: 768px) {
      body {
  	font-size: 16px;
      }
  }
  @media print {
      * {
  	background: transparent !important;
  	color: black !important;
  	filter: none !important;
  	-ms-filter: none !important;
      }

      body {
  	font-size: 12pt;
  	max-width: 100%;
      }

      a, a:visited {
  	text-decoration: underline;
      }

      hr {
  	height: 1px;
  	border: 0;
  	border-bottom: 1px solid black;
      }

      a[href]:after {
  	content: " (" attr(href) ")";
      }

      abbr[title]:after {
  	content: " (" attr(title) ")";
      }

      .ir a:after, a[href^="javascript:"]:after, a[href^="#"]:after {
  	content: "";
      }

      pre, blockquote {
  	border: 1px solid #999;
  	padding-right: 1em;
  	page-break-inside: avoid;
      }

      tr, img {
  	page-break-inside: avoid;
      }

      img {
  	max-width: 40% !important;
      max-height: 300px !important;
      }

      @page :left {
  	margin: 15mm 20mm 15mm 10mm;
      }

      @page :right {
  	margin: 15mm 10mm 15mm 20mm;
      }

      p, h2, h3 {
  	orphans: 3;
  	widows: 3;
      }

      h2, h3 {
  	page-break-after: avoid;
      }
  }


  ldata {
  	font-size: 0.7em;
  	margin-bottom: 0em;
  	color:#808080;
  	font-style:italic;
  }

  danger {
  	color:#FF0000;
  	font-weight:bold;
  }

  correct {
  	color:#39C900;
  	font-weight:bold;
  }

  clg{
      color:#39C900;
  	font-weight:bold;
  }

  clr{
  	color:#FF0000;
  	font-weight:bold;
  }

  clb{
  	color:#0000CC;
  	font-weight:bold;
  }

  clp{
  	color:#6600FF;
  	font-weight:bold;
  }

  clk{
  	color:#708cef;
  	font-weight:bold;
  }

  clo{
  	color:#CC6600;
  	font-weight:bold;
  }

  sc{
          font-variant: small-caps;
  }

  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h3 id="weve-come-a-long-way">We’ve come a LONG way</h3>
<video controls src="video/earlytts.mp4">
</audio>
<table style="width:6%;">
<colgroup>
<col style="width: 5%" />
</colgroup>
<tbody>
<tr class="odd">
<td># Legacy Approaches to TTS</td>
</tr>
<tr class="even">
<td>### Will Styler - LIGN 168</td>
</tr>
</tbody>
</table>
<h3 id="todays-plan">Today’s Plan</h3>
<ul>
<li><p>Unit Selection Synthesis</p></li>
<li><p>Strengths and Weaknesses of Unit Selection</p></li>
<li><p>Parametric TTS</p></li>
</ul>
<hr />
<h3 id="today-were-learning-more-legacy-methods">Today, we’re learning
more legacy methods</h3>
<ul>
<li>Neural Networks have ‘won’ TTS too
<ul>
<li>We’ll talk about how that works next time</li>
</ul></li>
<li>As always, old methods still work
<ul>
<li>… and they’re actually quite a bit easier to implement!</li>
</ul></li>
<li>Today, we’ll focus on two of them, parametric and unit selection
TTS</li>
</ul>
<hr />
<h3 id="there-are-four-main-methods-for-tts">There are four main methods
for TTS</h3>
<ul>
<li><p>Concatenative or Unit Selection Synthesis</p>
<ul>
<li>These generally are used interchangeably</li>
</ul></li>
<li><p>Parametric TTS</p></li>
<li><p>Neural TTS with intermediate representations</p></li>
<li><p>Neural End-to-End TTS</p></li>
</ul>
<hr />
<h3 id="each-works-slightly-differently">Each works slightly
differently</h3>
<ul>
<li>Concatenative, Unit Selection Synthesis
<ul>
<li>“Combine together existing chunks to form the desired utterance from
the text”</li>
</ul></li>
<li>Parametric TTS
<ul>
<li>“Let’s map text to acoustic parameters (e.g. LPC coefficents,
spectral envelopes, pitch) using statistics”</li>
</ul></li>
<li>Neural TTS
<ul>
<li>“Let’s turn text into an intermediate representation like a
spectrogram, and then create sound from that”</li>
</ul></li>
<li>Neural End-to-End TTS
<ul>
<li>“Let’s just map text directly to a waveform”</li>
</ul></li>
<li><em>Today, we’ll focus on the two pre-neural approaches!</em></li>
</ul>
<hr />
<h2 id="unit-selection-tts">Unit Selection TTS</h2>
<hr />
<h3 id="concatenative-tts">Concatenative TTS</h3>
<ul>
<li><p>We started thinking about this last time!</p></li>
<li><p>Concatenative TTS builds utterances from pre-recorded chunks</p>
<ul>
<li>You can record whatever chunks make sense</li>
<li>Or slice up existing recordings of text</li>
</ul></li>
<li><p>Synthesis is simply the selection and concatenation and
processing of pre-recorded units</p></li>
<li><p><em>Concatenative Synthesis doesn’t ever generate new audio, it
just combines and tweaks existing audio!</em></p></li>
</ul>
<hr />
<h3
id="concatenative-synthesis-units-can-be-of-any-length">Concatenative
Synthesis Units can be of any length!</h3>
<ul>
<li><p>Phones, diphones, triphones, syllables, words, utterances or
phrases</p></li>
<li><p>… But every size of unit comes with ups and downs!</p></li>
</ul>
<hr />
<h3 id="unit-size-tradeoffs">Unit size tradeoffs</h3>
<ul>
<li><p>(Di)phone chunks are flexible and tiny, but don’t sound as
natural as larger units</p></li>
<li><p>(Di)phone chunks can be used to create ‘guesses’ generated by
text analysis even for something out of the dictionary</p></li>
<li><p>Word chunks are natural and still have some flexibility, but have
choppy word-to-word transitions</p></li>
<li><p>Phrase-based chunks sound great internally, but completely lack
flexibility and breadth (without huge data size)</p></li>
<li><p><strong>Why not choose the best size for the
situation?</strong></p></li>
</ul>
<hr />
<h3 id="unit-selection">Unit Selection</h3>
<ul>
<li><p>“Let’s build everything by concatenation, but grab whichever
chunk makes the most sense!”</p></li>
<li><p>Generally, start by grabbing the largest chunks you can, then
grab individual words</p></li>
<li><p>Words which aren’t found in the data can be constructed from
existing (di)phones</p>
<ul>
<li>This is usually done by a spelling-to-phoneme converter</li>
</ul></li>
<li><p>Then smooth over the gaps by adjusting the prosody!</p></li>
</ul>
<hr />
<h3 id="domain-specific-systems">Domain-Specific Systems</h3>
<blockquote>
<p>…HIGH SURF THURSDAY AFTERNOON THROUGH FRIDAY…</p>
</blockquote>
<blockquote>
<p>A long period west to northwest swell will bring high surf Thursday
afternoon through Friday. The peak swell and surf will occur Thursday
night into early Friday morning, with the highest surf in southern San
Diego County. Minor coastal flooding will occur during periods of high
tides.</p>
</blockquote>
<hr />
<h3 id="arbitrary-text-systems">Arbitrary Text Systems</h3>
<blockquote>
<p>Alaina Rutkowska posted a great close-up photo of the tube of an
ice-cream cone worm. The tube is made of sand grains, carefully selected
and fitted together, and bound with a special adhesive. The worm has
golden bristles used to rake through sediment so it can pick up yummy
bits with little tentacles.</p>
</blockquote>
<hr />
<h3
id="concatenative-synthesis-requires-a-living-database-to-work-from">Concatenative
Synthesis requires a living database to work from</h3>
<ul>
<li>You’ll need access to the whole database to reproduce speech
<ul>
<li>Rather than other approaches which just use the corpus to learn
from</li>
</ul></li>
<li>The breadth of your database has a huge effect on the naturalness of
the system</li>
</ul>
<hr />
<h3 id="creating-a-concatenative-tts-system">Creating a Concatenative
TTS system</h3>
<ul>
<li><p>Record a large speech database</p></li>
<li><p>Annotate the features of the speech in the database</p></li>
<li><p>Select clever algorithms for text analysis, prosody generation,
and unit selection</p></li>
<li><p>Then deploy!</p></li>
</ul>
<hr />
<h3 id="concatenative-tts-has-a-few-steps">Concatenative TTS has a few
steps</h3>
<ul>
<li><p>Collect and annotate a speech database (once)</p></li>
<li><p>Do text analysis, grabbing chunks and modeling missing words from
graphemes</p></li>
<li><p>Model the prosody of the sentence</p></li>
<li><p>Select the right units</p></li>
<li><p>Combine them and smooth them over</p></li>
</ul>
<hr />
<h3 id="collecting-a-speech-database-once">Collecting a speech database
(once)</h3>
<ul>
<li><p>Record a LOT of speech from an actual human</p></li>
<li><p>You can record words, sentences, or running text, and then
segment down from there</p>
<ul>
<li>“Expect localized flooding.” gives ‘Expect’, ‘localized’, as well as
/ɛ k s p t/</li>
</ul></li>
<li><p>You want many versions of important words, and good coverage of
the possible set of words and phrases</p></li>
<li><p>You’ll need to update this as new words and vocabulary come into
use</p>
<ul>
<li>‘COVID’, ‘social distancing’, ‘Pete Buttigeig’, ‘skibidi’</li>
</ul></li>
<li><p><strong>All of this happens offline, before the system is
deployed</strong></p></li>
</ul>
<hr />
<h3 id="speech-databases-generally-have-one-actor">Speech Databases
generally have one actor</h3>
<iframe width="560" height="315" src="https://www.youtube.com/embed/7xvv560z2Go?si=9dO8pcCvmp_U3VPg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen>
</iframe>
<hr />
<h3 id="then-you-annotate-the-database-once">Then you annotate the
database (once)</h3>
<ul>
<li><p>You’ll mark boundaries of words, phrases, (di)phones</p></li>
<li><p>You’ll get the acoustic properties (via formants, MFCCs on edges)
and prosodic properties (pitch, duration)</p></li>
<li><p>You’ll mark the adjacent phonemes</p>
<ul>
<li>If you’re building ‘My cat king’, you’d prefer the ‘cat’ from ‘Sky
cat came’ rather than from ‘no cat checked’</li>
</ul></li>
<li><p>This will let you know which chunk is the best ‘fit’
later</p></li>
</ul>
<hr />
<h3 id="now-you-do-text-analysis-on-the-input-texts">Now you do text
analysis on the input texts</h3>
<ul>
<li><p>Basic syntactic parsing to choose the right parts of speech</p>
<ul>
<li>Record vs Record</li>
</ul></li>
<li><p>Bits of language modeling to pick up on DJs dropping the
bass</p></li>
<li><p>Dictionary lookups for individual words</p></li>
<li><p>Grapheme-to-phoneme modeling for the rest</p></li>
</ul>
<hr />
<h3 id="youll-model-prosody-too">You’ll model prosody too!</h3>
<ul>
<li><p>Generating values for pitch, duration, loudness, pauses</p></li>
<li><p>This is often done with syntactic models</p></li>
<li><p>Machine learning can take punctuation into account</p></li>
</ul>
<hr />
<h3 id="then-you-choose-the-optimal-chunks">Then, you choose the optimal
chunks</h3>
<ul>
<li><p>You’re going to have (generally) multiple recordings of a given
word, which combination is best?</p></li>
<li><p>You’ll do this by optimizing the <strong>Target Cost</strong> and
the <strong>Join Cost</strong></p></li>
</ul>
<hr />
<h3 id="target-cost">Target Cost</h3>
<ul>
<li><p><em>How good is a given recording for the context it’s going
in?</em></p></li>
<li><p>What are the neighboring phonemes?</p>
<ul>
<li>“Walk” before /i/ is different from before /a/</li>
</ul></li>
<li><p>How well does it fit the desired prosodic standards?</p>
<ul>
<li>Don’t put a stressed syllable in an unstressed context</li>
</ul></li>
<li><p>Is it the same part of speech, etc?</p>
<ul>
<li>Do you want to use ‘fit’ as a verb, as a noun?</li>
</ul></li>
</ul>
<hr />
<h3 id="join-cost">Join Cost</h3>
<ul>
<li><p><em>How well does the chunk ‘match’ with its
neighbors?</em></p></li>
<li><p>Do the edges of this recording sound more like the edges of the
neighbors?</p>
<ul>
<li>Often done by looking at spectral distance</li>
</ul></li>
<li><p>Are the neighbor tokens from different prosodic conditions?</p>
<ul>
<li>It may be better to ‘match’ neighbors even if the token is a bit
less right for the context and needs fixed</li>
</ul></li>
<li><p>Is the pitch before and after different from the token pitch?</p>
<ul>
<li>Our ability to adjust pitch is only so powerful!</li>
</ul></li>
</ul>
<hr />
<h3 id="we-want-both-of-these-things">We want both of these things!</h3>
<ul>
<li><p>We want to choose units which are <em>correct for the
context</em>, and <em>fit in well with adjacent tokens</em></p></li>
<li><p>This is a hard optimization problem</p></li>
<li><p><strong>Viterbi-ish Algorithms</strong> find the best pathway
through the data</p>
<ul>
<li>Forwards and backwards walks</li>
</ul></li>
<li><p>The optimal solution is the ‘least worst’ in terms of both target
and join costs</p></li>
</ul>
<hr />
<h3 id="then-you-concatenate-and-modify-the-output">Then you
concatenate, and modify the output</h3>
<ul>
<li><p>Adjust loudness directly</p></li>
<li><p>Adjust pitch and duration using PSOLA</p></li>
<li><p>Remove any join artifacts</p></li>
</ul>
<hr />
<h3 id="and-youre-done">… and you’re done!</h3>
<ul>
<li>You play back the audio generated!</li>
</ul>
<hr />
<h3 id="each-given-inference-looks-the-same">Each given inference looks
the same</h3>
<ul>
<li><p>Do text analysis, grabbing chunks and modeling missing words from
graphemes</p></li>
<li><p>Model the prosody of the sentence</p></li>
<li><p>Select the right units from the database</p></li>
<li><p>Combine them and smooth them over</p></li>
</ul>
<hr />
<h3 id="but-each-of-these-steps-is-hard">… but each of these steps is
hard!</h3>
<hr />
<h2 id="difficulties-with-unit-selection-tts">Difficulties with Unit
Selection TTS</h2>
<hr />
<h3 id="building-a-database-is-hard">Building a database is hard</h3>
<ul>
<li><p>Recording all the words is hard!</p></li>
<li><p>Segmenting sounds is hard</p></li>
<li><p>Measuring their properties is hard</p>
<ul>
<li>What about errors in the database?</li>
</ul></li>
<li><p>Keeping the database up to date is hard</p>
<ul>
<li>What if your ‘voice’ leaves the company?</li>
</ul></li>
</ul>
<hr />
<h3 id="aside-this-is-different-in-different-languages">Aside: This is
different in different languages</h3>
<ul>
<li><p>“Record all the words” is basically impossible for heavily
synthetic or morphology-using languages</p></li>
<li><p>So, you’ll need to work more below word level</p></li>
</ul>
<hr />
<h3 id="text-analysis-is-hard">Text Analysis is hard</h3>
<ul>
<li><p>Especially computational grapheme-to-phoneme modeling!</p>
<ul>
<li>Kaetlyinn</li>
<li>Ruaridh</li>
</ul></li>
<li><p>Modeling prosody is hard too!</p></li>
<li><p>All the rest of the text analysis difficulties we
covered!</p></li>
</ul>
<hr />
<h3 id="choosing-optimal-chunks-is-hard">Choosing optimal chunks is
hard</h3>
<ul>
<li><p>You don’t always have the chunks you want</p></li>
<li><p>The “San” from San Diego may be subtly different from the “San”
in “San Ysidro”</p></li>
<li><p>Many criteria for optimal fit, all need to be optimized
quickly!</p></li>
</ul>
<hr />
<h3 id="concatenation-is-hard">Concatenation is hard</h3>
<ul>
<li><p>Sometimes the ‘split’ points aren’t ideal</p></li>
<li><p>Remember PSOLA? Yeah, hard, and not 100% effective
(e.g. creak)</p></li>
<li><p>Even just joining waveforms without artifacts is hard</p></li>
</ul>
<hr />
<h3 id="so-concatenative-synthesis-is-hard">So, Concatenative synthesis
is hard!</h3>
<ul>
<li>We’ve all heard bad unit selection</li>
</ul>
<audio controls src="comp/nws_radio.mp3">
</audio>
<hr />
<h3
id="concatenative-synthesis-is-actually-still-quite-useful">Concatenative
Synthesis is actually still quite useful</h3>
<ul>
<li><p>It’s very possible to get a very OK result, with an OK amount of
work</p></li>
<li><p>The amount of data needed for a good database is <em>much</em>
less than what’s needed to train a neural TTS model</p></li>
<li><p>The running costs and compute requirements are <em>much</em>
lower</p></li>
<li><p>The results are very natural with large, predictable
units</p></li>
<li><p><strong>If you need to do TTS for a lower resource language, you
should probably think concatenative first!</strong></p></li>
</ul>
<hr />
<h3 id="what-if-i-dont-want-to-record-a-database">… What if I don’t want
to record a database?</h3>
<ul>
<li><p>Couldn’t we just figure out what phonemes sound like and then
generate the voice directly?</p></li>
<li><p>That’s…</p></li>
</ul>
<hr />
<h2 id="parametric-tts">Parametric TTS</h2>
<hr />
<h3 id="parametric-vocoders">Parametric Vocoders</h3>
<ul>
<li><p><a href="https://github.com/mmorise/World">WORLD</a> is a good
example of this</p></li>
<li><p>Takes as input ‘parameters’ which describe the desired signal,
like…</p>
<ul>
<li>Spectral shape (think LPC)</li>
<li>Pitch and source filters</li>
<li>Aperiodic components and their timings</li>
</ul></li>
<li><p>Outputs sound</p></li>
<li><p>There are similarities in concept to the decompression part of
speech compression</p></li>
</ul>
<hr />
<h3
id="parametric-tts-goes-from-text-to-vocoder-parameters-to-sound">Parametric
TTS goes from text to vocoder parameters to sound</h3>
<ul>
<li><p>“What sequence of parameters for a vocoder corresponds to this
text?”</p></li>
<li><p>The goal is to identify the parameters which create sound that
sounds like the given phoneme sequence</p></li>
<li><p>You’re ‘shaping’ the sound output based on the text, using a few
key parameters to control the vocoder</p></li>
</ul>
<hr />
<h3 id="parametric-tts-is-a-three-stage-process">Parametric TTS is a
three stage process</h3>
<ul>
<li><p>Text analysis</p></li>
<li><p>Parameter Modeling</p></li>
<li><p>Vocoding to produce sound</p></li>
</ul>
<hr />
<h3 id="text-analysis-works-just-like-weve-discussed-before">Text
Analysis works just like we’ve discussed before</h3>
<ul>
<li>Turning text into phoneme sequences with prosody</li>
</ul>
<p><img class="wide" src="comp/tts_phones.jpg"></p>
<hr />
<h3
id="model-training-associates-phoneme-sequences-with-parameter-sequences">Model
training associates phoneme sequences with parameter sequences</h3>
<ul>
<li><p>“In order to create this phoneme/diphone/triphone’s sound, what’s
the sequence of parameters I need to iterate through?”</p></li>
<li><p>This is usually HMM-based, predicting parameter vector ‘state’
from the phoneme sequence from text analysis</p>
<ul>
<li>Duration is modeled too, as well as rate of parameter change</li>
<li>We are not going deeper into this because no.</li>
</ul></li>
<li><p>The parameters are <em>smoothed</em>, because they shouldn’t
change faster than tongues do</p></li>
</ul>
<hr />
<h3 id="new-text-generates-new-parameter-sequences">New text generates
new parameter sequences</h3>
<ul>
<li><p>The model takes the text analysis output and creates a parameter
sequence which should approximate the sound of the words</p></li>
<li><p>Then, you just feed those parameters into the vocoder, and boom,
speech!</p></li>
</ul>
<hr />
<h3 id="parametric-tts-has-benefits">Parametric TTS has benefits</h3>
<ul>
<li>They’re wildly flexible
<ul>
<li>As long as text analysis can analyze it, you can create it</li>
</ul></li>
<li>You don’t need one specific voice actor
<ul>
<li>You can learn from any larger amount of phoneme-annotated data</li>
</ul></li>
<li>The system can be fairly small
<ul>
<li>You just need to do compute to run the statistical models</li>
</ul></li>
<li>There’s no chance of concatenation artifacts
<ul>
<li>It’s built continuously!</li>
</ul></li>
</ul>
<hr />
<h3 id="but-its-also-got-disadvantages">… but it’s also got
disadvantages</h3>
<ul>
<li><p>The naturalness is much lower than recordings of humans</p></li>
<li><p>The number of parameters to estimate gets very unwieldy
quickly</p></li>
<li><p>The compute needed isn’t trivial</p></li>
<li><p>Your training data needs to be a bit homogenous, as it can’t
generalize well</p></li>
<li><p>Long-distance effects are harder to capture with HMMs</p></li>
</ul>
<hr />
<h3 id="wait-a-second">Wait a second…</h3>
<ul>
<li><p>It needs to estimate a ton of parameters</p></li>
<li><p>HMM-based parametric synthesis already needs expensive
compute</p></li>
<li><p>It fails to generalize over heterogenous data</p></li>
<li><p>… and it has difficulty capturing long-distance sequence
effects?</p></li>
<li><p><strong>Aren’t these all things which deep neural networks are
really good at?</strong></p>
<ul>
<li>Foreshadowing? In my LIGN 168?</li>
</ul></li>
</ul>
<hr />
<h3 id="wrapping-up">Wrapping up</h3>
<ul>
<li><p>Unit Selection TTS combines existing chunks in smart ways to
generate good ouput</p></li>
<li><p>It can sound very natural and is (relatively) cheap to build</p>
<ul>
<li>… but it’s not as flexible, and has its own difficulties</li>
</ul></li>
<li><p>Parametric TTS uses fancy statistics to turn phoneme sequences
into vocoder parameters</p>
<ul>
<li>So it’s incredibly flexible, but had a number of problems which we
no longer need to have!</li>
</ul></li>
</ul>
<hr />
<h3 id="next-time">Next time</h3>
<ul>
<li>TTS with Deep Neural Networks</li>
</ul>
<hr />
<p><huge>Thank you!</huge></p>
</body>
</html>
