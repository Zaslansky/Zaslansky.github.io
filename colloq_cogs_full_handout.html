<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title></title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <style type="text/css">
  /*
   * I add this to html files generated with pandoc.
   * Originally from https://gist.github.com/killercup/5917178
   */

  html {
      overflow-y: scroll;
      -webkit-text-size-adjust: 100%;
      -ms-text-size-adjust: 100%;
  }

  body {
      color: #444;
      font-family: "Source Sans 3", Helvetica-Neue, Helvetica, Sans;
      line-height: 1.5;
      padding: 0.5em;
      margin: auto;
      max-width: 55em;
      background: #fefefe;
  }

  a {
      color: #2171b5;
      text-decoration: underline;
  }

  tr:nth-child(even) {background: #F8F8F8}
  tr:nth-child(odd) {background: #FFF}

  a:visited {
      color: #2171b5;
      text-decoration: none;
  }

  a:focus {
      outline: thin dotted;
  }

  *::-moz-selection {
      background: rgba(255, 255, 0, 0.3);
      color: #000;
  }

  *::selection {
      background: rgba(255, 255, 0, 0.3);
      color: #000;
  }

  a::-moz-selection {
      background: rgba(255, 255, 0, 0.3);
      color: #0645ad;
  }

  a::selection {
      background: rgba(255, 255, 0, 0.3);
      color: #0645ad;
  }

  p {
      margin: 0.75em 0;
  }

  img {
      max-width: 60%;
      max-height:400px;
  }

  video {
      max-width: 60%;
  }


  h1, h2, h3, h4, h5, h6 {
      color: #111;
      line-height: 80%;
      margin-top: 1em;
      margin-bottom: 0.5em;
      font-weight: normal;
  }

  h1, h2, h3, h4, h5, h6 {
      font-weight: bold;
  }

  h1 {
      font-size: 2em;
      line-height: 1.25;
      color:  #084594;

  }

  h1.title {
      margin-top:0.2em;
      font-size: 2em;
      line-height: 1.25;
  }

  h2 {
      font-size: 1.5em;
      line-height: 1.6em;
          color:  #084594;
      padding-bottom: 3px;

  }

  h3 {
      font-size: 1.2em;
      line-height: 1.6em;
  }


  h4 {
      font-size: 1.2em;
      line-height: 1.4em;
  }

  h5 {
      font-size: 1em;
  }

  h6 {
      font-size: 0.9em;
  }

  blockquote {
      color: #666666;
      margin: 0;
      padding-left: 3em;
      border-left: 0.5em #EEE solid;
  }

  hr {
      display: block;
      height: 2px;
      border: 0;
      border-top: 1px solid #aaa;
      border-bottom: 1px solid #eee;
      margin: 0.5em 0;
      padding: 0;
  }

  pre, code, kbd, samp {
      color: #000;
      font-family: monospace, monospace;
      _font-family: 'courier new', monospace;
      font-size: 0.98em;
  }

  pre {
      white-space: pre;
      white-space: pre-wrap;
      word-wrap: break-word;
  }

  .answer {
      color:#CC0033;
      font-style:italic;
  }

  b, strong {
      font-weight: bold;
  }

  dfn {
      font-style: italic;
  }

  ins {
      background: #ff9;
      color: #000;
      text-decoration: none;
  }

  mark {
      background: #ff0;
      color: #000;
      font-style: italic;
      font-weight: bold;
  }

  sub, sup {
      font-size: 75%;
      line-height: 0;
      position: relative;
      vertical-align: baseline;
  }

  sup {
      top: -0.5em;
  }

  sub {
      bottom: -0.25em;
  }

  ul, ol {
      margin: 0.5em 0;
      padding: 0em 0em 0em 1em;
  }

  ul img {
      list-style-type: none;
  }

  li p:last-child {
      margin-bottom: 0;
  }

  hr {
      border-top:none;
      height:0px;
      clear:both;
  }

  ul ul, ol ol {
      margin: .3em 0;
  }

  dl {
      margin-bottom: 1em;
  }

  dt {
      font-weight: bold;
      margin-bottom: .8em;
  }

  dd {
      margin: 0 0 .8em 2em;
  }

  dd:last-child {
      margin-bottom: 0;
  }

  img {
      border: 0;
      -ms-interpolation-mode: bicubic;
      vertical-align: middle;
  }

  figure {
      display: block;
      text-align: center;
      margin: 1em 0;
  }

  figure img {
      border: none;
      margin: 0 auto;
  }

  figcaption {
      font-size: 0.8em;
      font-style: italic;
      margin: 0 0 .8em;
  }

  table {
      margin-bottom: 2em;
      border-bottom: 1px solid #ddd;
      border-right: 1px solid #ddd;
      border-spacing: 0;
      border-collapse: collapse;
  }

  table th {
      padding: .2em 1em;
      background-color: #eee;
      border-top: 1px solid #ddd;
      border-left: 1px solid #ddd;
  }

  table td {
      padding: .2em 1em;
      border-top: 1px solid #ddd;
      border-left: 1px solid #ddd;
      vertical-align: top;
  }

  .author {
      font-size: 1.2em;
      text-align: center;
  }

  @media only screen and (min-width: 480px) {
      body {
  	font-size: 14px;
      }
  }
  @media only screen and (min-width: 768px) {
      body {
  	font-size: 16px;
      }
  }
  @media print {
      * {
  	background: transparent !important;
  	color: black !important;
  	filter: none !important;
  	-ms-filter: none !important;
      }

      body {
  	font-size: 12pt;
  	max-width: 100%;
      }

      a, a:visited {
  	text-decoration: underline;
      }

      hr {
  	height: 1px;
  	border: 0;
  	border-bottom: 1px solid black;
      }

      a[href]:after {
  	content: " (" attr(href) ")";
      }

      abbr[title]:after {
  	content: " (" attr(title) ")";
      }

      .ir a:after, a[href^="javascript:"]:after, a[href^="#"]:after {
  	content: "";
      }

      pre, blockquote {
  	border: 1px solid #999;
  	padding-right: 1em;
  	page-break-inside: avoid;
      }

      tr, img {
  	page-break-inside: avoid;
      }

      img {
  	max-width: 40% !important;
      max-height: 300px !important;
      }

      @page :left {
  	margin: 15mm 20mm 15mm 10mm;
      }

      @page :right {
  	margin: 15mm 10mm 15mm 20mm;
      }

      p, h2, h3 {
  	orphans: 3;
  	widows: 3;
      }

      h2, h3 {
  	page-break-after: avoid;
      }
  }


  ldata {
  	font-size: 0.7em;
  	margin-bottom: 0em;
  	color:#808080;
  	font-style:italic;
  }

  danger {
  	color:#FF0000;
  	font-weight:bold;
  }

  correct {
  	color:#39C900;
  	font-weight:bold;
  }

  clg{
      color:#39C900;
  	font-weight:bold;
  }

  clr{
  	color:#FF0000;
  	font-weight:bold;
  }

  clb{
  	color:#0000CC;
  	font-weight:bold;
  }

  clp{
  	color:#6600FF;
  	font-weight:bold;
  }

  clk{
  	color:#708cef;
  	font-weight:bold;
  }

  clo{
  	color:#CC6600;
  	font-weight:bold;
  }

  sc{
          font-variant: small-caps;
  }

  </style>
</head>
<body>
<h1
id="what-can-we-learn-about-humans-from-looking-at-speech-technology">What
can we learn about humans from looking at speech technology?</h1>
<h3 id="will-styler">Will Styler</h3>
<p><a href="https://savethevowels.org/talks/colloq_cogs_speechlg.html"
class="uri">https://savethevowels.org/talks/colloq_cogs_speechlg.html</a></p>
<hr />
<h3 id="first-the-elephant-in-the-room">First, the elephant in the
room</h3>
<ul>
<li>I’m not actually in Cognitive Science!</li>
</ul>
<hr />
<ul>
<li><img class="r-stretch" src="img/impostor.jpg"></li>
</ul>
<hr />
<h3
id="cognitive-science-wasnt-a-separate-program-where-i-grew-up">Cognitive
Science wasn’t a separate program where I grew up</h3>
<ul>
<li><p>I have my BA, MA and Doctorate in Linguistics from the <a
href="https://www.colorado.edu/linguistics/">University of Colorado at
Boulder</a></p></li>
<li><p>CU Boulder had the <a
href="https://www.colorado.edu/ics/">Institute of Cognitive
Science</a></p>
<ul>
<li>Collaborations between faculty in CS, Philosophy, Linguistics,
Psych, Education, and more</li>
</ul></li>
<li><p>I went to their talks and was advised by an affiliate, but there
was not a COGS major or Ph.D Specialization</p>
<ul>
<li>… but I am bothered by COGS-flavored questions</li>
</ul></li>
</ul>
<hr />
<h3 id="im-a-computational-phonetician">I’m a Computational
Phonetician</h3>
<ul>
<li><p>This means I study human speech perception and production</p>
<ul>
<li>… using computational methods and models</li>
</ul></li>
<li><p>This involves a mix of experiments, data analysis, recordings,
and instrumental measurements</p></li>
<li><p>I collaborate lots with <a
href="https://quote.ucsd.edu/lasr/">Dr. Sarah Creel</a></p></li>
</ul>
<hr />
<h3 id="im-also-director-of-computational-social-science-at-ucsd">I’m
also Director of <a href="https://css.ucsd.edu">Computational Social
Science</a> at UCSD</h3>
<ul>
<li>We have <a
href="https://css.ucsd.edu/people/faculty.html#Cognitive-Science">lots
of Cognitive Scientists</a>!
<ul>
<li>Including [Dr. Sean Tro]</li>
</ul></li>
<li>So, this means I’m always thinking about computers as a tool for
understanding humans!</li>
</ul>
<hr />
<h2 id="what-is-speech-technology">What is Speech Technology?</h2>
<hr />
<h3 id="were-getting-very-used-to-speech-technology">We’re getting very
used to speech technology</h3>
<ul>
<li><p>Siri/Alexa/GoogleAssistant</p></li>
<li><p>ChatGPT Voice Mode</p></li>
<li><p>Speech-to-Text Keyboards</p></li>
<li><p>Text-to-Speech (e.g. in Twitch streams)</p></li>
</ul>
<hr />
<h3 id="there-are-many-kinds-of-speech-technology">There are many kinds
of speech technology</h3>
<ul>
<li><p>Voice Activity Detection</p></li>
<li><p>Automatic Noise Filtering</p></li>
<li><p>Voice Compression and Encryption</p></li>
<li><p>Forced Alignment and Timestamping</p></li>
<li><p>Automatic Speech Recognition (ASR)</p></li>
<li><p>Speech Synthesis or Text-to-Speech (TTS)</p></li>
</ul>
<hr />
<h3 id="these-are-really-interesting-tools">These are really interesting
tools!</h3>
<ul>
<li><p>They allow new kinds of human-computer interactions</p></li>
<li><p>They are incredible tools for accessibility</p></li>
<li><p>They’re great for processing large amounts of data</p></li>
<li><p>… but the most interesting part of these tools?</p></li>
</ul>
<hr />
<h3
id="the-most-interesting-part-about-them-is-that-they-work-at-all">The
most interesting part about them is that they work at all!</h3>
<hr />
<h3 id="todays-plan">Today’s Plan</h3>
<ul>
<li><p>Why is speech so hard to produce?</p></li>
<li><p>How do computers produce speech?</p></li>
<li><p>Why is speech so hard to perceive?</p></li>
<li><p>How do computers perceive speech?</p></li>
</ul>
<hr />
<h2 id="speech-is-hard">Speech is Hard</h2>
<hr />
<h3 id="human-speech-is-incredibly-difficult">Human Speech is incredibly
difficult</h3>
<ul>
<li><p>This is an incredibly intricate gestural dance in your mind and
mouth</p></li>
<li><p>Let’s try it</p></li>
</ul>
<hr />
<h3 id="a-linguistics-major-goes-very-well-with-cognitive-science">“A
Linguistics Major goes very well with Cognitive Science”</h3>
<ul>
<li><p>First, focus on your jaw</p></li>
<li><p>Now, on your tongue</p></li>
<li><p>Now, feel the vibes</p></li>
</ul>
<hr />
<h3 id="ultrasound-of-speech">Ultrasound of Speech</h3>
<p><img class="r-stretch" src="phonmedia/tools_ultrasound.jpg"></p>
<hr />
<h3 id="ultrasound-of-speech-1">Ultrasound of Speech</h3>
<video width="1200" height="600" controls id="video">
<source src="video/ultrasound_northwind.mp4">
</video>
<p><br> <tiny>From University of Michigan Phonetics Lab</tiny></p>
<hr />
<h3 id="many-paths-to-the-same-sound">Many paths to the same sound</h3>
<hr />
<h3 id="many-paths-to-the-same-sound-1">Many paths to the same
sound</h3>
<p><img src="phonmedia/englishr.jpg" /></p>
<hr />
<h3 id="speech-is-hard-1">Speech is <em>hard</em></h3>
<ul>
<li><p>Speech is flapping bits of meat around in your head and throat
while you expel air.</p></li>
<li><p>This creates tiny vibrations in the air, ca</p></li>
</ul>
<hr />
<h3 id="so-what-do-these-vibrations-look-like">So, what do these
vibrations look like?</h3>
<hr />
<p><img class="r-stretch" src="phonmedia/sky.png"></p>
<hr />
<p><img class="r-stretch" src="phonmedia/skyspectrogram.png"></p>
<hr />
<h3 id="there-is-incredible-complexity-in-this-process">There is
incredible complexity in this process</h3>
<ul>
<li><p>Fluid movement of your mouth and tongue</p></li>
<li><p>Careful planning of air and breathing</p></li>
<li><p>Control of pitch, gestures, and other aspects</p></li>
</ul>
<hr />
<h3 id="and-we-want-to-do-this-with-software">… and we want to do
<em>this</em> with software?!?</h3>
<ul>
<li><p>‘Speech Synthesis’ or ‘Text-to-Speech’ (TTS)</p></li>
<li><p><em>How do we do that?</em></p></li>
</ul>
<hr />
<h3 id="the-task">The Task</h3>
<ul>
<li>“A linguistics major goes very well with Cognitive Science”</li>
</ul>
<audio controls src="phonmedia/lingwithcogs.mp3">
</audio>
<hr />
<h3 id="text-analysis">Text Analysis</h3>
<ul>
<li><p>“OK, the human gave me text, what do they actually want me to
say?”</p></li>
<li><p>This part is usually done in Python</p></li>
<li><p>This is actually hard</p>
<ul>
<li>‘1997’ is many things</li>
</ul></li>
</ul>
<hr />
<h3 id="pge-will-file-schedules-on-april-20.">“PG&amp;E will file
schedules on April 20.”</h3>
<ul>
<li><p><img class="wide" src="comp/tts_phones.jpg"></p></li>
<li><p><img class="wide" src="comp/tts_wave.jpg"></p></li>
<li><p>(Thanks to Julia Hirschberg for this annotated chunk)</p></li>
</ul>
<hr />
<h3 id="then-we-turn-that-into-audio">Then, we turn that into audio</h3>
<ul>
<li>“I know what needs to be said, now, give me a wave I can play back
for the humans”</li>
</ul>
<hr />
<h3 id="for-a-long-time-we-cheated-using-humans">For a long time, we
cheated using humans!</h3>
<ul>
<li><p><strong>Concatenative or ‘Unit Selection’ TTS</strong> chops up
bits and pieces of existing speech to create new speech</p></li>
<li><p>You record a huge database of speech from a voice actor, with
optimum ‘coverage’</p>
<ul>
<li>You update as new words emerge (e.g. COVID, rawdogging,
skibidi)</li>
</ul></li>
<li><p><strong>You then combine these words into sentences to match the
text</strong></p>
<ul>
<li>… and you use fancy algorithms to smooth the results out.</li>
</ul></li>
</ul>
<hr />
<h3 id="this-isnt-easy">This isn’t easy</h3>
<ul>
<li>You have to choose the best recorded token
<ul>
<li>You might have 500 recordings of ‘went’</li>
</ul></li>
<li>Context matters a lot
<ul>
<li>“park” can sound very different in different places with words</li>
</ul></li>
<li>You can’t get full coverage
<ul>
<li>“Ruaridh”, “Krivokapic”, “simp”, “La Jolla”</li>
</ul></li>
</ul>
<hr />
<h3 id="the-result-can-be-imperfect">The result can be imperfect</h3>
<video controls src="video/donutquantity.mp4">
</video>
<hr />
<h3 id="and-youve-only-got-one-voice">… and you’ve only got one
voice</h3>
<ul>
<li>Which means that each new person needs a new collection of data</li>
</ul>
<hr />
<h3
id="then-artificial-neural-networks-arrived-and-everything-changed">Then,
Artificial Neural Networks arrived, and everything changed</h3>
<p><img class="r-stretch" src="dalle/cuteneuralnetwork.jpg"></p>
<hr />
<h3 id="the-worlds-worst-introduction-to-neural-networks">The World’s
Worst Introduction to Neural Networks</h3>
<p><img class="r-stretch" src="img/dnn.jpg"></p>
<hr />
<h3 id="take-cogs-181-to-actually-understand-this">Take COGS 181 to
actually understand this!</h3>
<hr />
<h3 id="neural-tts-is-quite-powerful">Neural TTS is quite powerful</h3>
<ul>
<li><p>Train a neural network with text and corresponding audio</p></li>
<li><p>Make it output something which can made into a wave very
readily</p></li>
<li><p>Either make the wave directly, or make an intermediate
representation which can be turned into a wave</p></li>
</ul>
<hr />
<h3 id="tacotron2-is-a-relatively-simple-open-system"><a
href="https://arxiv.org/pdf/1712.05884">TacoTron2</a> is a relatively
simple, open system</h3>
<ul>
<li>It takes text, and generates spectrograms, chunk-by-chunk, which can
be turned into a waveform</li>
</ul>
<hr />
<h3 id="tacotron-2">TacoTron 2</h3>
<p><img class="r-stretch" src="phonmedia/tts_tacotron2.png"></p>
<hr />
<h3 id="this-allows-us-to-go-from-text-to-speech">This allows us to go
from text to speech!</h3>
<ul>
<li><p>We feed in text, and we get back a wave, with no humans involved
past making training data!</p></li>
<li><p>The results are getting very, very good.</p></li>
</ul>
<audio controls src="comp/tts_lingcogs.mp3">
</audio>
<hr />
<h3 id="the-state-of-the-art-is-advanced-but-closed">The State of the
Art is Advanced, but closed</h3>
<ul>
<li>Current state of the art models from ElevenLabs, OpenAI, Google, and
Amazon are all closed and proprietary
<ul>
<li>If you want the best TTS in the world, it has to happen on somebody
else’s computer</li>
</ul></li>
<li>Details are often not published and considered “trade secrets”
<ul>
<li>They may well be open-source models with changes and tweaks</li>
</ul></li>
<li>It’s not currently possible to teach the state of the art in TTS!
<ul>
<li>… and this should disturb us as a society</li>
</ul></li>
</ul>
<hr />
<h3 id="neural-tts-can-be-trained-using-any-voice">Neural TTS can be
trained using <em>any</em> voice</h3>
<ul>
<li><p>You can build a model from the ground up using any voice you’d
like</p>
<ul>
<li><a
href="https://www.npr.org/2024/05/20/1252495087/openai-pulls-ai-voice-that-was-compared-to-scarlett-johansson-in-the-movie-her">Except
Scarlett Johansson</a></li>
</ul></li>
<li><p>If all your training data are from a bored Bostonian, you’ll end
up with a bored Bostonian TTS voice</p></li>
<li><p>This is very expensive, though, and doesn’t scale well at all</p>
<ul>
<li>You also need <em>lots</em> of data from the new speaker</li>
</ul></li>
</ul>
<hr />
<blockquote>
<p>All human beings are born free and equal in dignity and rights. They
are endowed with reason and conscience and should act towards one
another in a spirit of brotherhood.</p>
</blockquote>
<audio controls src="comp/tts_rights_brianvoice.mp3">
</audio>
<audio controls src="comp/tts_rights_rachelvoice.mp3">
</audio>
<hr />
<h3
id="but-isnt-linguistic-information-separate-from-talker-information">…
but isn’t linguistic information separate from talker information?</h3>
<ul>
<li>Why can’t we just adapt to new voices?</li>
</ul>
<hr />
<h3 id="we-can-think-of-all-speech-as-having-content-and-style">We can
think of all speech as having ‘content’ and ‘style’</h3>
<ul>
<li>Voices express linguistic ‘content’
<ul>
<li>Phonemes, with ordering, and necessary tone/prosody for
comprehension</li>
<li>This is ‘all we need’ to understand the utterances</li>
</ul></li>
<li>‘Style’ is everything else we’ve been talking about
<ul>
<li>‘Speaker’ identity</li>
<li>Social components</li>
<li>Emotional content</li>
<li>Plus prosodic factors (e.g. speed, emphasis, prosodic ‘tunes’,
sarcasm)</li>
</ul></li>
</ul>
<hr />
<h3
id="couldnt-we-just-abstract-out-the-style-component-and-apply-it-to-whatever-linguistic-content-wed-like">Couldn’t
we just abstract out the ‘style’ component and apply it to whatever
Linguistic content we’d like?</h3>
<ul>
<li>Yes!</li>
</ul>
<hr />
<h3 id="heres-a-multi-speaker-version-of-tacotron-2">Here’s a
Multi-Speaker Version of TacoTron 2</h3>
<p><img class="r-stretch" src="phonmedia/tts_tacotron2_multispeaker.png"></p>
<hr />
<h3 id="the-results-of-this-are-terrifying">The results of this are…
terrifying</h3>
<ul>
<li>… and has given rise to ‘Deepfake’ voices</li>
</ul>
<hr />
<h3 id="neural-styler-transfer">Neural Styler Transfer</h3>
<audio controls src="comp/tts_will_ling.wav">
</audio>
<p>(TacoTron2)</p>
<audio controls src="comp/tts_will_elclone.wav">
</audio>
<p>(ElevenLabs)</p>
<p>(Credit to Erick Amaro and Mia Khattar!)</p>
<hr />
<h3 id="multilingual-examples">Multilingual Examples</h3>
<audio controls src="comp/tts_will_english.mp3">
</audio>
<p>(English)</p>
<audio controls src="comp/tts_will_french.mp3">
</audio>
<p>(French)</p>
<audio controls src="comp/tts_will_spanish.mp3">
</audio>
<p>(Spanish)</p>
<audio controls src="comp/tts_will_mandarin.mp3">
</audio>
<p>(Mandarin)</p>
<audio controls src="comp/tts_will_italian.mp3">
</audio>
<p>(Italian)</p>
<audio controls src="comp/tts_will_russian.mp3">
</audio>
<p>(Russian)</p>
<audio controls src="comp/tts_will_japanese.mp3">
</audio>
<p>(Japanese)</p>
<hr />
<h3 id="this-system-isnt-perfect">This system isn’t perfect</h3>
<blockquote>
<p>Adenocarcinoma in Tubovillious Adenoma bona fide certiorari de jure
collusion RICO ex post facto CVN AWACS Escapement Tourbillion Remontoir
de Egalite</p>
</blockquote>
<audio controls src="comp/tts_will_jargon.mp3">
</audio>
<hr />
<h3 id="prosody-is-still-hard">Prosody is still hard</h3>
<audio controls src="comp/tts_will_rick.mp3">
</audio>
<hr />
<h3
id="but-omg-this-thing-can-do-arbitrary-speech-in-an-arbitrary-voice">…
but OMG, this thing can do arbitrary speech, in an arbitrary voice</h3>
<ul>
<li><p>… and it’s never had a tongue, had phonics training, and doesn’t
actually know anything at all about mouths</p></li>
<li><p>Arguably, it doesn’t know anything about English</p>
<ul>
<li>… although some systems use a language model too</li>
</ul></li>
<li><p><em>This is amazing!</em></p>
<ul>
<li>… but it can model more complexity still</li>
</ul></li>
</ul>
<hr />
<h3 id="code-switching">Code Switching</h3>
<p>It’s like sometimes mezclo un poco de español con my English, cuando
me siento particularmente spicy, y tengo curiosidad to know cómo la TTS
handles it.</p>
<audio controls src="comp/tts_will_codeswitch.mp3">
</audio>
<hr />
<h3 id="wow.">Wow.</h3>
<ul>
<li><p>Not only can exposure to data allow a deep neural network to
learn to map written language into speech in one language</p></li>
<li><p>… but it can do it for two languages</p></li>
<li><p>… at once</p></li>
<li><p>… with clear mixing of the two</p></li>
</ul>
<hr />
<h1 id="this-shouldnt-work">This shouldn’t work</h1>
<ul>
<li><p>Yet, here we are</p></li>
<li><p>Espicy!</p></li>
</ul>
<hr />
<h1 id="what-about-speech-perception">What about Speech Perception?</h1>
<hr />
<h3 id="speech-perception-is-hard">Speech Perception is
<em>hard</em></h3>
<ul>
<li><p>Speech is flapping bits of meat around in your head and throat
while you expel air.</p></li>
<li><p>This creates tiny vibrations in the air</p></li>
<li><p><strong>Speech perception is turning the resulting vibrations in
the air back into language</strong></p></li>
</ul>
<hr />
<h3 id="for-computers-this-is-automatic-speech-recognition-asr">For
computers, this is ‘Automatic Speech Recognition’ (ASR)</h3>
<ul>
<li><p>The task is to turn speech into equivalent text</p></li>
<li><p>This is <em>really, really hard</em></p></li>
</ul>
<hr />
<h3 id="lets-focus-on-one-of-the-really-hard-problems">Let’s focus on
one of the really hard problems</h3>
<hr />
<h2 id="vowel-perception">Vowel Perception</h2>
<hr />
<h3 id="what-is-a-vowel">What is a vowel?</h3>
<p>What kind of vowels are we talking about?</p>
<hr />
<p><img src="phonmedia/aeiou.png"></p>
<hr />
<p><img class="big" src="phonmedia/vowelcharts_english.png"></p>
<hr />
<h3 id="review-what-is-a-vowel">Review: What is a vowel?</h3>
<ul>
<li><p>A vowel is voicing passing through (and resonating in) an
unobstructed vocal tract!</p></li>
<li><p>If we change the position of the tongue, we change the
resonances</p></li>
</ul>
<hr />
<p><img class="big" src="phonmedia/voweltongue.png"></p>
<hr />
<h3 id="review-what-is-a-vowel-1">Review: What is a vowel?</h3>
<p>A vowel is voicing passing through (and resonating in) an
unobstructed vocal tract!</p>
<p>If we change the position of the tongue, we change the resonances</p>
<ul>
<li><p>Different resonances <em>filter</em> the sound differently and
determine the vowel quality</p></li>
<li><p><strong>Different tongue shapes create different resonances, and
different vowels!</strong></p></li>
</ul>
<hr />
<p><img class="big" src="phonmedia/voweltongue2.png"></p>
<hr />
<h3 id="what-do-vowels-sound-like">What do vowels sound like?</h3>
<ul>
<li><p>We talk about vowel quality in terms of “formants”</p></li>
<li><p>These are bands of the spectrum where the energy is
strongest</p></li>
<li><p>The frequencies of these formants are our primary cues</p></li>
</ul>
<hr />
<p><img class="big" src="phonmedia/iformants.png"></p>
<hr />
<p><img class="big" src="phonmedia/iformantslabeled.png"></p>
<hr />
<h3 id="vowel-formants">Vowel formants</h3>
<ul>
<li><p>F1 and F2 are generally considered to be the most
important</p></li>
<li><p>F3 is good for rounding and rhoticity</p></li>
</ul>
<hr />
<h3 id="formants-alone-can-be-enough-for-some-perception">Formants alone
can be enough for some perception!</h3>
<hr />
<h3 id="lets-listen-to-some-sounds">Let’s listen to some sounds</h3>
<audio controls>
<source src="phonmedia/lingmajor_f3.mp3" type="audio/mp3">
</audio>
<audio controls>
<source src="phonmedia/lingmajor_f2.mp3" type="audio/mp3">
</audio>
<audio controls>
<source src="phonmedia/lingmajor_f1.mp3" type="audio/mp3">
</audio>
<hr />
<h3 id="lets-listen-to-some-sounds-1">Let’s listen to some sounds</h3>
<audio controls>
<source src="phonmedia/lingmajor_f3.mp3" type="audio/mp3">
</audio>
<audio controls>
<source src="phonmedia/lingmajor_f2.mp3" type="audio/mp3">
</audio>
<audio controls>
<source src="phonmedia/lingmajor_f1.mp3" type="audio/mp3">
</audio>
<p><br></p>
<h3 id="now-lets-play-all-three-at-once">Now let’s play all three at
once!</h3>
<audio controls>
<source src="phonmedia/lingmajor_sine.mp3" type="audio/mp3">
</audio>
<hr />
<h3 id="lets-listen-to-some-sounds-2">Let’s listen to some sounds</h3>
<audio controls>
<source src="phonmedia/lingmajor_f3.mp3" type="audio/mp3">
</audio>
<audio controls>
<source src="phonmedia/lingmajor_f2.mp3" type="audio/mp3">
</audio>
<audio controls>
<source src="phonmedia/lingmajor_f1.mp3" type="audio/mp3">
</audio>
<p><br></p>
<h3 id="now-lets-play-all-three-at-once-1">Now let’s play all three at
once!</h3>
<audio controls>
<source src="phonmedia/lingmajor_sine.mp3" type="audio/mp3">
</audio>
<p><br></p>
<h3 id="does-this-help">Does this help?</h3>
<audio controls>
<source src="phonmedia/lingmajor_orig.mp3" type="audio/mp3">
</audio>
<hr />
<h3 id="so-vowels-are-basically-formant-patterns">So, vowels are
basically formant patterns</h3>
<hr />
<p><img class="big" src="phonmedia/vowelformants.gif"> <small>Different
American English vowels, as spoken by a male speaker</small></p>
<hr />
<h3 id="and-vowel-formants-map-to-articulation">… and vowel formants map
to articulation!</h3>
<hr />
<p><img class="big" src="phonmedia/formantsarticulation.jpg"></p>
<hr />
<h2 id="speaker-variation">Speaker Variation!</h2>
<hr />
<h3 id="speaker-vowel-space-variation">Speaker Vowel Space
Variation</h3>
<ul>
<li><p>Different speakers produce different resonances, even for the
“same” vowels</p>
<ul>
<li>Vocal tracts can be shorter, longer, wider…</li>
</ul></li>
</ul>
<hr />
<p><img class="big" src="ling_memes/vocaltract.jpg"></p>
<hr />
<h3 id="speaker-vowel-space-variation-1">Speaker Vowel Space
Variation</h3>
<p>Different speakers produce different resonances, even for the “same”
vowels</p>
<ul>
<li><p>Speaker can have colds or allergies, can have more nasal
voices…</p></li>
<li><p>Sociolinguistic factors galore</p></li>
<li><p>Every person has a different set of basic vowel formant
positions</p>
<ul>
<li>This is called the speaker’s “vowel space”</li>
</ul></li>
</ul>
<hr />
<p><img class="big" src="phonmedia/ipaformantsgraph.png"></p>
<hr />
<p><img class="big" src="phonmedia/clearspeech_speakeraverages.png"></p>
<hr />
<h3 id="moment-to-moment-vowel-variation">Moment-to-moment Vowel
Variation</h3>
<ul>
<li><p>Even the same speaker will have variation from moment to
moment</p></li>
<li><p>Sometimes we misarticulate, accidentally making the wrong vowel
quality</p>
<ul>
<li><p>Or we talk with food in our mouths, producing different
resonances</p></li>
<li><p>Or sometimes, we’re just plain lazy</p></li>
</ul></li>
<li><p>This leads to constant and massive changes in vowel
production</p></li>
</ul>
<hr />
<p><img class="big" src="phonmedia/clearspeech_speakeraverages.png"></p>
<hr />
<p><img class="big" src="phonmedia/clearspeech_alltokens.png"></p>
<hr />
<p><img class="big" src="phonmedia/clearspeech_alltokensellipses.png"></p>
<hr />
<p><img class="big" src="humorimg/trainwreck.png"></p>
<hr />
<h3
id="every-person-youve-ever-talked-with-has-had-different-vowel-formant-patterns">Every
person you’ve ever talked with has had different vowel formant
patterns</h3>
<ul>
<li>… and yet, we understand each other, somehow</li>
</ul>
<hr />
<p><img class="big" src="img/magic.jpg"></p>
<hr />
<h3 id="there-are-a-few-ways-this-might-work">There are a few ways this
might work</h3>
<hr />
<h3 id="speaker-intrinsic-vowel-space-normalization">Speaker-intrinsic
vowel space normalization</h3>
<ul>
<li><p>Normalization is a process that “happens”</p></li>
<li><p>You meet somebody, you create a model of their vowel space, and
you move on</p></li>
<li><p>These models of speaker vowels are maintained in memory</p></li>
<li><p>One model per person, and a new model each time!</p></li>
</ul>
<hr />
<h3 id="direct-realism">Direct Realism</h3>
<ul>
<li><p>We’re using our senses to form a model of reality, including
inside the mouth</p></li>
<li><p>We don’t really care about the acoustics per se, just estimating
the gestures</p></li>
<li><p>“Based on everything I’m hearing, this seems like she’s making
the same tongue shape I hear in /i/”</p></li>
<li><p>This includes lots of adjustments ‘for free’</p></li>
</ul>
<hr />
<h3 id="speaker-extrinsic-vowel-space-normalization">Speaker-extrinsic
vowel space normalization</h3>
<ul>
<li><p>We store information from <em>every vowel we hear</em>!</p></li>
<li><p>Normalization is then just bulk comparison and probability</p>
<ul>
<li>Vowel identities are probabilistically determined</li>
</ul></li>
<li><p>Perhaps we also segment by speaker, dialect, language,
etc</p></li>
</ul>
<hr />
<p><img class="big" src="phonmedia/clearspeech_alltokens.png"></p>
<hr />
<h3 id="humans-are-able-to-do-this-process">Humans are able to do this
process</h3>
<ul>
<li>… but what about ASR?</li>
</ul>
<hr />
<h2 id="asr">ASR</h2>
<hr />
<h3 id="asr-builds-mappings-from-audio-to-text">ASR builds mappings from
audio to text</h3>
<ul>
<li><p>We feed the system lots of text, and lots of corresponding
audio</p></li>
<li><p>It learns the patterns of sound associated with a given
text</p></li>
<li><p>Some use language models to give better predictions</p></li>
</ul>
<hr />
<h3
id="vintage-asr-used-to-require-explicit-speaker-normalization">Vintage
ASR used to require explicit speaker normalization</h3>
<ul>
<li><p>In the HMM days, ASR software required personalization and
‘training’</p></li>
<li><p>Setup began with “Read these texts aloud”</p>
<ul>
<li>It would then process for a little while as it ‘customized’ to your
voice</li>
</ul></li>
<li><p>The model <em>simply wouldn’t work</em> without this level of
customization</p></li>
</ul>
<hr />
<h3 id="but-when-neural-networks-happened-things-changed">… but when
Neural Networks happened, things changed</h3>
<hr />
<h3 id="whispers-architecture-is-complicated">Whisper’s architecture is
complicated</h3>
<p><img class="r-stretch" src="phonmedia/asr_whisper.png"></p>
<hr />
<h3 id="but-it-is-wildly-effective">… but it is wildly effective</h3>
<ul>
<li><p>It works relatively quickly</p></li>
<li><p>On relatively low-end hardware</p></li>
<li><p>… and most amazing of all…</p></li>
</ul>
<h3
id="whisper-can-get-human-like-performance-in-speech-transcription">Whisper
can get human-like performance in speech transcription*</h3>
<p><img class="r-stretch" src="phonmedia/asr_whispervshumans.png"></p>
<hr />
<h3 id="wait-what-was-that-asterisk">Wait, what was that asterisk?</h3>
<hr />
</body>
</html>
