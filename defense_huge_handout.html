<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title></title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <style type="text/css">
  /*
   * I add this to html files generated with pandoc.
   * Originally from https://gist.github.com/killercup/5917178
   */

  html {
      overflow-y: scroll;
      -webkit-text-size-adjust: 100%;
      -ms-text-size-adjust: 100%;
  }

  body {
      color: #444;
      font-family: "Source Sans 3", Helvetica-Neue, Helvetica, Sans;
      line-height: 1.5;
      padding: 0.5em;
      margin: auto;
      max-width: 55em;
      background: #fefefe;
  }

  a {
      color: #2171b5;
      text-decoration: underline;
  }

  tr:nth-child(even) {background: #F8F8F8}
  tr:nth-child(odd) {background: #FFF}

  a:visited {
      color: #2171b5;
      text-decoration: none;
  }

  a:focus {
      outline: thin dotted;
  }

  *::-moz-selection {
      background: rgba(255, 255, 0, 0.3);
      color: #000;
  }

  *::selection {
      background: rgba(255, 255, 0, 0.3);
      color: #000;
  }

  a::-moz-selection {
      background: rgba(255, 255, 0, 0.3);
      color: #0645ad;
  }

  a::selection {
      background: rgba(255, 255, 0, 0.3);
      color: #0645ad;
  }

  p {
      margin: 0.75em 0;
  }

  img {
      max-width: 60%;
      max-height:400px;
  }

  video {
      max-width: 60%;
  }


  h1, h2, h3, h4, h5, h6 {
      color: #111;
      line-height: 80%;
      margin-top: 1em;
      margin-bottom: 0.5em;
      font-weight: normal;
  }

  h1, h2, h3, h4, h5, h6 {
      font-weight: bold;
  }

  h1 {
      font-size: 2em;
      line-height: 1.25;
      color:  #084594;

  }

  h1.title {
      margin-top:0.2em;
      font-size: 2em;
      line-height: 1.25;
  }

  h2 {
      font-size: 1.5em;
      line-height: 1.6em;
          color:  #084594;
      padding-bottom: 3px;

  }

  h3 {
      font-size: 1.2em;
      line-height: 1.6em;
  }


  h4 {
      font-size: 1.2em;
      line-height: 1.4em;
  }

  h5 {
      font-size: 1em;
  }

  h6 {
      font-size: 0.9em;
  }

  blockquote {
      color: #666666;
      margin: 0;
      padding-left: 3em;
      border-left: 0.5em #EEE solid;
  }

  hr {
      display: block;
      height: 2px;
      border: 0;
      border-top: 1px solid #aaa;
      border-bottom: 1px solid #eee;
      margin: 0.5em 0;
      padding: 0;
  }

  pre, code, kbd, samp {
      color: #000;
      font-family: monospace, monospace;
      _font-family: 'courier new', monospace;
      font-size: 0.98em;
  }

  pre {
      white-space: pre;
      white-space: pre-wrap;
      word-wrap: break-word;
  }

  .answer {
      color:#CC0033;
      font-style:italic;
  }

  b, strong {
      font-weight: bold;
  }

  dfn {
      font-style: italic;
  }

  ins {
      background: #ff9;
      color: #000;
      text-decoration: none;
  }

  mark {
      background: #ff0;
      color: #000;
      font-style: italic;
      font-weight: bold;
  }

  sub, sup {
      font-size: 75%;
      line-height: 0;
      position: relative;
      vertical-align: baseline;
  }

  sup {
      top: -0.5em;
  }

  sub {
      bottom: -0.25em;
  }

  ul, ol {
      margin: 0.5em 0;
      padding: 0em 0em 0em 1em;
  }

  ul img {
      list-style-type: none;
  }

  li p:last-child {
      margin-bottom: 0;
  }

  hr {
      border-top:none;
      height:0px;
      clear:both;
  }

  ul ul, ol ol {
      margin: .3em 0;
  }

  dl {
      margin-bottom: 1em;
  }

  dt {
      font-weight: bold;
      margin-bottom: .8em;
  }

  dd {
      margin: 0 0 .8em 2em;
  }

  dd:last-child {
      margin-bottom: 0;
  }

  img {
      border: 0;
      -ms-interpolation-mode: bicubic;
      vertical-align: middle;
  }

  figure {
      display: block;
      text-align: center;
      margin: 1em 0;
  }

  figure img {
      border: none;
      margin: 0 auto;
  }

  figcaption {
      font-size: 0.8em;
      font-style: italic;
      margin: 0 0 .8em;
  }

  table {
      margin-bottom: 2em;
      border-bottom: 1px solid #ddd;
      border-right: 1px solid #ddd;
      border-spacing: 0;
      border-collapse: collapse;
  }

  table th {
      padding: .2em 1em;
      background-color: #eee;
      border-top: 1px solid #ddd;
      border-left: 1px solid #ddd;
  }

  table td {
      padding: .2em 1em;
      border-top: 1px solid #ddd;
      border-left: 1px solid #ddd;
      vertical-align: top;
  }

  .author {
      font-size: 1.2em;
      text-align: center;
  }

  @media only screen and (min-width: 480px) {
      body {
  	font-size: 14px;
      }
  }
  @media only screen and (min-width: 768px) {
      body {
  	font-size: 16px;
      }
  }
  @media print {
      * {
  	background: transparent !important;
  	color: black !important;
  	filter: none !important;
  	-ms-filter: none !important;
      }

      body {
  	font-size: 12pt;
  	max-width: 100%;
      }

      a, a:visited {
  	text-decoration: underline;
      }

      hr {
  	height: 1px;
  	border: 0;
  	border-bottom: 1px solid black;
      }

      a[href]:after {
  	content: " (" attr(href) ")";
      }

      abbr[title]:after {
  	content: " (" attr(title) ")";
      }

      .ir a:after, a[href^="javascript:"]:after, a[href^="#"]:after {
  	content: "";
      }

      pre, blockquote {
  	border: 1px solid #999;
  	padding-right: 1em;
  	page-break-inside: avoid;
      }

      tr, img {
  	page-break-inside: avoid;
      }

      img {
  	max-width: 40% !important;
      max-height: 300px !important;
      }

      @page :left {
  	margin: 15mm 20mm 15mm 10mm;
      }

      @page :right {
  	margin: 15mm 10mm 15mm 20mm;
      }

      p, h2, h3 {
  	orphans: 3;
  	widows: 3;
      }

      h2, h3 {
  	page-break-after: avoid;
      }
  }


  ldata {
  	font-size: 0.7em;
  	margin-bottom: 0em;
  	color:#808080;
  	font-style:italic;
  }

  danger {
  	color:#FF0000;
  	font-weight:bold;
  }

  correct {
  	color:#39C900;
  	font-weight:bold;
  }

  clg{
      color:#39C900;
  	font-weight:bold;
  }

  clr{
  	color:#FF0000;
  	font-weight:bold;
  }

  clb{
  	color:#0000CC;
  	font-weight:bold;
  }

  clp{
  	color:#6600FF;
  	font-weight:bold;
  }

  clk{
  	color:#708cef;
  	font-weight:bold;
  }

  clo{
  	color:#CC6600;
  	font-weight:bold;
  }

  sc{
          font-variant: small-caps;
  }

  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<p><img class="big" src="humorimg/wizardcat.jpg"></p>
<hr />
<h1 id="on-the-acoustical-and-perceptual-features-of-vowel-nasality">On
the Acoustical and Perceptual Features of Vowel Nasality</h1>
<h3 id="will-styler">Will Styler</h3>
<hr />
<h3 id="acknowledgements">Acknowledgements</h3>
<ul>
<li><p>My Advisor, Rebecca</p></li>
<li><p>My Committee</p></li>
<li><p>Luciana Marques, Georgia Zellou, and Story Kiser</p></li>
<li><p>The rest of the CU Linguistic Community</p>
<ul>
<li>The Illocutionary Force</li>
</ul></li>
</ul>
<hr />
<h3 id="more-acknowledgement">More Acknowledgement</h3>
<ul>
<li><p>My family</p></li>
<li><p>Jessica</p></li>
<li><p>Vowels</p></li>
</ul>
<hr />
<h3 id="hi-im-will.">Hi! I’m Will.</h3>
<hr />
<h1 id="ive-got-a-nasality-problem.">I’ve got a Nasality problem.</h1>
<hr />
<h3 id="vowel-nasality">Vowel Nasality</h3>
<p>Opening the Velopharyngeal Port during vowel production to allow
nasal airflow</p>
<hr />
<p><img class="big" src="phonmedia/sagittal.png"></p>
<hr />
<h3 id="coarticulatory-nasality-in-english">Coarticulatory Nasality in
English</h3>
<center>
<table>
<tr>
<th>
‘Pats’<br>[pæts]
</th>
<th>
‘Pants’<br>[pæ̃nts]
</th>
</tr>
</table>
</center>
<hr />
<h3 id="contrastive-nasality-in-lakota">Contrastive Nasality in
Lakota</h3>
<center>
<table>
<tr>
<th>
‘seed’ <br>[su]
</th>
<th>
‘braid’<br>[sũ]
</th>
</tr>
</table>
</center>
<audio controls>
<source src="phonmedia/della_su-396.wav" type="audio/wav">
</audio>
<audio controls>
<source src="phonmedia/della_suN_102-397.wav" type="audio/wav">
</audio>
<table style="width:6%;">
<colgroup>
<col style="width: 5%" />
</colgroup>
<tbody>
<tr class="odd">
<td>### Humans are OK with vowel nasality</td>
</tr>
<tr class="even">
<td>* … Yet it’s complicated for Linguists…</td>
</tr>
</tbody>
</table>
<blockquote>
<p>“…To do my experiment I will need to find the point where nasality
starts in a vowel, and I am struggling with that a bit. </p>
</blockquote>
<h2
id="would-you-have-an-idea-about-possible-ways-to-look-for-this-point-in-time-where-nasality-actually-starts-for-each-vowel-based-on-sound">&gt;
Would you have an idea about possible ways to look for this point in
time where nasality actually starts for each vowel, based on
sound?”</h2>
<h1 id="nope.">Nope.</h1>
<hr />
<p>Our current methods just aren’t that accurate</p>
<table style="width:6%;">
<colgroup>
<col style="width: 5%" />
</colgroup>
<tbody>
<tr class="odd">
<td>### A1-P0: The Reigning Champion</td>
</tr>
<tr class="even">
<td><img src="phonmedia/chen1997figure.png"></td>
</tr>
<tr class="odd">
<td>* <em>“Nasality makes the vowel formants drop in power, and
introduces a nasal resonance. Compare the two.”</em></td>
</tr>
</tbody>
</table>
<h3 id="a1-p0-lets-us-say-things-about-classes-of-vowels.">A1-P0 lets us
say things about <em>classes</em> of vowels.</h3>
<hr />
<p>“CVN words should have increasing nasality through the vowel”</p>
<p>(A1-P0 should drop)</p>
<hr />
<h3 id="sure-does">Sure does!*</h3>
<p><img class="big" src="phonmedia/scarboroughfigure.png"></p>
<ul>
<li>*<danger>(when you have 576 measurements per point)</danger></li>
</ul>
<hr />
<h3 id="but-we-cant-say-much-about-nasality-in-this-vowel-right-here.">…
but we can’t say much about nasality in this vowel right here.</h3>
<hr />
<p>Going from known-oral to known-nasal parts of vowels, nasality should
always go up.</p>
<ul>
<li><danger>A1-P0 shows this increase <em>only 56% of the
time</em>.</danger></li>
</ul>
<hr />
<p><img class="big" src="humorimg/surprisedkoala.jpg"></p>
<hr />
<p>Listeners clearly can make judgements about nasality in individual
vowels*, but linguists can’t.</p>
<ul>
<li><p><small>(c.f. Lahiri and Marslen-Wilson 1991, Beddor and Krakow
1999, Beddor 2013, Kingston and Macmillin 1995, Macmillin et al
1999)</small></p></li>
<li><h2 id="clearly-then-nasality-is-magic.">Clearly, then, Nasality is
Magic.</h2></li>
</ul>
<hr />
<p><img class="big" src="img/magic.jpg"></p>
<hr />
<h2 id="or-we-just-dont-know-what-makes-nasals-nasal-for-humans.">(Or we
just don’t know what makes nasals nasal for humans.)</h2>
<hr />
<h3 id="thats-where-i-come-in">That’s where I come in!</h3>
<p><img class="big" src="img/will_thumbsup.jpg"></p>
<hr />
<h3 id="two-goals">Two Goals</h3>
<ul>
<li><ol type="1">
<li>Figure out what acoustical features change with nasality in English
and French</li>
</ol></li>
<li><ol start="2" type="1">
<li>Figure out which ones humans are actually <em>using</em></li>
</ol></li>
</ul>
<hr />
<h2 id="the-fundamental-problem">The Fundamental Problem</h2>
<ul>
<li><h3 id="there-are-a-lot-of-possible-features-for-nasality">There are
a <em>lot</em> of possible features for nasality</h3></li>
<li><img src="phonmedia/chen1997figure.png"></li>
</ul>
<hr />
<h2 id="the-plan">The Plan</h2>
<ul>
<li><p>Collect Data and measure possible features</p></li>
<li><p><strong>Experiment 1</strong> - What features are statistically
linked to nasality?</p></li>
<li><p><strong>Experiment 2</strong> - What features are useful for
identifying nasal vowels by machine?</p>
<ul>
<li>(These two experiments combine to tell us which features look most
promising)</li>
</ul></li>
<li><p><strong>Experiment 3</strong> - What features are humans using to
perceive nasality?</p></li>
<li><p><strong>Experiment 4</strong> - Do computers show a similar
perceptual pattern?</p></li>
</ul>
<hr />
<h1 id="data-collection">Data Collection!</h1>
<hr />
<h3 id="data-collection-1">Data Collection</h3>
<ul>
<li><p>I recorded 12 English and 8 French speakers making words with
oral and nasal(ized) vowels</p>
<ul>
<li><p>For English, we recorded CVC/CVN/NVC/NVN words</p></li>
<li><p>For French, we recorded nasal/oral vowel minimal pairs</p></li>
</ul></li>
<li><p>Find things that <em>could</em> encode nasality, and measure
them!</p>
<ul>
<li><p>All measurement was done automatically by Praat Script</p></li>
<li><p>Measurements happened at two timepoints per vowel</p></li>
</ul></li>
</ul>
<hr />
<h3 id="feature-selection">Feature Selection</h3>
<p><img src="phonmedia/chen1997figure.png"></p>
<hr />
<p><img class="big" src="img/diss_featurelist.png"></p>
<hr />
<h3 id="lets-talk-about-a-few-features-more-specifically">Let’s talk
about a few features more specifically</h3>
<hr />
<h3 id="a1-p0">A1-P0</h3>
<p><img src="phonmedia/chen1997figure.png"></p>
<hr />
<h3 id="p0-prominence">P0 Prominence</h3>
<p><img src="phonmedia/chen1997figure.png"></p>
<hr />
<h3 id="vowel-formant-bandwidth">Vowel Formant Bandwidth</h3>
<p><img class="big" src="phonmedia/iformantslabeled.png
"></p>
<table style="width:6%;">
<colgroup>
<col style="width: 5%" />
</colgroup>
<tbody>
<tr class="odd">
<td>### Vowel Formant Bandwidth</td>
</tr>
<tr class="even">
<td><img src="phonmedia/ispectrum.png"></td>
</tr>
</tbody>
</table>
<h3 id="vowel-duration">Vowel Duration</h3>
<p><img class="big" src="img/stopwatch.png"></p>
<hr />
<h3 id="spectral-tilt-a3-p0">Spectral Tilt (A3-P0)</h3>
<p><img src="phonmedia/ispectrum.png"></p>
<hr />
<h3 id="the-data">The Data!</h3>
<ul>
<li><p>A file showing data 29 different features for two measurements
per vowel per word</p>
<ul>
<li>15,449 rows!</li>
</ul></li>
<li><p>Annotated with information about language, speaker, phonological
structure, etc</p></li>
<li><p>All of this was read into the R Statistics Suite</p></li>
</ul>
<hr />
<h1 id="experiment-1-statistical-analysis">Experiment 1: Statistical
Analysis!</h1>
<hr />
<h3 id="the-idea">The Idea</h3>
<ul>
<li><p><em>“If a feature doesn’t meaningfully change between oral and
nasal vowels, humans won’t use it.”</em></p></li>
<li><p><strong>Let’s test which features are different in oral and nasal
vowels!</strong></p></li>
</ul>
<hr />
<h3 id="the-plan-1">The Plan</h3>
<ul>
<li><ol type="1">
<li>Run a bunch of Linear Mixed-Effects Analyses for English and
French</li>
</ol>
<ul>
<li>This will show the <em>statistical</em> link between the features
and nasality</li>
</ul></li>
<li><ol start="2" type="1">
<li>See which features showed significant differences in nasal(ized)
vowels</li>
</ol>
<ul>
<li>… and how large the ∆Feature is for each</li>
</ul></li>
<li><ol start="3" type="1">
<li>See if this differs in English and French</li>
</ol></li>
</ul>
<hr />
<h3 id="the-analyses">The Analyses</h3>
<blockquote>
<p>lmer(Amp_F1 ~ nasality + repetition + vowel + Timepoint + (1+
nasality|speaker) + (1|Word), data = eng)</p>
</blockquote>
<ul>
<li><p>This compares the “nasal” vowels to the “oral” vowels</p>
<ul>
<li>In English, this means CVC to CVN/NVC/NVN</li>
</ul></li>
<li><p>Random slopes for speaker allow by-speaker variation in
<em>amount</em> of change</p></li>
<li><p>Not all features showed a significant statstical link with
nasality!</p></li>
</ul>
<hr />
<h3 id="the-findings-english">The Findings (English)</h3>
<p><img class="big" src="img/diss_encorr.png"></p>
<hr />
<h3 id="the-findings-french">The Findings (French)</h3>
<p><img class="big" src="img/diss_frcorr.png"></p>
<hr />
<h3 id="the-most-promising-features">The <em>Most Promising</em>
Features</h3>
<ul>
<li><p><strong>A1-P0</strong> performed well in both languages</p></li>
<li><p><strong>Duration</strong> showed major changes in both
languages</p>
<ul>
<li>(English nasalized vowels appear shorter, French nasal vowels appear
longer)</li>
</ul></li>
<li><p><strong>Spectral Tilt</strong> was really strong in French, less
so in English</p></li>
<li><p><strong>Formant Bandwidth</strong> was really strong in both
languages</p></li>
<li><p>There was some <strong>Formant Frequency</strong> effect
too</p></li>
<li><p><strong>P0’s Prominence</strong> is looking pretty good
too.</p></li>
</ul>
<hr />
<h3 id="experiment-1-wrap-up">Experiment 1 Wrap-up</h3>
<ul>
<li><p>Humans probably don’t use the features that didn’t show
significant oral-to-nasal ∆Feature</p></li>
<li><p>We now know which features are linked with nasality <em>across
the entire dataset</em></p></li>
<li><p>… and which ones show the largest and most meaningful ∆Feature
values</p></li>
</ul>
<hr />
<h3
id="but-how-do-we-know-if-theyre-actually-helpful-for-identifying-nasality">…
but how do we know if they’re actually helpful for identifying
nasality?</h3>
<ul>
<li><p>These tests show <em>overall trends</em> across several thousand
words</p></li>
<li><p>What about vowel-by-vowel identification? How can we test
that?</p></li>
</ul>
<hr />
<p>Uh… do a perception experiment?</p>
<hr />
<h3 id="the-problem">The Problem:</h3>
<p>Perceptual testing using humans is inefficient and expensive.
<!-- .element: class="fragment" --></p>
<hr />
<p><img class="big" src="img/cash.png"></p>
<hr />
<p><img class="big" src=img/feast.jpg></p>
<hr />
<p><img class="big" src=img/house.jpg></p>
<hr />
<h3 id="and-they-cheat">… and they cheat</h3>
<ul>
<li><p>“I’m rooting for the <strong>pats</strong> in the Super
Bowl”</p></li>
<li><p>“One should likely wear <strong>pants</strong> to a thesis
defense”</p></li>
</ul>
<hr />
<h3 id="and-they-give-awful-feedback">… and they give awful
feedback</h3>
<ul>
<li><p>“Uh, it just sounded like”pants”, bro.”</p></li>
<li><p>“Well, that one sounded more nasally.”</p></li>
<li><p>“Can I get my extra credit already?”</p></li>
</ul>
<hr />
<h3 id="the-solution">The Solution?</h3>
<hr />
<h2 id="ask-a-computer">Ask a Computer!</h2>
<h2 id="section"><img class="big" src="img/hal9000.jpg"></h2>
<h1 id="experiment-2-machine-learning">Experiment 2: Machine
Learning!</h1>
<hr />
<h3 id="the-idea-1">The Idea</h3>
<p>Humans hear a signal, find acoustical features, and then make
judgements.</p>
<ul>
<li><p><strong>Machines can be given features, and then make judgements
too</strong>.</p></li>
<li><p>Better accuracy with a feature means the feature is more
<em>useful</em>.</p></li>
</ul>
<hr />
<h3 id="machines-have-some-advantages">Machines have some
advantages!</h3>
<ul>
<li><p>Their decisions are easier to quantify.</p></li>
<li><p>They’ll tell you <em>how</em> they made the decision they
did.</p></li>
<li><p>They live in my apartment!</p></li>
<li><p>They have no idea what “pants” are, and don’t watch
football.</p></li>
</ul>
<hr />
<h3 id="the-plan-2">The Plan</h3>
<ul>
<li><ol type="1">
<li>Choose an Algorithm</li>
</ol></li>
<li><ol start="2" type="1">
<li>Give all 29 features to a Machine Learning Algorithm
individually</li>
</ol>
<ul>
<li>The most accurate features should be the most useful</li>
</ul></li>
<li><ol start="3" type="1">
<li>Give them <em>everything</em>, and ask which features are most
useful.</li>
</ol>
<ul>
<li>Feature weighting and importance are handy!</li>
</ul></li>
<li><ol start="4" type="1">
<li>Find the best group of features</li>
</ol>
<ul>
<li>Find the balance between “few features” and “good accuracy”</li>
</ul></li>
<li><ol start="5" type="1">
<li>Test <em>those</em> features with expensive humans.</li>
</ol></li>
</ul>
<hr />
<h1 id="choosing-algorithms">Choosing Algorithms</h1>
<hr />
<h3 id="machine-classification">Machine Classification</h3>
<blockquote>
<p>“Is this datapoint likely in class A, or class B?”</p>
</blockquote>
<ul>
<li><p>“Is the car driving normally, or crashing?”</p></li>
<li><p>“Is this language English or Chinese?”</p></li>
<li><p>“Is this handwritten symbol”1”? “2”? “3”? (…)</p></li>
<li><p>“Is this word a noun, or a verb, or an adjective, or…?”</p></li>
</ul>
<hr />
<h3 id="my-algorithms-of-choice">My Algorithms of Choice</h3>
<ul>
<li>RandomForests
<ul>
<li>Because they’re transparent</li>
</ul></li>
<li>Support Vector Machines
<ul>
<li>Because they’re the gold standard</li>
</ul></li>
</ul>
<hr />
<p>Before we discuss RandomForests, we need to talk about…</p>
<hr />
<h2 id="decision-trees">## Decision Trees</h2>
<p>Let’s pretend to be classifiers!</p>
<hr />
<p><img class="big" src="img/kayaking.jpg"></p>
<hr />
<blockquote>
<p>“I’m looking at a bird. What kind of bird is it?”</p>
</blockquote>
<hr />
<p>One Approach:</p>
<ul>
<li><strong>Ask questions, then make decisions based on the
answer!</strong></li>
</ul>
<hr />
<h2 id="section-1"><img width=1200px src="img/birds_waterfowl.png"></h2>
<p>By asking enough questions looking at a training set, you’d end up
with a <strong>Decision Tree</strong>.</p>
<ul>
<li><p>Classification is just “following the tree”</p></li>
<li><p>Ask a question, then ask a different question based on the first
one, then ask another….</p></li>
</ul>
<hr />
<h2 id="randomforests">RandomForests</h2>
<hr />
<h3 id="to-make-a-randomforest">To make a RandomForest:</h3>
<ul>
<li><ol type="1">
<li>Make a decision tree using a subset of the features and data</li>
</ol></li>
<li><ol start="2" type="1">
<li>Make another decision tree using another random subset of features
and data</li>
</ol></li>
<li><p>3-500) Do that 498 more times</p></li>
<li><ol start="501" type="1">
<li>Synthesize these models into a single, best-performing model</li>
</ol></li>
<li><ol start="502" type="1">
<li>Classify using that mega-tree!</li>
</ol></li>
</ul>
<hr />
<p>Let’s make a RandomForest!</p>
<hr />
<p><img class="big" src="img/birds_waterfowl.png"></p>
<hr />
<h3 id="randomforests-are-great">RandomForests are great!</h3>
<ul>
<li><p>They work well with small and large datasets</p></li>
<li><p>They’re transparent!</p></li>
<li><p>… but they’re not the most accurate algorithms out there</p></li>
<li><p>… and they’re a bit… odd sometimes.</p></li>
<li><p>So we should also use a model which is more accurate</p>
<ul>
<li>… and less weird</li>
</ul></li>
</ul>
<hr />
<h2 id="support-vector-machines">Support Vector Machines!</h2>
<hr />
<p>Back to the waterfowl!</p>
<hr />
<center>
<table>
<tr>
<th>
<img height="1000" src="img/bird_mallard.jpg">
</th>
<th>
<img height="1000" src="img/bird_swan.jpg">
</th>
</tr>
</table>
</center>
<hr />
<h3
id="your-kayaking-relative-has-taken-a-hands-on-approach-to-classification">Your
Kayaking Relative has taken a hands-on approach to classification</h3>
<ul>
<li><p>You are now recieving texts with bill length and body-length
measurements for birds</p></li>
<li><p>The question is “Swan, or Duck?”</p></li>
</ul>
<hr />
<p><img class="big" src="img/birds_lengthsize.png"></p>
<hr />
<h3 id="support-vector-machines-1">Support Vector Machines</h3>
<ul>
<li><p>Look at all the data in an n dimensional space</p>
<ul>
<li>n is the number of features</li>
</ul></li>
<li><p>Try to find a hyperplane with the best separation</p></li>
<li><p>This hyperplane is delineated by the “support vectors”</p></li>
<li><p>Classification is just seeing where the new data is relative to
that line</p></li>
</ul>
<hr />
<p><img class="big" src="img/birds_lengthsize_line.png"></p>
<hr />
<p>(My approach was slightly more complex, using “kernels”, but you’ll
have to read the paper for more info!)</p>
<hr />
<h3 id="support-vector-machines-2">Support Vector Machines</h3>
<ul>
<li><p>SVMs are <em>really</em> accurate</p>
<ul>
<li>… and anything that beats them is usually really complex</li>
</ul></li>
<li><p>They act exemplar-ish, when used as I used them.</p></li>
<li><p>They’re a “gold standard” for machine learning</p></li>
</ul>
<hr />
<h3 id="so-we-have-two-algorithms">So, we have two algorithms</h3>
<ul>
<li><p>RandomForests for transparency</p></li>
<li><p>SVMs for accuracy</p></li>
</ul>
<hr />
<h2 id="lets-do-some-classification">Let’s do some classification!</h2>
<hr />
<h2 id="single-feature-tests">Single-feature tests</h2>
<hr />
<h3 id="single-feature-testing">Single-Feature testing</h3>
<ul>
<li><p>Are any features good enough <em>on their own</em> to allow nasal
perception?</p></li>
<li><p>Using both SVMs and RandomForests.</p></li>
<li><p>Using 10-fold cross-validation</p></li>
<li><p>116 models, one per feature per algorithm per language</p></li>
</ul>
<hr />
<p><img src="img/diss_overallacc.png"></p>
<hr />
<p><img src="img/diss_by_feature_en.png"></p>
<hr />
<p><img src="img/diss_by_feature_fr.png"></p>
<hr />
<p>So, none of the features are good enough on their own.</p>
<ul>
<li><h3 id="what-about-as-part-of-a-larger-group">What about as part of
a larger group?</h3></li>
</ul>
<hr />
<h2 id="evaluating-feature-importance">Evaluating Feature
Importance</h2>
<hr />
<h3 id="randomforest-importance">RandomForest Importance</h3>
<ul>
<li><p>RandomForests can calculate <em>which features were most
useful</em> for classification!</p></li>
<li><p>Reclassify, but shuffle the data for one feature per run</p>
<ul>
<li>If this hurts the accuracy, that feature’s important!</li>
</ul></li>
<li><p>This is <em>awesome</em>!</p></li>
</ul>
<hr />
<h3 id="evaluating-feature-importance-1">Evaluating Feature
Importance</h3>
<ul>
<li><p>Run an all-features-included RandomForest</p></li>
<li><p>Compare the Importance Values for each feature</p></li>
</ul>
<hr />
<p><img class="big" src="img/diss_by_feature_importance.png"></p>
<hr />
<p>… I wonder if all these important features would perform well as a
group…?</p>
<hr />
<h2 id="multi-feature-models">Multi-feature Models</h2>
<hr />
<h3 id="multi-feature-modeling">Multi-feature modeling</h3>
<ul>
<li><p>Pick six <em>a priori</em> feature groupings</p>
<ul>
<li>There are 20,030,007 other possible groupings of 10 features out of
29</li>
</ul></li>
<li><p>Test them with SVMs and RandomForests</p></li>
<li><p>Compare accuracy <em>in light of the number of features</em></p>
<ul>
<li><h2 id="more-features-will-usually-mean-better-accuracy">More
features will usually mean better accuracy</h2></li>
</ul></li>
</ul>
<p><img src="img/diss_multifeature.png"></p>
<hr />
<p>Remember, we did this for both English and French</p>
<ul>
<li><h3 id="so-we-can-ask-some-fun-questions">So we can ask some fun
questions!</h3></li>
</ul>
<hr />
<h3
id="do-english-and-french-differ-in-terms-of-which-features-are-important">Do
English and French differ in terms of which features are important?</h3>
<ul>
<li><img src="img/diss_enfrimportance.png"></li>
</ul>
<hr />
<h3 id="does-the-same-classifier-work-well-on-both">Does the same
classifier work well on both?</h3>
<ul>
<li><p>What happens if you train a model on English, then test it on
French?</p>
<ul>
<li>… and vice versa.</li>
</ul></li>
<li><p>If they’re fundamentally similar, it won’t matter!</p></li>
</ul>
<hr />
<h3 id="cross-language-classification">Cross-language
Classification</h3>
<p><img src="img/diss_enfraccuracy.png"></p>
<hr />
<h2 id="french-and-english-do-nasality-differently">French and English
do nasality differently</h2>
<ul>
<li><img class="big" src="humorimg/surprisedbuffalo.jpg"></li>
</ul>
<hr />
<h3 id="the-findings">The Findings</h3>
<ul>
<li><p><strong>P0’s Prominence</strong> was not very useful at all.</p>
<ul>
<li>We can eliminate it going forward.</li>
</ul></li>
<li><p><strong>Formant Bandwidth</strong> was the best feature for
English, strong in French</p></li>
<li><p><strong>Spectral Tilt</strong> was the most useful feature in
French, less so in English</p></li>
<li><p><strong>A1-P0</strong> performed well in both languages</p></li>
<li><p><strong>Duration</strong> was <em>really</em> useful in both
languages</p>
<ul>
<li>… but this could be because it lends itself particularly well to
classification</li>
</ul></li>
<li><p>There was some <strong>Formant Frequency</strong> effect
too</p></li>
</ul>
<hr />
<p>So… uh… what about humans?</p>
<hr />
<h1 id="experiment-3-human-perception">Experiment 3: Human
Perception</h1>
<hr />
<h3 id="the-idea-2">The Idea</h3>
<blockquote>
<p>“English can use vowel nasality to identify ambiguous words. Let’s
see which of these features is helpful!”</p>
</blockquote>
<ul>
<li><p>Create nasal vowels where each nasal feature is
<em>reduced</em></p></li>
<li><p>Create oral vowels where each nasal feature is
<em>added</em></p></li>
<li><p>Put them in contexts where part of the word is missing - ba(d) or
ba(n)</p></li>
<li><p>If the listeners are confused more often or take more time to
choose, the feature’s important!</p></li>
</ul>
<hr />
<h3 id="the-plan-3">The Plan</h3>
<ul>
<li><ol type="1">
<li>Create modified stimuli</li>
</ol>
<ul>
<li>Modify A1-P0, A3-P0, Formant Structure and Duration</li>
</ul></li>
<li><ol start="2" type="1">
<li>Create Control Stimuli</li>
</ol>
<ul>
<li>Make similar modifications, but then reverse them</li>
</ul></li>
<li><ol start="3" type="1">
<li>Present both to English listeners</li>
</ol></li>
<li><ol start="4" type="1">
<li>Analyze Accuracy and Reaction Time</li>
</ol>
<ul>
<li>If modifying different conditions shows different effects, we will
understand the cue!</li>
</ul></li>
</ul>
<hr />
<h3 id="the-modifications">The Modifications</h3>
<h2
id="section-2"><img class="big" src="img/diss_deltafeature.png"></h2>
<h3 id="the-modifications-1">The Modifications</h3>
<h2
id="section-3"><img class="big" src="img/diss_deltabandwidth.png"></h2>
<h3 id="the-experiment">The Experiment</h3>
<hr />
<center>
<table>
<tr>
<th>
<h1>
bad
</h1>
</th>
<th>
<h1>
ban
</h1>
</th>
</tr>
</table>
</center>
<audio controls>
<source src="phonmedia/diss_hazel_BAD_nfor_ex_c.wav" type="audio/wav">
</audio>
<hr />
<center>
<table>
<tr>
<th>
<h1>
bomb
</h1>
</th>
<th>
<h1>
bob
</h1>
</th>
</tr>
</table>
</center>
<audio controls>
<source src="phonmedia/diss_molly_BOMB_ofor_ex_c.wav" type="audio/wav">
</audio>
<table style="width:6%;">
<colgroup>
<col style="width: 5%" />
</colgroup>
<tbody>
<tr class="odd">
<td></td>
</tr>
<tr class="even">
<td></td>
</tr>
<tr class="odd">
<td><audio controls>
<source src="phonmedia/diss_hazel_DAD_ndur_ex_o.wav" type="audio/wav">
</audio></td>
</tr>
</tbody>
</table>
<h3 id="the-analysis">The Analysis</h3>
<ul>
<li><p><strong>Reduction</strong> stimuli, where nasal features are
<em>reduced</em> in nasal vowels</p></li>
<li><p><strong>Addition</strong> stimuli, where nasal features are
<em>added</em> to oral vowels</p></li>
<li><p>Linear Mixed Effects Regressions for accuracy and RT by
Condition*Control</p></li>
</ul>
<hr />
<h2 id="addition-stimuli-findings">Addition Stimuli Findings</h2>
<hr />
<p><img class="big" src="img/diss_conf.add.sum.png"></p>
<table style="width:6%;">
<colgroup>
<col style="width: 5%" />
</colgroup>
<tbody>
<tr class="odd">
<td><img class="big" src="img/diss_rt.add.sum.png"></td>
</tr>
</tbody>
</table>
<h3 id="addition-summary">Addition Summary</h3>
<ul>
<li><p>Modifying all conditions or formants resulted in more
confusion</p></li>
<li><p>Modifying all conditions or formants resulted in slower
responses</p>
<ul>
<li>Post-hoc tests show that “All” and “Formant” modification did not
meaningfully differ for either</li>
</ul></li>
<li><p><strong>Only modifying formant frequency and bandwidth had an
effect on perception</strong></p></li>
</ul>
<hr />
<h2 id="removal-stimuli-findings">Removal Stimuli Findings</h2>
<table style="width:6%;">
<colgroup>
<col style="width: 5%" />
</colgroup>
<thead>
<tr class="header">
<th><img class="big" src="img/diss_conf.rem.sum.png"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><img class="big" src="img/diss_rt.rem.sum.png"></td>
</tr>
</tbody>
</table>
<h3 id="removal-summary">Removal Summary</h3>
<ul>
<li><p><em>None of the experimental modifications</em> affected
confusion</p></li>
<li><p>Modifying all conditions or formants resulted in slower
responses</p>
<ul>
<li>Post-hoc tests show that “All” and “Formant” modification did not
meaningfully differ</li>
</ul></li>
<li><p><strong>Only modifying formant frequency and bandwidth had an
effect on perception</strong></p>
<ul>
<li>… but it wasn’t enough to change classification!</li>
</ul></li>
</ul>
<hr />
<h3 id="experiment-3-summary">Experiment 3 Summary</h3>
<ul>
<li><p>Only <strong>formant modification</strong> had a significant
effect on perception</p></li>
<li><p>Formant modification caused listeners to respond more
slowly</p></li>
<li><p>Formant modification made listeners call some oral vowels
“nasal”</p></li>
<li><p>Formant modification <strong>wasn’t enough</strong> to make nasal
vowels “oral”</p></li>
</ul>
<hr />
<p>(We’ll talk more about that asymmetry at the end!)</p>
<hr />
<p>So, computers predicted F1’s bandwidth as the most useful
feature…</p>
<ul>
<li><h3 id="how-similar-are-the-svms-and-the-humans">How similar
<em>are</em> the SVMs and the humans?</h3></li>
</ul>
<hr />
<h1 id="experiment-4-humans-vs.-machines">Experiment 4: Humans
vs. Machines</h1>
<ul>
<li><img class="big" src="img/terminator.png"></li>
</ul>
<hr />
<h3 id="the-idea-3">The Idea</h3>
<blockquote>
<p>“Let’s give the computer the same experimental task as the humans,
using the same altered stimuli, and see how they compare!”</p>
</blockquote>
<hr />
<h3 id="the-plan-4">The Plan</h3>
<ul>
<li><ol type="1">
<li>Train SVMs on different datasets</li>
</ol></li>
<li><ol start="2" type="1">
<li>Test those SVMs on the experimental stimuli (classifying “oral” or
“nasal”)</li>
</ol></li>
<li><ol start="3" type="1">
<li>Compare the by-condition results to the humans</li>
</ol></li>
</ul>
<hr />
<h3 id="the-svms">The SVMs</h3>
<ul>
<li><p>NoNVN - Trained on CVCs, CVNs, and NVCs</p></li>
<li><p>EnAll - Trained on <em>all</em> the English data</p></li>
<li><p>EnFrAll - Trained on all data, English <em>and</em>
French</p></li>
</ul>
<hr />
<p><img src="img/diss_stimml_human_vs_machine_conex.png"></p>
<hr />
<p><img src="img/diss_stimml_human_vs_machine_con.png"></p>
<hr />
<p><img src="img/diss_stimml_human_vs_machine_ex.png"></p>
<hr />
<p><img src="img/diss_humanvsmachine_rankings.png"></p>
<hr />
<h3 id="experiment-4-summary">Experiment 4 Summary</h3>
<ul>
<li><p>Humans and machines <em>did</em> show similar patterns</p></li>
<li><p>Putting aside duration, the EnAll SVM mirrored the humans very
well</p></li>
<li><p>Perceptual testing with machine learning isn’t crazy</p></li>
<li><p>Humans still win.</p></li>
</ul>
<hr />
<h3 id="hooray">Hooray!</h3>
<p><img class="big" src="img/morpheus.png"></p>
<hr />
<h1 id="so-what-does-it-all-mean">So… what does it all mean?</h1>
<hr />
<h3 id="formants-are-the-cue-to-nasality-perception-in-english">Formants
are the cue to nasality perception in English</h3>
<ul>
<li><p>It’s probably F1’s bandwidth</p>
<ul>
<li><p>It worked best in ML, had the best statistical link, and it makes
sense acoustically</p></li>
<li><p>Hawkins and Stevens (1985) also points that direction</p></li>
</ul></li>
<li><p>… but it’s probably not the <em>only</em> cue for vowel
nasality</p></li>
</ul>
<hr />
<h3
id="reducing-formant-bandwidth-doesnt-make-nasal-vowels-oral">Reducing
Formant Bandwidth doesn’t make nasal vowels “oral”</h3>
<ul>
<li><p>There was still something “nasal” about the vowels</p></li>
<li><p>We only took away <em>universal</em> formant changes</p>
<ul>
<li>We modeled the changes that happen in <em>every</em> nasal
vowels</li>
</ul></li>
<li><p>Vowel-specific formants changes in nasal vowels were <em>not</em>
affected</p></li>
</ul>
<hr />
<h3 id="maybe-nasal-vowels-are-produced-differently-in-the-mouth">Maybe
nasal vowels are produced differently in the mouth?</h3>
<ul>
<li><p>This is in line with prior work</p>
<ul>
<li>(Carignan et al. (2015), Carignan (2014), Carignan et al. (2011) and
Shosted et al. (2012))</li>
</ul></li>
<li><p>This makes phonological sense</p>
<ul>
<li>Nasal vowel systems are often really different than the oral vowel
systems</li>
</ul></li>
<li><p>If nasal vowels are <em>orally</em> different, then of course we
wouldn’t confuse listeners</p></li>
</ul>
<hr />
<h3 id="so-wrapping-up">So wrapping up</h3>
<ul>
<li><p>Our current measurements of nasality aren’t bad!</p></li>
<li><p>Machines <em>can</em> accurately classify nasality</p>
<ul>
<li>… and simulate human perception</li>
</ul></li>
<li><p>The acoustics of nasality <em>per se</em> are clearly useful</p>
<ul>
<li>Particularly formant bandwidth!</li>
</ul></li>
<li><p>… but other aspects of the vowel articulation are important
too!</p></li>
</ul>
<hr />
<p>Most importantly…</p>
<hr />
<h3 id="theres-more-to-a-nasal-vowel-than-nasal-airflow">There’s more to
a “nasal vowel” than nasal airflow</h3>
<hr />
<p><huge>Thank you very much!</huge></p>
<hr />
<h3 id="references">References</h3>
<ul>
<li><p>Carignan, C. (2014). An acoustic and articulatory examination of
the oral in nasal: The oral articulations of french nasal vowels are not
arbitrary. Journal of Phonetics, 46(0):23–33.</p></li>
<li><p>Carignan, C., Shosted, R., Shih, C., and Rong, P. (2011).
Compensatory articulation in american english nasalized vowels. Journal
of Phonetics, 39(4):668 – 682.</p></li>
<li><p>Carignan, C., Shosted, R. K., Fu, M., Liang, Z.-P., and Sutton,
B. P. (2015). A real-time mri investigation of the role of lingual and
pharyngeal articulation in the production of the nasal vowel system of
french. Journal of Phonetics, 50(0):34 – 51.</p></li>
</ul>
<hr />
<h3 id="references-continued">References Continued</h3>
<ul>
<li><p>Chen, M. Y. (1997). Acoustic correlates of english and french
nasalized vowels. The Journal of the Acoustical Society of America,
102(4):2350–2370.</p></li>
<li><p>Hawkins, S. and Stevens, K. N. (1985b). Acoustic and perceptual
correlates of the non-nasal–nasal distinction for vowels. The Journal of
the Acoustical Society of America, 77(4):1560–1575.</p></li>
<li><p>Shosted, R., Carignan, C., and Rong, P. (2012). Managing the
distinctiveness of phonemic nasal vowels: Articulatory evidence from
hindi. The Journal of the Acoustical Society of America,
131(1):455–465.</p></li>
</ul>
<hr />
</body>
</html>
