<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title></title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <style type="text/css">
  /*
   * I add this to html files generated with pandoc.
   * Originally from https://gist.github.com/killercup/5917178
   */

  html {
      overflow-y: scroll;
      -webkit-text-size-adjust: 100%;
      -ms-text-size-adjust: 100%;
  }

  body {
      color: #444;
      font-family: "Source Sans 3", Helvetica-Neue, Helvetica, Sans;
      line-height: 1.5;
      padding: 0.5em;
      margin: auto;
      max-width: 55em;
      background: #fefefe;
  }

  a {
      color: #2171b5;
      text-decoration: underline;
  }

  tr:nth-child(even) {background: #F8F8F8}
  tr:nth-child(odd) {background: #FFF}

  a:visited {
      color: #2171b5;
      text-decoration: none;
  }

  a:focus {
      outline: thin dotted;
  }

  *::-moz-selection {
      background: rgba(255, 255, 0, 0.3);
      color: #000;
  }

  *::selection {
      background: rgba(255, 255, 0, 0.3);
      color: #000;
  }

  a::-moz-selection {
      background: rgba(255, 255, 0, 0.3);
      color: #0645ad;
  }

  a::selection {
      background: rgba(255, 255, 0, 0.3);
      color: #0645ad;
  }

  p {
      margin: 0.75em 0;
  }

  img {
      max-width: 60%;
      max-height:400px;
  }

  video {
      max-width: 60%;
  }


  h1, h2, h3, h4, h5, h6 {
      color: #111;
      line-height: 80%;
      margin-top: 1em;
      margin-bottom: 0.5em;
      font-weight: normal;
  }

  h1, h2, h3, h4, h5, h6 {
      font-weight: bold;
  }

  h1 {
      font-size: 2em;
      line-height: 1.25;
      color:  #084594;

  }

  h1.title {
      margin-top:0.2em;
      font-size: 2em;
      line-height: 1.25;
  }

  h2 {
      font-size: 1.5em;
      line-height: 1.6em;
          color:  #084594;
      padding-bottom: 3px;

  }

  h3 {
      font-size: 1.2em;
      line-height: 1.6em;
  }


  h4 {
      font-size: 1.2em;
      line-height: 1.4em;
  }

  h5 {
      font-size: 1em;
  }

  h6 {
      font-size: 0.9em;
  }

  blockquote {
      color: #666666;
      margin: 0;
      padding-left: 3em;
      border-left: 0.5em #EEE solid;
  }

  hr {
      display: block;
      height: 2px;
      border: 0;
      border-top: 1px solid #aaa;
      border-bottom: 1px solid #eee;
      margin: 0.5em 0;
      padding: 0;
  }

  pre, code, kbd, samp {
      color: #000;
      font-family: monospace, monospace;
      _font-family: 'courier new', monospace;
      font-size: 0.98em;
  }

  pre {
      white-space: pre;
      white-space: pre-wrap;
      word-wrap: break-word;
  }

  .answer {
      color:#CC0033;
      font-style:italic;
  }

  b, strong {
      font-weight: bold;
  }

  dfn {
      font-style: italic;
  }

  ins {
      background: #ff9;
      color: #000;
      text-decoration: none;
  }

  mark {
      background: #ff0;
      color: #000;
      font-style: italic;
      font-weight: bold;
  }

  sub, sup {
      font-size: 75%;
      line-height: 0;
      position: relative;
      vertical-align: baseline;
  }

  sup {
      top: -0.5em;
  }

  sub {
      bottom: -0.25em;
  }

  ul, ol {
      margin: 0.5em 0;
      padding: 0em 0em 0em 1em;
  }

  ul img {
      list-style-type: none;
  }

  li p:last-child {
      margin-bottom: 0;
  }

  hr {
      border-top:none;
      height:0px;
      clear:both;
  }

  ul ul, ol ol {
      margin: .3em 0;
  }

  dl {
      margin-bottom: 1em;
  }

  dt {
      font-weight: bold;
      margin-bottom: .8em;
  }

  dd {
      margin: 0 0 .8em 2em;
  }

  dd:last-child {
      margin-bottom: 0;
  }

  img {
      border: 0;
      -ms-interpolation-mode: bicubic;
      vertical-align: middle;
  }

  figure {
      display: block;
      text-align: center;
      margin: 1em 0;
  }

  figure img {
      border: none;
      margin: 0 auto;
  }

  figcaption {
      font-size: 0.8em;
      font-style: italic;
      margin: 0 0 .8em;
  }

  table {
      margin-bottom: 2em;
      border-bottom: 1px solid #ddd;
      border-right: 1px solid #ddd;
      border-spacing: 0;
      border-collapse: collapse;
  }

  table th {
      padding: .2em 1em;
      background-color: #eee;
      border-top: 1px solid #ddd;
      border-left: 1px solid #ddd;
  }

  table td {
      padding: .2em 1em;
      border-top: 1px solid #ddd;
      border-left: 1px solid #ddd;
      vertical-align: top;
  }

  .author {
      font-size: 1.2em;
      text-align: center;
  }

  @media only screen and (min-width: 480px) {
      body {
  	font-size: 14px;
      }
  }
  @media only screen and (min-width: 768px) {
      body {
  	font-size: 16px;
      }
  }
  @media print {
      * {
  	background: transparent !important;
  	color: black !important;
  	filter: none !important;
  	-ms-filter: none !important;
      }

      body {
  	font-size: 12pt;
  	max-width: 100%;
      }

      a, a:visited {
  	text-decoration: underline;
      }

      hr {
  	height: 1px;
  	border: 0;
  	border-bottom: 1px solid black;
      }

      a[href]:after {
  	content: " (" attr(href) ")";
      }

      abbr[title]:after {
  	content: " (" attr(title) ")";
      }

      .ir a:after, a[href^="javascript:"]:after, a[href^="#"]:after {
  	content: "";
      }

      pre, blockquote {
  	border: 1px solid #999;
  	padding-right: 1em;
  	page-break-inside: avoid;
      }

      tr, img {
  	page-break-inside: avoid;
      }

      img {
  	max-width: 40% !important;
      max-height: 300px !important;
      }

      @page :left {
  	margin: 15mm 20mm 15mm 10mm;
      }

      @page :right {
  	margin: 15mm 10mm 15mm 20mm;
      }

      p, h2, h3 {
  	orphans: 3;
  	widows: 3;
      }

      h2, h3 {
  	page-break-after: avoid;
      }
  }


  ldata {
  	font-size: 0.7em;
  	margin-bottom: 0em;
  	color:#808080;
  	font-style:italic;
  }

  danger {
  	color:#FF0000;
  	font-weight:bold;
  }

  correct {
  	color:#39C900;
  	font-weight:bold;
  }

  clg{
      color:#39C900;
  	font-weight:bold;
  }

  clr{
  	color:#FF0000;
  	font-weight:bold;
  }

  clb{
  	color:#0000CC;
  	font-weight:bold;
  }

  clp{
  	color:#6600FF;
  	font-weight:bold;
  }

  clk{
  	color:#708cef;
  	font-weight:bold;
  }

  clo{
  	color:#CC6600;
  	font-weight:bold;
  }

  sc{
          font-variant: small-caps;
  }

  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h3 id="no-class-friday">NO CLASS FRIDAY</h3>
<ul>
<li>You are legally obligated to take the time to do something kind to
yourself</li>
</ul>
<hr />
<h1 id="sound-computers-and-asr">Sound, Computers, and ASR</h1>
<h3 id="will-styler---lign-6">Will Styler - LIGN 6</h3>
<hr />
<h3 id="how-do-acousticians-say-hello">How do acousticians say
hello?</h3>
<ul>
<li>They wave!</li>
</ul>
<hr />
<h3 id="todays-plan">Today’s Plan</h3>
<ul>
<li><p>Computers and Sound</p></li>
<li><p>Turning Signals into features</p></li>
<li><p>Automatic Speech Recognition</p></li>
</ul>
<hr />
<h3 id="weve-got-a-fundamental-problem-to-start">We’ve got a fundamental
problem, to start</h3>
<hr />
<h3 id="computers-dont-do-waves">Computers don’t do waves</h3>
<p><img class="r-stretch" src="phonmedia/sampling_raw.jpg"></p>
<p>010001110010101000100101101010101010</p>
<hr />
<h3 id="sound-is-analog-computers-are-digital">Sound is analog,
computers are digital</h3>
<ul>
<li>How do we deal with that?</li>
</ul>
<hr />
<h3 id="quantization-sampling">Quantization (‘Sampling’)</h3>
<p><img class="r-stretch" src="phonmedia/sampling_wave.jpg"></p>
<hr />
<h3 id="quantization-sampling-1">Quantization (‘Sampling’)</h3>
<p><img class="r-stretch" src="phonmedia/sampling_quantized.jpg"></p>
<hr />
<h3 id="quantization-sampling-2">Quantization (‘Sampling’)</h3>
<p><img class="r-stretch" src="phonmedia/sampling_measures.jpg"></p>
<hr />
<h3 id="analog-to-digital-conversion">Analog-to-digital conversion</h3>
<ul>
<li><p>Sample the wave many times per second</p></li>
<li><p>Record the amplitude at each sample</p></li>
<li><p>The resulting wave will faithfully capture the signal</p></li>
</ul>
<hr />
<h3 id="how-often-do-we-sample">How often do we sample?</h3>
<ul>
<li><p>This is called the ‘Sampling Rate’</p></li>
<li><p>Measured in samples per second (Hz)</p></li>
</ul>
<hr />
<h3 id="sampling-rate">Sampling Rate</h3>
<p><img class="r-stretch" src="phonmedia/sampling_quantized.jpg"></p>
<hr />
<h3 id="sampling-rate-1">Sampling Rate</h3>
<p><img class="r-stretch" src="phonmedia/sampling_highrate.jpg"></p>
<hr />
<h3 id="sampling-rate-low-rate">Sampling Rate (low rate)</h3>
<p><img class="r-stretch" src="phonmedia/sampling_lowrate.jpg"></p>
<hr />
<h3 id="sampling-rate-awful-rate">Sampling Rate (awful rate)</h3>
<p><img class="r-stretch" src="phonmedia/sampling_toolow.jpg"></p>
<hr />
<h3 id="bad-sampling-makes-for-bad-waves">Bad sampling makes for bad
waves</h3>
<p><img class="r-stretch" src="phonmedia/sampling_undersampled.jpg"></p>
<hr />
<h2 id="nyquist-theorem">Nyquist Theorem</h2>
<p>The highest frequency captured by a sample signal is one half the
sampling rate</p>
<hr />
<h3
id="sampling-rates-shpongle---nothing-is-something-worth-doing">Sampling
Rates (Shpongle - ‘Nothing is something worth doing’)</h3>
<p>44,100 Hz
<audio controls src="phonmedia/nothingsomething44100.wav"></audio></p>
<p>22,050 Hz
<audio controls src="phonmedia/nothingsomething22050.wav"></audio></p>
<p>11,025 Hz
<audio controls src="phonmedia/nothingsomething11025.wav"></audio></p>
<p>6000 Hz
<audio controls src="phonmedia/nothingsomething6000.wav"></audio></p>
<hr />
<h3
id="sampling-rates-shpongle---nothing-is-something-worth-doing-1">Sampling
Rates (Shpongle - ‘Nothing is something worth doing’)</h3>
<p>44,100 Hz
<audio controls src="phonmedia/nothingsomething44100.wav"></audio></p>
<p>6000 Hz
<audio controls src="phonmedia/nothingsomething6000.wav"></audio></p>
<p>3000 Hz
<audio controls src="phonmedia/nothingsomething3000.wav"></audio></p>
<p>1500 Hz
<audio controls src="phonmedia/nothingsomething1500.wav"></audio></p>
<p>800 Hz
<audio controls src="phonmedia/nothingsomething800.wav"></audio></p>
<hr />
<h3 id="different-media-use-different-sampling-rates">Different media
use different sampling rates</h3>
<ul>
<li><p>Radio was historically less than this</p></li>
<li><p>CDs are at 44,100 Hz</p></li>
<li><p>DVDs are at 48,000 Hz</p></li>
<li><p>High-End Audio DVDs are at 96,000 Hz</p></li>
<li><p>Some people want 192,000 Hz</p>
<ul>
<li>Likely they are dolphins</li>
</ul></li>
</ul>
<hr />
<h3
id="the-bit-depth-controls-how-much-detail-we-store-about-each-amplitude">The
‘Bit Depth’ controls how much detail we store about each amplitude</h3>
<ul>
<li>16 bits gives 65,563 levels, which is the default in modern
machines</li>
</ul>
<hr />
<h3
id="heres-a-talk-about-this-i-did-which-goes-into-more-detail">Here’s a
talk about this I did which goes into more detail</h3>
<ul>
<li><p>Covers compression, bit depth, mp3, and more</p></li>
<li><p><a href="https://www.youtube.com/watch?v=o1OADV71g1Y"
class="uri">https://www.youtube.com/watch?v=o1OADV71g1Y</a></p></li>
<li><p>Also LIGN 168!</p></li>
</ul>
<hr />
<h3 id="ad-conversion-now-yields-a-signal-that-the-computer-can-read">AD
Conversion now yields a signal that the computer can read</h3>
<ul>
<li>… but how does it interpret it?</li>
</ul>
<hr />
<h3 id="well-much-like-the-rest-of-us">Well, much like the rest of
us!</h3>
<p><img class="r-stretch" src="phonmedia/noisebbspectrogram.jpg"></p>
<hr />
<h3 id="there-are-more-problems">There are more problems</h3>
<ul>
<li><p>We’re going to use Neural Networks</p>
<ul>
<li>Or, historically, hidden markov models</li>
</ul></li>
<li><p>… but what are the algorithms looking at?</p></li>
</ul>
<hr />
<h3
id="putting-in-the-waveform-itself-was-historically-a-poor-choice">Putting
in the waveform itself was historically a poor choice</h3>
<ul>
<li><p>It’s cheap and easy</p></li>
<li><p>NNs weren’t amazing at estimating frequency-based effects</p>
<ul>
<li>Recent approaches are changing that (c.f. <a
href="https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/">Wav2Vec</a>)</li>
</ul></li>
<li><p>Important parts of the signal live only in frequency band
info</p></li>
<li><p>We want to be able to give it all the information we can, in the
most useful format!</p></li>
</ul>
<hr />
<h3 id="why-not-linguistically-useful-features">Why not linguistically
useful features?</h3>
<p><img class="r-stretch" src="phonmedia/noisebbspectrogram.jpg"></p>
<hr />
<h3 id="linguistically-useful-features-benefits">Linguistically useful
features benefits</h3>
<ul>
<li><p>They reflect speech-specific understanding</p>
<ul>
<li>They treat speech as “special”</li>
</ul></li>
<li><p>They reflect articulatory facts</p></li>
<li><p>They’re efficient</p>
<ul>
<li>Optimal informativeness per feature</li>
</ul></li>
<li><p>They’re very transparent</p>
<ul>
<li>We know what each of them means</li>
</ul></li>
</ul>
<hr />
<h3 id="linguistically-useful-features-downsides">Linguistically useful
features downsides</h3>
<ul>
<li><p>Slow to extract</p></li>
<li><p>Require specialized algorithms to extract</p></li>
<li><p>They treat speech as “special”</p></li>
</ul>
<hr />
<h3 id="for-research-linguistically-useful-features-are-great">For
research, linguistically useful features are great</h3>
<ul>
<li>… but in production, we don’t care</li>
</ul>
<hr />
<h3 id="we-dont-need-transparent-or-minimal">We don’t need transparent
or minimal</h3>
<ul>
<li><p>We’re plugging it into a black box</p></li>
<li><p>We’re happy to plug in hundreds of features, if need be</p></li>
<li><p>We’d just as soon turn that sound into a boring matrix</p></li>
</ul>
<hr />
<h3 id="lets-get-that-algorithm-a-matrix">Let’s get that algorithm a
Matrix</h3>
<ul>
<li>Algorithms love Matrices</li>
</ul>
<hr />
<h1 id="mel-frequency-cepstral-coefficients-mfccs">Mel-Frequency
Cepstral Coefficients (MFCCs)</h1>
<hr />
<h3 id="were-not-going-deep-here">We’re not going deep here</h3>
<ul>
<li><p>This is a lot of signal processing</p></li>
<li><p>We’re going to teach the idea, not the practice</p></li>
</ul>
<hr />
<h3 id="mfccs">MFCCs</h3>
<p><img class="r-stretch" src="phonmedia/mfcc.jpg"></p>
<hr />
<h3 id="mfcc-process">MFCC Process</h3>
<ul>
<li><p>1: Create a spectrogram</p></li>
<li><p>2: Extract the most useful bands for speech (in Mels)</p></li>
<li><p>3: Look at the frequencies of this banded signal (repeating the
Fourier Transform process)</p></li>
<li><p>4: Simplify this into a smaller number of coefficients using
DCT</p>
<ul>
<li>Usually 12 or 13</li>
</ul></li>
</ul>
<hr />
<h3 id="mfcc-input">MFCC Input</h3>
<p><img class="r-stretch" src="phonmedia/noisewaveform.jpg"></p>
<hr />
<h3 id="mfcc-output">MFCC Output</h3>
<p><img class="r-stretch" src="phonmedia/noise_mfcc.jpg"></p>
<hr />
<h3 id="so-the-sound-becomes-a-matrix-of-features">So, the sound becomes
a matrix of features</h3>
<ul>
<li><p>Many rows (representing time during the signal)</p></li>
<li><p>N columns (usually 13) with coefficients which tell us the
spectral shape</p></li>
<li><p>It’s black-boxy, but we don’t care.</p></li>
<li><p>We’ve created a Matrix</p></li>
</ul>
<hr />
<p><img class="r-stretch" src="humorimg/whoa_neo.jpg"></p>
<hr />
<h3 id="now-weve-got-a-matrix-representing-the-sound">Now we’ve got a
matrix representing the sound</h3>
<ul>
<li>… which captures frequency information, according to our perceptual
needs</li>
</ul>
<hr />
<h3 id="its-neural-network-time">It’s Neural Network time!</h3>
<p><img class="r-stretch" src="img/neuralnetwork.jpg"></p>
<hr />
<p>… Wait, hold on.</p>
<ul>
<li><h3 id="what-are-we-actually-recognizing">What are we actually
recognizing?</h3></li>
</ul>
<hr />
<h3 id="what-are-we-recognizing-in-speech-recognition">What are we
recognizing in speech recognition?</h3>
<ul>
<li><p>We need to give the NN labeled data</p></li>
<li><p>[Chunk of Sound MFCCed] == [Labeled Linguistic Info]</p>
<ul>
<li>(for Many many many many tokens)</li>
</ul></li>
<li><p>What level do we want to recognize at?</p></li>
</ul>
<hr />
<h3 id="possible-levels-of-recognition">Possible levels of
recognition</h3>
<ul>
<li><p>Sentences?</p></li>
<li><p>Words?</p></li>
<li><p>Letters?</p></li>
<li><p>Phones?</p></li>
<li><p>Diphones?</p></li>
</ul>
<hr />
<h3 id="sentences">Sentences</h3>
<ul>
<li>Why are sentences a bad idea?</li>
</ul>
<hr />
<h3 id="words">Words</h3>
<p><img class="r-stretch" src="phonmedia/noisebbspectrogram.jpg"></p>
<p>“Noise”</p>
<hr />
<h3 id="word-recognition-pros">Word Recognition Pros</h3>
<ul>
<li><p>Handles larger patterns of coarticulation</p></li>
<li><p>Captures word specific effects</p></li>
<li><p>Robust to short duration noise</p></li>
<li><p>Word annotation is <em>way</em> cheaper</p></li>
</ul>
<hr />
<h3 id="word-recognition-cons">Word Recognition Cons</h3>
<ul>
<li><p>What about novel words?</p></li>
<li><p>Training data becomes much more sparse</p></li>
<li><p>Can we really learn nothing about “boy” from “soy”?</p></li>
</ul>
<hr />
<h3 id="grapheme-based-recognition">Grapheme-based Recognition</h3>
<ul>
<li><p>You could use the orthography itself as the ‘pronunciation
dictionary’ and recognize letters (‘graphemes’)</p></li>
<li><p>Mapping straight from letters to speech signal</p></li>
<li><p>This is actually happening now!</p>
<ul>
<li><p><a href="https://aclanthology.org/2020.sltu-1.7.pdf">Here’s one
example</a> and <a
href="http://www.interspeech2020.org/uploadfile/pdf/Wed-2-8-8.pdf">another</a></p></li>
<li><p>Here’s another production system you can play with: <a
href="https://huggingface.co/docs/transformers/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC">HuggingFace2</a></p></li>
</ul></li>
</ul>
<hr />
<h3 id="grapheme-based-pros">Grapheme-based Pros</h3>
<ul>
<li><p>The data are much easier to get</p>
<ul>
<li>Subtitles, transcripts, etc</li>
</ul></li>
<li><p>More able to handle new words and names</p>
<ul>
<li>It can guess how ‘Haligtree’ or ‘Maliketh’ sound without dictionary
entries</li>
</ul></li>
<li><p><strong>You don’t need dictionaries to map from words to
phones!</strong></p></li>
</ul>
<hr />
<h3 id="grapheme-based-cons">Grapheme-based Cons</h3>
<ul>
<li><p>Grapheme-to-phone conversion is very language specific</p></li>
<li><p>It’s often roughly and thoroughly arbitrary</p></li>
<li><p>Some languages’ writing systems have less mutual information with
spoken language</p></li>
<li><p>It throws away data for many homograph differences (e.g. record,
villa, does)</p></li>
</ul>
<hr />
<h3 id="phones">Phones</h3>
<p><img src="phonmedia/noise_phones.jpg"></p>
<hr />
<h3 id="phone-recognition-pros">Phone Recognition Pros</h3>
<ul>
<li><p>The most basic unit, so training data is rich</p></li>
<li><p>Can (theoretically) work for any language</p></li>
<li><p>Can still capture unknown words</p>
<ul>
<li>“Fuzzy matching”</li>
</ul></li>
</ul>
<hr />
<h3 id="phone-recognition-cons">Phone Recognition Cons</h3>
<ul>
<li><p>Annotation is brutally expensive</p></li>
<li><p>Coarticulation is problematic</p></li>
<li><p>Phone-level recognition is overkill for many contexts</p></li>
</ul>
<hr />
<h3 id="diphones">Diphones</h3>
<p><img src="phonmedia/noise_diphones.jpg"></p>
<hr />
<h3 id="diphone-recognition-pros">Diphone Recognition Pros</h3>
<ul>
<li><p>Coarticulation becomes a feature, not a bug</p></li>
<li><p>Still very basic, so all training data provides data</p></li>
<li><p>Can still (theoretically) work for any language</p>
<ul>
<li>… but patterns of coarticulation differ</li>
</ul></li>
<li><p>Can still capture unknown words via Fuzzy matching</p></li>
</ul>
<hr />
<h3 id="diphone-recognition-cons">Diphone Recognition Cons</h3>
<ul>
<li><p>Still stupidly expensive to annotate</p></li>
<li><p>Still overkill in many contexts</p></li>
</ul>
<hr />
<h3 id="in-practice-many-systems-use-diphones">In practice, many systems
use diphones</h3>
<ul>
<li><p><a href="https://cmusphinx.github.io/">CMU’s Sphynx
does</a></p></li>
<li><p>As do many others</p></li>
<li><p>Triphones are often a possibility</p></li>
</ul>
<hr />
<h3 id="but-modern-systems-are-often-going-waveform-to-grapheme">… but
modern systems are often going waveform-to-grapheme</h3>
<ul>
<li>This is absolutely wild</li>
</ul>
<hr />
<h3 id="so-we-can-now-train-a-system">So, we can now train a system</h3>
<ul>
<li><p>Capture sounds and annotate them as diphones or words</p></li>
<li><p>MFCC them, or read in the waveform alongside word labels, and
feed them into a neural network as training data</p></li>
<li><p>Then later, feed new data in and get back a list of phones (or
words), which you can use to predict which words were intended!</p></li>
</ul>
<hr />
<h3 id="thats-a-tricky-step-right-there">That’s a tricky step right
there</h3>
<ul>
<li>Why?</li>
</ul>
<hr />
<h3
id="your-asr-system-is-only-as-good-as-your-dictionary-andor-training-data">Your
ASR system is only as good as your dictionary and/or training data</h3>
<ul>
<li><p>“For shizzle, Bashira”</p></li>
<li><p>“Mel Frequency Cepstral Coefficient”</p></li>
<li><p>“Differentiating Theta and Eth”</p></li>
<li><p>“Take Caminito Santa Fe, then Mira Mesa into La Jolla”</p></li>
</ul>
<hr />
<h3 id="users-have-very-specific-matches-they-expect">Users have very
specific matches they expect</h3>
<hr />
<h3 id="hey-siri-play-songs-by-the-bedsit-infamy">“Hey Siri play songs
by the Bedsit Infamy”</h3>
<ul>
<li><img class="r-stretch" src="img/bedsitting.png"></li>
</ul>
<hr />
<h3 id="hey-siri-play-songs-by-the-bedsit-infamy-1">“Hey Siri play songs
by the Bedsit Infamy”</h3>
<p><img class="r-stretch" src="img/bedsit.png"></p>
<hr />
<h3 id="how-do-we-test-the-system">How do we test the system?</h3>
<hr />
<h3 id="like-this">Like this</h3>
<p><a href="https://dictation.io/speech"
class="uri">https://dictation.io/speech</a></p>
<hr />
<h3 id="wrapping-up">Wrapping Up</h3>
<ul>
<li><p>Computers can learn to do the wave</p></li>
<li><p>MFCCs turn beautiful sounds into opaque, useful matrices</p></li>
<li><p>Speech Recognition often uses diphones</p></li>
<li><p>You’re only as good as your dictionary</p></li>
</ul>
<hr />
<h2 id="for-next-time">For next time</h2>
<ul>
<li><p><strong>NO CLASS FRIDAY</strong></p></li>
<li><p>Why is speech recognition so damned hard?</p></li>
</ul>
<hr />
<p><huge>Thank you!</huge></p>
</body>
</html>
