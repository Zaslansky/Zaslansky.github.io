<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title></title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <style type="text/css">
  /*
   * I add this to html files generated with pandoc.
   * Originally from https://gist.github.com/killercup/5917178
   */

  html {
      overflow-y: scroll;
      -webkit-text-size-adjust: 100%;
      -ms-text-size-adjust: 100%;
  }

  body {
      color: #444;
      font-family: "Source Sans 3", Helvetica-Neue, Helvetica, Sans;
      line-height: 1.5;
      padding: 0.5em;
      margin: auto;
      max-width: 55em;
      background: #fefefe;
  }

  a {
      color: #2171b5;
      text-decoration: underline;
  }

  tr:nth-child(even) {background: #F8F8F8}
  tr:nth-child(odd) {background: #FFF}

  a:visited {
      color: #2171b5;
      text-decoration: none;
  }

  a:focus {
      outline: thin dotted;
  }

  *::-moz-selection {
      background: rgba(255, 255, 0, 0.3);
      color: #000;
  }

  *::selection {
      background: rgba(255, 255, 0, 0.3);
      color: #000;
  }

  a::-moz-selection {
      background: rgba(255, 255, 0, 0.3);
      color: #0645ad;
  }

  a::selection {
      background: rgba(255, 255, 0, 0.3);
      color: #0645ad;
  }

  p {
      margin: 0.75em 0;
  }

  img {
      max-width: 60%;
      max-height:400px;
  }

  video {
      max-width: 60%;
  }


  h1, h2, h3, h4, h5, h6 {
      color: #111;
      line-height: 80%;
      margin-top: 1em;
      margin-bottom: 0.5em;
      font-weight: normal;
  }

  h1, h2, h3, h4, h5, h6 {
      font-weight: bold;
  }

  h1 {
      font-size: 2em;
      line-height: 1.25;
      color:  #084594;

  }

  h1.title {
      margin-top:0.2em;
      font-size: 2em;
      line-height: 1.25;
  }

  h2 {
      font-size: 1.5em;
      line-height: 1.6em;
          color:  #084594;
      padding-bottom: 3px;

  }

  h3 {
      font-size: 1.2em;
      line-height: 1.6em;
  }


  h4 {
      font-size: 1.2em;
      line-height: 1.4em;
  }

  h5 {
      font-size: 1em;
  }

  h6 {
      font-size: 0.9em;
  }

  blockquote {
      color: #666666;
      margin: 0;
      padding-left: 3em;
      border-left: 0.5em #EEE solid;
  }

  hr {
      display: block;
      height: 2px;
      border: 0;
      border-top: 1px solid #aaa;
      border-bottom: 1px solid #eee;
      margin: 0.5em 0;
      padding: 0;
  }

  pre, code, kbd, samp {
      color: #000;
      font-family: monospace, monospace;
      _font-family: 'courier new', monospace;
      font-size: 0.98em;
  }

  pre {
      white-space: pre;
      white-space: pre-wrap;
      word-wrap: break-word;
  }

  .answer {
      color:#CC0033;
      font-style:italic;
  }

  b, strong {
      font-weight: bold;
  }

  dfn {
      font-style: italic;
  }

  ins {
      background: #ff9;
      color: #000;
      text-decoration: none;
  }

  mark {
      background: #ff0;
      color: #000;
      font-style: italic;
      font-weight: bold;
  }

  sub, sup {
      font-size: 75%;
      line-height: 0;
      position: relative;
      vertical-align: baseline;
  }

  sup {
      top: -0.5em;
  }

  sub {
      bottom: -0.25em;
  }

  ul, ol {
      margin: 0.5em 0;
      padding: 0em 0em 0em 1em;
  }

  ul img {
      list-style-type: none;
  }

  li p:last-child {
      margin-bottom: 0;
  }

  hr {
      border-top:none;
      height:0px;
      clear:both;
  }

  ul ul, ol ol {
      margin: .3em 0;
  }

  dl {
      margin-bottom: 1em;
  }

  dt {
      font-weight: bold;
      margin-bottom: .8em;
  }

  dd {
      margin: 0 0 .8em 2em;
  }

  dd:last-child {
      margin-bottom: 0;
  }

  img {
      border: 0;
      -ms-interpolation-mode: bicubic;
      vertical-align: middle;
  }

  figure {
      display: block;
      text-align: center;
      margin: 1em 0;
  }

  figure img {
      border: none;
      margin: 0 auto;
  }

  figcaption {
      font-size: 0.8em;
      font-style: italic;
      margin: 0 0 .8em;
  }

  table {
      margin-bottom: 2em;
      border-bottom: 1px solid #ddd;
      border-right: 1px solid #ddd;
      border-spacing: 0;
      border-collapse: collapse;
  }

  table th {
      padding: .2em 1em;
      background-color: #eee;
      border-top: 1px solid #ddd;
      border-left: 1px solid #ddd;
  }

  table td {
      padding: .2em 1em;
      border-top: 1px solid #ddd;
      border-left: 1px solid #ddd;
      vertical-align: top;
  }

  .author {
      font-size: 1.2em;
      text-align: center;
  }

  @media only screen and (min-width: 480px) {
      body {
  	font-size: 14px;
      }
  }
  @media only screen and (min-width: 768px) {
      body {
  	font-size: 16px;
      }
  }
  @media print {
      * {
  	background: transparent !important;
  	color: black !important;
  	filter: none !important;
  	-ms-filter: none !important;
      }

      body {
  	font-size: 12pt;
  	max-width: 100%;
      }

      a, a:visited {
  	text-decoration: underline;
      }

      hr {
  	height: 1px;
  	border: 0;
  	border-bottom: 1px solid black;
      }

      a[href]:after {
  	content: " (" attr(href) ")";
      }

      abbr[title]:after {
  	content: " (" attr(title) ")";
      }

      .ir a:after, a[href^="javascript:"]:after, a[href^="#"]:after {
  	content: "";
      }

      pre, blockquote {
  	border: 1px solid #999;
  	padding-right: 1em;
  	page-break-inside: avoid;
      }

      tr, img {
  	page-break-inside: avoid;
      }

      img {
  	max-width: 40% !important;
      max-height: 300px !important;
      }

      @page :left {
  	margin: 15mm 20mm 15mm 10mm;
      }

      @page :right {
  	margin: 15mm 10mm 15mm 20mm;
      }

      p, h2, h3 {
  	orphans: 3;
  	widows: 3;
      }

      h2, h3 {
  	page-break-after: avoid;
      }
  }


  ldata {
  	font-size: 0.7em;
  	margin-bottom: 0em;
  	color:#808080;
  	font-style:italic;
  }

  danger {
  	color:#FF0000;
  	font-weight:bold;
  }

  correct {
  	color:#39C900;
  	font-weight:bold;
  }

  clg{
      color:#39C900;
  	font-weight:bold;
  }

  clr{
  	color:#FF0000;
  	font-weight:bold;
  }

  clb{
  	color:#0000CC;
  	font-weight:bold;
  }

  clp{
  	color:#6600FF;
  	font-weight:bold;
  }

  clk{
  	color:#708cef;
  	font-weight:bold;
  }

  clo{
  	color:#CC6600;
  	font-weight:bold;
  }

  sc{
          font-variant: small-caps;
  }

  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="neural-speech-recognition">Neural Speech Recognition</h1>
<h3 id="will-styler---lign-168">Will Styler - LIGN 168</h3>
<hr />
<h3 id="so-now-we-know-what-neural-networks-are">So, now we know what
Neural Networks are</h3>
<ul>
<li><p>We understand their pros, their cons, and their various
types</p></li>
<li><p><strong>How can they be applied for ASR?</strong></p></li>
</ul>
<hr />
<h3 id="today-is-just-a-sampler-platter">Today is just a sampler
platter</h3>
<ul>
<li><p>There are hundreds of ways to do this task</p></li>
<li><p>We’re going to talk about a few interesting approaches</p>
<ul>
<li>As always, you can dive deeper if you’d like</li>
</ul></li>
</ul>
<hr />
<h3 id="todays-plan">Today’s Plan</h3>
<ul>
<li><p>Spectral and CNN-based Approaches</p></li>
<li><p>The Alignment Problem</p></li>
<li><p>Sound-to-Vector Models and Wav2Vec 2</p></li>
<li><p>Whisper</p></li>
<li><p>Neural ASR is boring</p></li>
</ul>
<hr />
<h2 id="spectral-and-cnn-based-approaches">Spectral and CNN-based
Approaches</h2>
<hr />
<h3 id="so-we-know-neural-networks-win-at-asr">So, we know Neural
Networks win at ASR</h3>
<ul>
<li>… but how do we give acoustical data to these models?</li>
</ul>
<hr />
<h3 id="spectral-representations-as-input">Spectral Representations as
Input</h3>
<ul>
<li><p>“Let’s give the model a spectrogram directly”</p></li>
<li><p>Often something like a logged mel spectrogram</p></li>
<li><p>Sometimes something like a mel cepstrogram</p></li>
<li><p>Sometimes a png file of a spectrogram itself</p>
<ul>
<li>Please don’t</li>
</ul></li>
</ul>
<hr />
<h3 id="once-we-have-a-grid-containing-useful-acoustic-data">Once we
have a grid containing useful acoustic data…</h3>
<ul>
<li><p>Treat it like any other image!</p></li>
<li><p>Convolutional Neural Networks excel at finding patterns in
images</p></li>
<li><p>Spectrogram reading is just finding patterns in images</p></li>
<li><p>Neural Network training will ensure you find the right
patterns</p></li>
</ul>
<hr />
<h3 id="you-can-go-from-spectrogram-to-phones">You can go from
spectrogram to phones!</h3>
<p><img class="r-stretch" src="img/nn_conv_fullpath.jpg"></p>
<hr />
<h3 id="now-you-just-need-to-match-those-phones-to-words">Now you just
need to match those phones to words</h3>
<ul>
<li>… which raises….</li>
</ul>
<hr />
<h2 id="the-alignment-problem">The Alignment Problem</h2>
<hr />
<h3 id="note-that-alignment-has-many-meanings-in-ml">Note that
‘alignment’ has many meanings in ML</h3>
<ul>
<li><p>“Is this system going to do what I want it to do, for the reasons
I want it to do it?”</p></li>
<li><p>“Are these two things synchronized, ordered, and time-aligned
properly?”</p>
<ul>
<li>This is the problem we have today!</li>
</ul></li>
</ul>
<hr />
<h3 id="some-tasks-have-one-to-one-inputoutput-alignment">Some tasks
have one-to-one input/output alignment</h3>
<ul>
<li><p>“Is this picture a rabbit or a cat?”</p></li>
<li><p>“Would this Instagram user buy diet pills, based on their follow
list?”</p></li>
<li><p>“Is this email likely an attempt to purchase chemical
weapons?”</p></li>
</ul>
<hr />
<h3 id="other-tasks-have-ambiguous-alignment">Other tasks have ambiguous
alignment</h3>
<ul>
<li><p>“Detect motion in this noisy video feed from the
warehouse”</p></li>
<li><p>“Identify all characters in this cursive sentence”</p></li>
<li><p>“Take this music and generate a score for it”</p></li>
<li><p><strong>ASR!</strong></p></li>
</ul>
<hr />
<h3 id="alignment-is-hard-in-asr">Alignment is hard in ASR</h3>
<ul>
<li>I have an input waveform which describes a sequence of phones
<ul>
<li>e.g. /lɪŋgwɪstɪks/</li>
</ul></li>
<li>I have a set of output characters I’d like from that
<ul>
<li>e.g. “linguistics”</li>
</ul></li>
<li>You’ll likely need to process the sound frame-by-frame</li>
</ul>
<hr />
<h3 id="alignment-for-a-given-frame-is-hard">Alignment for a given frame
is hard</h3>
<ul>
<li><p>Not all frames contain a character</p></li>
<li><p>Some characters take more than one frame to complete</p>
<ul>
<li>A long /i/ vowel might be several frames long</li>
</ul></li>
<li><p>Some characters are repeated</p>
<ul>
<li>‘Mississippi’ doesn’t involve multiple /s/ or /p/</li>
</ul></li>
<li><p><strong>Any ASR approach needs to address this!</strong></p></li>
</ul>
<hr />
<h3
id="connectionist-temporal-classification-ctc-is-one-way">Connectionist
Temporal Classification (CTC) is one way</h3>
<ul>
<li><p>Let’s assume something allows us to assign every frame to a
likely category label (e.g. phone)</p></li>
<li><p>Create a list of possible output labels (e.g. ‘cat’, ‘penguin’,
‘access’)</p>
<ul>
<li>If output repeats characters place a blank (‘-’) into the label
between the repeated items (‘ac-ces-s’)</li>
</ul></li>
<li><p>“If a frame label repeats across multiple frames, it’s probably
the same chunk”</p></li>
<li><p>“If an output label has a blank between, it’s probably just one
chunk”</p></li>
</ul>
<hr />
<h3 id="ctc-collapsing">CTC Collapsing</h3>
<p><img class="r-stretch" src="phonmedia/asr_ctc.png"></p>
<hr />
<h3 id="ctc-just-finds-the-most-likely-path-through-the-options">CTC
just finds the most likely path through the options</h3>
<ul>
<li><p>Sort of like decoding an HMM using the Viterbi algorithm</p></li>
<li><p>“What’s the output label(s) for this sequence that best fit the
data?”</p></li>
<li><p>Now, you can match a series of frame labels, to the most likely
output labels</p></li>
<li><p><strong>The probability of the correct choice being made is also
a good loss function!</strong></p></li>
</ul>
<hr />
<h3 id="ctc-decoding">CTC Decoding</h3>
<p><img class="r-stretch" src="phonmedia/asr_ctctotal.jpg"></p>
<hr />
<h3 id="we-can-unite-the-output-with-a-language-model">We can unite the
output with a language model</h3>
<p><img class="r-stretch" src="diagrams/asr_architecture.jpg"></p>
<hr />
<h3 id="and-we-figure-out-the-most-likely-candidates">… and we figure
out the most likely candidates</h3>
<ul>
<li>Either choose the most likely candidate over and over given both
acoustic and language model data
<ul>
<li>“Greedy” decoding</li>
</ul></li>
<li>Or you do <a href="https://en.wikipedia.org/wiki/Beam_search">Beam
Search</a> to identify multiple candidates
<ul>
<li>You limit the number of candidates to consider at once</li>
<li>Then, choose the sequence with the highest overall probability</li>
</ul></li>
</ul>
<hr />
<h3 id="beam-search">Beam Search</h3>
<p><img class="r-stretch" src="phonmedia/asr_beam_search.gif"></p>
<p>(Source: BogdanShevchenko - Own work, CC BY-SA 4.0,
https://commons.wikimedia.org/w/index.php?curid=128835919)</p>
<hr />
<h3 id="now-we-have-a-working-asr-model">Now we have a working ASR
model!</h3>
<ul>
<li><p>We get predictions from the acoustics by feeding</p></li>
<li><p>We match those to word representations using CTC</p></li>
<li><p>We can unite this with a language model, evaluating probabilities
using beam search</p></li>
<li><p>This can turn a sequence of phones into a transcript!</p></li>
</ul>
<hr />
<h3 id="thats-amazing">That’s amazing!</h3>
<ul>
<li>… but can’t we make this simpler?</li>
</ul>
<hr />
<h2 id="sound-to-vector-models-and-wav2vec2">Sound-to-Vector Models and
Wav2Vec2</h2>
<hr />
<h3
id="it-would-be-nice-not-to-need-to-generate-spectral-represenations">It
would be nice not to need to generate spectral represenations</h3>
<ul>
<li><p>What if we could turn the waveform into a vector directly, and
just go end-to-end?</p></li>
<li><p>This leads to approaches like…</p></li>
</ul>
<hr />
<h3 id="wav2vec">Wav2Vec</h3>
<ul>
<li><p>Developed by Facebook’s AI team</p></li>
<li><p>Takes in a waveform directly, and outputs higher quality features
for use in ASR</p></li>
<li><p><strong>Feature Encoding:</strong> Uses CNNs to create ‘latent
representations’ every 10ms or so</p>
<ul>
<li>These aren’t exactly segments, they’re just ‘things the network
notices’</li>
</ul></li>
<li><p><strong>Context Network:</strong> Combines the features to
capture adjacency effects</p></li>
</ul>
<hr />
<h3 id="wav2vec-is-trained-in-a-self-supervised-way">Wav2Vec is trained
in a <em>self supervised</em> way</h3>
<ul>
<li><p>We’re going to take away (‘mask’) some of the representations
coming out of the model</p></li>
<li><p>Now, we make the algorithm predict what’s missing</p></li>
<li><p>The loss function involves correctly choosing the missing
representations, among a random set sampled from elsewhere</p></li>
<li><p><strong>This allows it to learn without as much labeled
data</strong></p></li>
</ul>
<hr />
<h3 id="wav2vec-2.0">Wav2Vec 2.0</h3>
<ul>
<li>Same idea, but the encoded features get quantized to a set of
specific ‘tokens’
<ul>
<li>Tokens are kind of like phones, but not 1-to-1</li>
<li>We get a sequence out of the waveform</li>
</ul></li>
<li>The context network can now be a transformer
<ul>
<li>This has all the benefits, plus easy fine-tuning for new languages
or tasks</li>
</ul></li>
<li><strong>You can map these tokens to whatever representation you’d
like</strong>
<ul>
<li>Phones/Diphones/Triphones</li>
<li>IPA characters</li>
<li>Even direct to orthography!</li>
</ul></li>
</ul>
<hr />
<h3 id="wav2vec2">Wav2Vec2</h3>
<p><img class="r-stretch" src="phonmedia/asr_wav2vec2.png"></p>
<hr />
<h3 id="wav2vec2-can-go-from-sound-to-orthography">Wav2Vec2 can go from
sound to orthography!</h3>
<ul>
<li><p>This is absolutely wild, given how bad our writing system
is!</p></li>
<li><p>It also doesn’t need CTC, as that’s a part of the transformer’s
core competencies</p></li>
<li><p>These quantized units raise the possibility of a
language-independent ASR system!</p>
<ul>
<li>The ‘Universal Transcriber’</li>
</ul></li>
</ul>
<hr />
<h3 id="wav2vec2-replaces-the-entire-acoustical-pipeline">Wav2Vec2
replaces the entire acoustical pipeline</h3>
<p><img class="r-stretch" src="diagrams/asr_architecture.jpg"></p>
<hr />
<h3 id="to-decode-wav2vec2-data-into-words">To ‘Decode’ Wav2Vec2 data
into words</h3>
<p>You combine the token-representation with a separate language model
to find hypotheses</p>
<ul>
<li><p>Again, you can use probabilities and/or beam search to find the
best approach</p></li>
<li><p>Some people are trying to make this <em>fully</em> end to end,
building the language model into the Wav2Vec Model</p>
<ul>
<li>We’ll see an example of this shortly!</li>
</ul></li>
</ul>
<hr />
<h3 id="this-is-a-working-asr-system">This is a working ASR system!</h3>
<ul>
<li><p>Wav2Vec2 goes from acoustics to quantized tokens</p></li>
<li><p>Those tokens can be mapped directly to linguistic units</p></li>
<li><p>Linguistic units plus a language model give probabilities for
outputs</p></li>
<li><p>Output probabilities can be beam-searched, to arrive at the best
transcription</p></li>
<li><p><strong>We’ve gone from Waves to Words in just two
steps!</strong></p>
<ul>
<li>… and they’re both very boring steps</li>
</ul></li>
</ul>
<hr />
<h3 id="you-can-also-build-on-this-system">You can also build on this
system!</h3>
<ul>
<li>You can take an existing model, and fine tune with a bit of data for
another language
<ul>
<li>So, we use a model trained on all speech, and then fine tune on
Tira</li>
<li>This is great for low-resource languages</li>
</ul></li>
<li><a href="https://github.com/lingjzhu/charsiu">Char Siu</a> turns
Wav2Vec2 into a forced aligner
<ul>
<li>“Find me the exact temporal boundaries of these segments”</li>
</ul></li>
<li><strong>Open Models make the world better!</strong></li>
</ul>
<hr />
<h3 id="this-is-a-very-common-setup-now">This is a very common setup
now</h3>
<ul>
<li><p>It’s not often revealed how commercial ASR systems work</p>
<ul>
<li>Secret Sauce abounds!</li>
</ul></li>
<li><p>We should assume many systems are using a similar architecture
under the hood!</p></li>
<li><p>… but at least one system is different!</p></li>
</ul>
<hr />
<h2 id="whisper">Whisper</h2>
<hr />
<h3 id="whisper-is-an-asr-model-from-openai">Whisper is an ASR Model
from OpenAI</h3>
<ul>
<li><p>It does transcription</p>
<ul>
<li>It also does time-aligned transcription (e.g. for video
captioning)</li>
</ul></li>
<li><p>It does some multilingual ASR and translation too!</p>
<ul>
<li>This is a neat trick!</li>
</ul></li>
<li><p>It also can be used for language identification!</p></li>
<li><p>It is shockingly good!</p></li>
</ul>
<hr />
<h3 id="whisper-performs-nearly-as-well-as-human-transcribers">Whisper
performs nearly as well as human transcribers</h3>
<p><img class="r-stretch" src="phonmedia/asr_whispervshumans.png"></p>
<hr />
<h3 id="you-can-actually-use-it">You can actually use it!!</h3>
<ul>
<li>It is free and open
<ul>
<li>Unlike anything OpenAI does these days</li>
</ul></li>
<li>You can use and download the models for free
<ul>
<li><a href="https://github.com/openai/whisper"
class="uri">https://github.com/openai/whisper</a></li>
</ul></li>
<li>It’s been tuned to run (slowly) even without a GPU!
<ul>
<li>This allows fully local transcription!!</li>
</ul></li>
</ul>
<hr />
<h3 id="whisper-uses-a-hybrid-of-cnns-and-transformers">Whisper uses a
hybrid of CNNs and Transformers</h3>
<p><img class="r-stretch" src="phonmedia/asr_whisper.png"></p>
<hr />
<h3
id="whisper-embeds-the-language-model-into-the-acoustic-model">Whisper
embeds the language model into the acoustic model!</h3>
<ul>
<li>There is no separate hypothesis step and no separate language
model!</li>
</ul>
<hr />
<h3 id="classical-asr-architecture">Classical ASR Architecture</h3>
<p><img class="r-stretch" src="diagrams/asr_architecture.jpg"></p>
<hr />
<h3 id="whispers-end-to-end-architecture">Whisper’s End-to-End
Architecture</h3>
<p><img class="r-stretch" src="diagrams/asr_architecture_endtoend.jpg"></p>
<hr />
<h3 id="whisper-offers-a-number-of-models">Whisper offers a number of
models</h3>
<p><img class="r-stretch" src="phonmedia/asr_whisper_layers.png"></p>
<hr />
<h3 id="whisper-works-mostly-great">Whisper works (mostly) great!</h3>
<ul>
<li><p>It’s probably the right choice for transcribing files at the
moment!</p></li>
<li><p>… it’s also the last neural ASR architecture we’re going to look
at this quarter!</p></li>
</ul>
<hr />
<h2 id="neural-asr-is-boring-lately">Neural ASR is boring lately</h2>
<hr />
<h3 id="this-is-a-rant">This is a rant</h3>
<ul>
<li><p>Many people have lovely careers, and the tools are exhilaratingly
powerful</p></li>
<li><p>… but…</p></li>
</ul>
<hr />
<h3 id="neural-asr-has-largely-abandoned-linguistics">Neural ASR has
largely abandoned linguistics</h3>
<ul>
<li><p>“Feed in waves and text to a transformer, cook for two weeks,
then have a model”</p></li>
<li><p>There is no linguistic nuance</p></li>
<li><p>There’s not even transcription anymore!</p></li>
</ul>
<hr />
<h3 id="more-parameters-more-better">More parameters == More Better</h3>
<ul>
<li><p>Thus, more memory and energy cost generally wins</p></li>
<li><p>This privileges large companies with large resources</p></li>
<li><p>This focuses development on wealthy languages and groups</p></li>
</ul>
<hr />
<h3 id="asr-is-approaching-solved-for-high-resource-people">ASR is
approaching ‘solved’ for high-resource people</h3>
<ul>
<li>For the mainest-stream speakers of American English, ASR is amazing
<ul>
<li>ASR performance for me is at a place I didn’t expect to see in my
lifetime</li>
<li>There is room for improvement, to be sure, but it’s not a lot of
room</li>
</ul></li>
<li>Most of the improvement is in brittleness with other dialects and
variation</li>
</ul>
<hr />
<h3
id="the-most-interesting-work-in-asr-right-now-is-for-low-resource-languages">The
most interesting work in ASR right now is for low resource
languages</h3>
<ul>
<li><p>How do we get <em>great</em> ASR working for a language with low
amounts of data/money/hardware?</p>
<ul>
<li>Imagine your language not having voice-to-type?</li>
</ul></li>
<li><p>How can we use ASR to more quickly generate, study, and clean
language data?</p></li>
<li><p>How can we use ASR to enable field work and linguistic inquiry
more effectively</p></li>
<li><p><strong>How can ASR help people who the tech industry doesn’t
care enough to help?</strong></p></li>
</ul>
<hr />
<h3 id="for-information-on-this-talk-to-mark-simmons">For information on
this, talk to Mark Simmons!</h3>
<ul>
<li><p>He’s in the trenches with these ideas right now!</p></li>
<li><p>… and we’ll hear from him next time!</p></li>
</ul>
<hr />
<h3 id="wrapping-up">Wrapping up</h3>
<ul>
<li><p>Feeding spectral information into CNNs is a great way to extract
features</p></li>
<li><p>You need to solve the alignment problem, either with CTC or
Transformers</p></li>
<li><p>Wav2Vec2 offers a path straight from audio to intermediate
representations</p>
<ul>
<li>… which can be fed into a language model</li>
</ul></li>
<li><p>Whisper is free, great, and completely end-to-end!</p></li>
</ul>
<hr />
<p><huge>Thank you!</huge></p>
</body>
</html>
