<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title></title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <style type="text/css">
  /*
   * I add this to html files generated with pandoc.
   * Originally from https://gist.github.com/killercup/5917178
   */

  html {
      overflow-y: scroll;
      -webkit-text-size-adjust: 100%;
      -ms-text-size-adjust: 100%;
  }

  body {
      color: #444;
      font-family: "Source Sans 3", Helvetica-Neue, Helvetica, Sans;
      line-height: 1.5;
      padding: 0.5em;
      margin: auto;
      max-width: 55em;
      background: #fefefe;
  }

  a {
      color: #2171b5;
      text-decoration: underline;
  }

  tr:nth-child(even) {background: #F8F8F8}
  tr:nth-child(odd) {background: #FFF}

  a:visited {
      color: #2171b5;
      text-decoration: none;
  }

  a:focus {
      outline: thin dotted;
  }

  *::-moz-selection {
      background: rgba(255, 255, 0, 0.3);
      color: #000;
  }

  *::selection {
      background: rgba(255, 255, 0, 0.3);
      color: #000;
  }

  a::-moz-selection {
      background: rgba(255, 255, 0, 0.3);
      color: #0645ad;
  }

  a::selection {
      background: rgba(255, 255, 0, 0.3);
      color: #0645ad;
  }

  p {
      margin: 0.75em 0;
  }

  img {
      max-width: 60%;
      max-height:400px;
  }

  video {
      max-width: 60%;
  }


  h1, h2, h3, h4, h5, h6 {
      color: #111;
      line-height: 80%;
      margin-top: 1em;
      margin-bottom: 0.5em;
      font-weight: normal;
  }

  h1, h2, h3, h4, h5, h6 {
      font-weight: bold;
  }

  h1 {
      font-size: 2em;
      line-height: 1.25;
      color:  #084594;

  }

  h1.title {
      margin-top:0.2em;
      font-size: 2em;
      line-height: 1.25;
  }

  h2 {
      font-size: 1.5em;
      line-height: 1.6em;
          color:  #084594;
      padding-bottom: 3px;

  }

  h3 {
      font-size: 1.2em;
      line-height: 1.6em;
  }


  h4 {
      font-size: 1.2em;
      line-height: 1.4em;
  }

  h5 {
      font-size: 1em;
  }

  h6 {
      font-size: 0.9em;
  }

  blockquote {
      color: #666666;
      margin: 0;
      padding-left: 3em;
      border-left: 0.5em #EEE solid;
  }

  hr {
      display: block;
      height: 2px;
      border: 0;
      border-top: 1px solid #aaa;
      border-bottom: 1px solid #eee;
      margin: 0.5em 0;
      padding: 0;
  }

  pre, code, kbd, samp {
      color: #000;
      font-family: monospace, monospace;
      _font-family: 'courier new', monospace;
      font-size: 0.98em;
  }

  pre {
      white-space: pre;
      white-space: pre-wrap;
      word-wrap: break-word;
  }

  .answer {
      color:#CC0033;
      font-style:italic;
  }

  b, strong {
      font-weight: bold;
  }

  dfn {
      font-style: italic;
  }

  ins {
      background: #ff9;
      color: #000;
      text-decoration: none;
  }

  mark {
      background: #ff0;
      color: #000;
      font-style: italic;
      font-weight: bold;
  }

  sub, sup {
      font-size: 75%;
      line-height: 0;
      position: relative;
      vertical-align: baseline;
  }

  sup {
      top: -0.5em;
  }

  sub {
      bottom: -0.25em;
  }

  ul, ol {
      margin: 0.5em 0;
      padding: 0em 0em 0em 1em;
  }

  ul img {
      list-style-type: none;
  }

  li p:last-child {
      margin-bottom: 0;
  }

  hr {
      border-top:none;
      height:0px;
      clear:both;
  }

  ul ul, ol ol {
      margin: .3em 0;
  }

  dl {
      margin-bottom: 1em;
  }

  dt {
      font-weight: bold;
      margin-bottom: .8em;
  }

  dd {
      margin: 0 0 .8em 2em;
  }

  dd:last-child {
      margin-bottom: 0;
  }

  img {
      border: 0;
      -ms-interpolation-mode: bicubic;
      vertical-align: middle;
  }

  figure {
      display: block;
      text-align: center;
      margin: 1em 0;
  }

  figure img {
      border: none;
      margin: 0 auto;
  }

  figcaption {
      font-size: 0.8em;
      font-style: italic;
      margin: 0 0 .8em;
  }

  table {
      margin-bottom: 2em;
      border-bottom: 1px solid #ddd;
      border-right: 1px solid #ddd;
      border-spacing: 0;
      border-collapse: collapse;
  }

  table th {
      padding: .2em 1em;
      background-color: #eee;
      border-top: 1px solid #ddd;
      border-left: 1px solid #ddd;
  }

  table td {
      padding: .2em 1em;
      border-top: 1px solid #ddd;
      border-left: 1px solid #ddd;
      vertical-align: top;
  }

  .author {
      font-size: 1.2em;
      text-align: center;
  }

  @media only screen and (min-width: 480px) {
      body {
  	font-size: 14px;
      }
  }
  @media only screen and (min-width: 768px) {
      body {
  	font-size: 16px;
      }
  }
  @media print {
      * {
  	background: transparent !important;
  	color: black !important;
  	filter: none !important;
  	-ms-filter: none !important;
      }

      body {
  	font-size: 12pt;
  	max-width: 100%;
      }

      a, a:visited {
  	text-decoration: underline;
      }

      hr {
  	height: 1px;
  	border: 0;
  	border-bottom: 1px solid black;
      }

      a[href]:after {
  	content: " (" attr(href) ")";
      }

      abbr[title]:after {
  	content: " (" attr(title) ")";
      }

      .ir a:after, a[href^="javascript:"]:after, a[href^="#"]:after {
  	content: "";
      }

      pre, blockquote {
  	border: 1px solid #999;
  	padding-right: 1em;
  	page-break-inside: avoid;
      }

      tr, img {
  	page-break-inside: avoid;
      }

      img {
  	max-width: 40% !important;
      max-height: 300px !important;
      }

      @page :left {
  	margin: 15mm 20mm 15mm 10mm;
      }

      @page :right {
  	margin: 15mm 10mm 15mm 20mm;
      }

      p, h2, h3 {
  	orphans: 3;
  	widows: 3;
      }

      h2, h3 {
  	page-break-after: avoid;
      }
  }


  ldata {
  	font-size: 0.7em;
  	margin-bottom: 0em;
  	color:#808080;
  	font-style:italic;
  }

  danger {
  	color:#FF0000;
  	font-weight:bold;
  }

  correct {
  	color:#39C900;
  	font-weight:bold;
  }

  clg{
      color:#39C900;
  	font-weight:bold;
  }

  clr{
  	color:#FF0000;
  	font-weight:bold;
  }

  clb{
  	color:#0000CC;
  	font-weight:bold;
  }

  clp{
  	color:#6600FF;
  	font-weight:bold;
  }

  clk{
  	color:#708cef;
  	font-weight:bold;
  }

  clo{
  	color:#CC6600;
  	font-weight:bold;
  }

  sc{
          font-variant: small-caps;
  }

  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="cepstra-pitch-tracking-and-voice-activity-detection">Cepstra,
Pitch Tracking and Voice Activity Detection</h1>
<h3 id="will-styler---lign-168">Will Styler - LIGN 168</h3>
<hr />
<h3 id="last-time-we-talked-about-estimating-the-filter">Last time, we
talked about estimating the filter</h3>
<ul>
<li><p>LPC gives us a sense of how the vocal tract is <em>filtering</em>
the vocal folds</p></li>
<li><p>We estimate the filter’s shape, independent of the voicing (the
‘excitation signal’)</p></li>
<li><p>We discussed how LPC can be used to deconstruct and then
reconstitute the voice</p></li>
<li><p><strong>… but what about estimating the source?</strong></p></li>
</ul>
<hr />
<h3 id="the-excitation-signal">The Excitation Signal</h3>
<ul>
<li><p>To do LPC synthesis and resynthesis, we need two pieces of
information about the source</p></li>
<li><p>What is the pitch/fundamental frequency/f0 of the voice?</p></li>
<li><p>Where (during the duration of the file) is there voicing to
reconstruct?</p>
<ul>
<li>“From 253-284 ms, there’s no voicing”</li>
</ul></li>
<li><p>Are there any irregular pulses?</p>
<ul>
<li>“There’s creak here, so, drop single pulses here, here, and
here”</li>
</ul></li>
<li><p>We can model this, effectively, as a series of numbers indicating
f0 over time, with ‘zero’ as an option</p>
<ul>
<li>With some noise modeling for not-so-voicing-like components</li>
</ul></li>
</ul>
<hr />
<p><img class="r-stretch" src="phonmedia/spectrogram_legendaboiling.png"></p>
<hr />
<h3 id="to-get-this-we-need-to-be-able-to-reliably-find-f0">To get this,
we need to be able to reliably find f0!</h3>
<ul>
<li><p>… and to find where somebody’s talking at all</p></li>
<li><p>So, how’s that happen?</p>
<ul>
<li>… and what kind of representations of speech are used to generate
it?</li>
</ul></li>
</ul>
<hr />
<h3 id="todays-plan">Today’s Plan</h3>
<ul>
<li><p>The Cepstral Domain</p></li>
<li><p>Mel Frequency Cepstral Coefficients (MFCCs)</p></li>
<li><p>Voice Activity Detection</p></li>
<li><p>How do we determine the f0 of the voice?</p></li>
</ul>
<hr />
<h2 id="the-cepstral-domain">The Cepstral Domain</h2>
<hr />
<h3 id="weve-already-thought-a-lot-about-the-time-domain">We’ve already
thought a lot about the time domain</h3>
<ul>
<li><p>Amplitude varying over time gives us the ‘time domain’</p></li>
<li><p>Waveforms view signals in the time domain</p></li>
<li><p><strong>Very good</strong> for understanding pressure, for
playback, and otherwise</p></li>
</ul>
<hr />
<h3 id="then-we-went-into-the-frequency-domain">Then we went into the
frequency domain</h3>
<ul>
<li><p>“What are the component frequencies and their time courses of
this signal?”</p></li>
<li><p>Fourier Transform turns a time-domain signal into a frequency
domain signal</p></li>
<li><p><strong>Very good</strong> for seeing frequency components,
spectral balance, and resonance</p></li>
</ul>
<hr />
<h3 id="now-were-going-into-the-cepstral-domain">Now we’re going into
the cepstral domain</h3>
<ul>
<li><p>“What are the dominant patterns of periodicity? What rates of
change matter most?”</p></li>
<li><p>We do a <em>cepstral</em> transformation to see the cepstral
domain</p></li>
<li><p><strong>Very good</strong> for seeing periodicity and patterns of
change</p></li>
</ul>
<hr />
<h3 id="to-do-a-cepstral-transform">To do a Cepstral Transform</h3>
<ul>
<li><p>First, do fourier analysis to get a spectrum of a signal</p></li>
<li><p>Then, log <strong>each individual amplitude measure in the power
spectrum</strong></p>
<ul>
<li>This de-emphasizes larger values, and has other mathematically
convenient purposes</li>
</ul></li>
<li><p>Then, do a fourier analysis <em>of the log-transformed fourier
analysis</em>!</p>
<ul>
<li>This captures the periodicities <em>of the spectrum itself</em></li>
</ul></li>
</ul>
<hr />
<p><img class="r-stretch" src="phonmedia/spectrum_ae.png"></p>
<hr />
<h3 id="this-results-in-a-cepstrum">This results in a ‘cepstrum’</h3>
<ul>
<li>Cepstra show amplitude by <strong>quefrency</strong>
<ul>
<li>This can be shown on a <strong>cepstrogram</strong></li>
<li>Filtering in the cepstral domain is <strong>liftering</strong></li>
</ul></li>
<li>Spectra separate components by frequency
<ul>
<li>Cepstra separate components by rate of change</li>
</ul></li>
<li>They’re not straightforward to interpret for humans</li>
</ul>
<hr />
<p><img class="r-stretch" src="phonmedia/waveform_eh.png"></p>
<hr />
<p><img class="r-stretch" src="phonmedia/spectrum_eh.png"></p>
<hr />
<p><img class="r-stretch" src="phonmedia/cepstrum_eh.png"></p>
<hr />
<h3 id="this-is-a-very-common-signal-processing-tool">This is a very
common signal processing tool</h3>
<ul>
<li><p>Excellent when what you care about boils down to a change in rate
of change</p></li>
<li><p>… and is very important for…</p></li>
</ul>
<hr />
<h2 id="mel-frequency-cepstral-coefficients-mfccs">Mel-Frequency
Cepstral Coefficients (MFCCs)</h2>
<hr />
<h3
id="mel-frequency-cepstral-coefficients-turn-sounds-into-matrices">Mel-Frequency
Cepstral Coefficients turn sounds into matrices</h3>
<ul>
<li><p>Each frame is put into the frequency domain and scaled to
Mels</p></li>
<li><p>Then it’s put into the quefrency domain, to emphasize time
differences</p></li>
<li><p>Then, we reduce the dimensionality using DCT</p></li>
</ul>
<hr />
<h3 id="why-mels">Why Mels?</h3>
<hr />
<h3 id="our-perception-of-frequency-is-non-linear">Our perception of
frequency is non-linear</h3>
<ul>
<li><p>This is partly due to hearing anatomy</p></li>
<li><p>This is partly evolution’s fault too</p>
<ul>
<li>We care primarily about ~80-4000Hz!</li>
</ul></li>
</ul>
<hr />
<h3 id="but-hertz-doesnt-capture-this-at-all">… but Hertz doesn’t
capture this at all</h3>
<ul>
<li><p>Hertz captures cycles per second</p>
<ul>
<li>… but not our perception of frequency!</li>
</ul></li>
</ul>
<hr />
<h3 id="our-perception-of-frequency-is-weird">Our perception of
frequency is weird!</h3>
<ul>
<li><p>We’ve already talked about auditory masking</p>
<ul>
<li>Two sounds within the same ‘critical band’ seem like one sound</li>
</ul></li>
<li><p>We also percieve jumps in frequency non-linearly</p></li>
</ul>
<hr />
<h3 id="do-we-hear-frequency-in-a-linear-and-reliable-way">Do we hear
frequency in a linear and reliable way?</h3>
<p>Is the jump in file A the same as in file B?</p>
<p>A. <audio controls src="phonmedia/jump_400_600.wav"></audio></p>
<p>B. <audio controls src="phonmedia/jump_10400_10600.wav"></audio></p>
<ul>
<li><strong>Both of these are a 200Hz Jump!</strong></li>
</ul>
<hr />
<p>Is the jump in file A the same as in file B?</p>
<p>A. <audio controls src="phonmedia/jump_400_500.wav"></audio></p>
<p>B. <audio controls src="phonmedia/jump_6400_6500.wav"></audio></p>
<ul>
<li><strong>Both of these are a 100Hz Jump</strong></li>
</ul>
<hr />
<h3 id="so-our-perception-of-frequency-isnt-hertz-like">So, our
perception of frequency isn’t Hertz-like</h3>
<ul>
<li><strong>We want a perceptual scale for hearing!</strong></li>
</ul>
<hr />
<h3 id="mel-scale">Mel Scale</h3>
<ul>
<li><p>Maps numerical pitch measures to human perceptions of changes in
pitch</p></li>
<li><p>People will tell you that a sound’s pitch is ‘half as high’ at
x/2 mels relative to x mels</p></li>
<li><p>Mel is the dominant perceptual frequency scale in use</p></li>
</ul>
<hr />
<h3 id="mel-scale-1">Mel Scale</h3>
<p><img class="r-stretch" src="hearing/mel_scale.jpg"></p>
<hr />
<h3 id="mel-formula">Mel Formula</h3>
<ul>
<li><p>Mel(f) = 1125 * ln(1+f/700)</p>
<ul>
<li>f = Frequency in Hertz</li>
<li>This is a natural log</li>
<li>There are multiple formulae!</li>
</ul></li>
</ul>
<hr />
<h3 id="mfccs-approximate-human-hearing-better">MFCCs approximate human
hearing better</h3>
<ul>
<li><p>Mels scale to the frequencies which matter to humans</p></li>
<li><p>… and hopefully that helps the computers find the most important
elements of speech, too!</p></li>
</ul>
<hr />
<h3
id="mel-frequency-cepstral-coefficient-process-continued">Mel-Frequency
Cepstral Coefficient Process, Continued</h3>
<ul>
<li><p>Each frame is put into the frequency domain and scaled to
Mels</p></li>
<li><p>Then it’s put into the quefrency domain, to emphasize time
differences</p></li>
<li><p>Then, we reduce the dimensionality using DCT</p></li>
</ul>
<hr />
<h3 id="mfccs">MFCCs</h3>
<p><img class="r-stretch" src="phonmedia/mfcc.jpg"></p>
<hr />
<h3 id="discrete-cosine-transform-dct">Discrete Cosine Transform
(DCT)</h3>
<ul>
<li><p>DCT breaks a complex curve into the sum of many cosine functions’
coefficents</p></li>
<li><p>“Which frequency of cosine functions would we need, and with what
power, in order to describe the shape of this curve?”</p>
<ul>
<li>Each cosine we ‘need’ is a coefficient</li>
<li>More coefficients mean more detail, but also more data</li>
<li>Some coefficients <em>really won’t matter at all</em>, and can be
discarded or set to zero</li>
</ul></li>
</ul>
<hr />
<h3 id="dct-for-mfcc">DCT for MFCC</h3>
<ul>
<li><p>“Let’s find the most meaningful N coefficients which describe
what’s going on in the cepstrum <em>for each frame</em>”</p></li>
<li><p>We get a set of numbers which describe the frame-by-frame
cepstral shape of the sound</p>
<ul>
<li>With some of the least important coefficients thrown away</li>
</ul></li>
<li><p>Usually the first 12-13 coefficients are used in MFCCs</p>
<ul>
<li>The highest frequencies aren’t as relevant in the cepstral
domain</li>
</ul></li>
</ul>
<hr />
<h3 id="were-going-to-see-dct-a-lot-this-quarter">We’re going to see DCT
a LOT this quarter</h3>
<ul>
<li><p>Particularly next week</p></li>
<li><p><a href="https://www.youtube.com/watch?v=Q2aEzeMDHMA">You should
watch Computerphile’s Video on DCT</a></p>
<ul>
<li>This will also prepare you for our discussion on compression and
codecs</li>
</ul></li>
</ul>
<hr />
<h3 id="mfcc-input">MFCC Input</h3>
<p><img class="r-stretch" src="phonmedia/noisewaveform.jpg"></p>
<hr />
<h3 id="mfcc-output">MFCC Output</h3>
<p><img class="r-stretch" src="phonmedia/noise_mfcc.jpg"></p>
<hr />
<h3 id="so-the-sound-becomes-a-matrix-of-features">So, the sound becomes
a matrix of features</h3>
<ul>
<li><p>Many rows (representing time during the signal)</p></li>
<li><p>N columns (usually 13) with coefficients which <strong>tell us
the spectral shape</strong></p></li>
<li><p>It’s black-boxy, but we don’t care.</p></li>
<li><p>We’ve created a Matrix</p></li>
</ul>
<hr />
<p><img class="r-stretch" src="humorimg/whoa_neo.jpg"></p>
<hr />
<h3 id="nerdy-aside-mfcc-vs.-pca">Nerdy Aside: MFCC vs. PCA</h3>
<ul>
<li><p>In practice, MFCC is not so different from doing Principal
Component Analysis on a log spectrum</p></li>
<li><p>Both find dominant patterns of variation in the time
domain</p></li>
<li><p>They’re not exactly equivalent, but the results are very
close</p></li>
<li><p>This helps with intuition, if you’re used to PCA!</p></li>
</ul>
<hr />
<h3 id="both-mfcc-and-lpc-represent-the-spectral-shape">Both MFCC and
LPC represent the spectral shape</h3>
<ul>
<li><p>LPC tries explicitly to model only the filter</p></li>
<li><p>MFCC models everything, agnostically</p>
<ul>
<li>It’s also much better for simplifying the signal than LPC</li>
</ul></li>
</ul>
<hr />
<h3 id="now-weve-got-a-matrix-representing-the-sound">Now we’ve got a
matrix representing the sound</h3>
<ul>
<li><p>… which captures frequency information, according to our
perceptual needs</p></li>
<li><p>This will be useful for many things, including…</p></li>
</ul>
<hr />
<h2 id="voice-activity-detection">Voice Activity Detection</h2>
<hr />
<h3 id="not-every-sound-file-contains-speech">Not every sound file
contains speech</h3>
<ul>
<li><p>… sadly</p></li>
<li><p>One important task before we start modeling speech is detecting
whether it’s present or not</p></li>
<li><p>This is referred to as ‘Voice Activity Detection’ or VAD</p></li>
<li><p>“In this file, for every frame, tell me whether there’s voicing
or not”</p></li>
</ul>
<hr />
<h3 id="the-simplicitycomplexity-tradeoff-in-speech-tools">The
Simplicity/Complexity Tradeoff in Speech Tools</h3>
<ul>
<li>There is a ‘dumb and cheap’ way to do a task
<ul>
<li>Poor results, but extremely fast computation and low data
requirements</li>
</ul></li>
<li>Balanced approaches between complexity and accuracy often exist
<ul>
<li>Reasonable results, at cost of complexity and required training
data</li>
<li>Often, these are the top performing ‘legacy’ approaches</li>
</ul></li>
<li>Expensive and accurate approaches exist, but require lots of compute
or <em>massive</em> training data
<ul>
<li>Neural approaches often fall into this category</li>
</ul></li>
<li><strong>We’ll see this tradeoff again and again this
quarter!</strong></li>
</ul>
<hr />
<h3 id="dumb-and-cheap-vad-methods">Dumb and Cheap VAD methods</h3>
<ul>
<li><p>These work poorly, but cheaply!</p></li>
<li><p><strong>Signal Intensity</strong>: “Is this signal loud enough to
be voice?”</p>
<ul>
<li>This is useful for detecting speech vs. silence</li>
</ul></li>
<li><p><strong>Spectral Slope</strong>: “Is there more energy at the
lower frequencies than higher frequencies?”</p>
<ul>
<li>Speech generally has a falling spectral slope</li>
</ul></li>
<li><p><strong>Zero-Crossing Rate</strong>: “How often does this signal
cross zero?”</p>
<ul>
<li>Speech has different rates (due to expected f0 ranges) than
non-speech</li>
</ul></li>
</ul>
<hr />
<p><img class="r-stretch" src="phonmedia/waveform_ae.png"></p>
<hr />
<h3 id="balanced-approaches-to-vad">Balanced Approaches to VAD</h3>
<ul>
<li><strong>Classification based on Spectral Features</strong>: Use
machine learning classifiers to interpret frequency patterns as ‘voice’
or ‘non-voice’
<ul>
<li>“Does this particular balance of power in the spectrum look
speechy?”</li>
</ul></li>
<li><strong>Classification based on Mel-Frequency Cepstral
Coefficients</strong>: Use ML classifiers to interpret MFCCs as “voice”
or not.
<ul>
<li>“Here’s what this sound looks like in its entirety. Is it
speechy?”</li>
</ul></li>
<li>Often, this uses Hidden Markov Models and Gaussian Mixture Models to
do the classification
<ul>
<li>But other classifiers work too!</li>
</ul></li>
</ul>
<hr />
<h3 id="expensive-and-accurate-neural-vad-systems">Expensive and
Accurate Neural VAD systems</h3>
<ul>
<li>Turn a great deal of training data into a matrix of numbers somehow
(MFCCs, wav2vec, or others)
<ul>
<li>Label the data as ‘speech’ and ‘non-speech’</li>
<li>This is generally data from a particular language</li>
</ul></li>
<li>Train a deep neural network on that data
<ul>
<li>Usually CNN, RNN, or LSTM</li>
</ul></li>
<li>The neural network looks at the data and decides
<ul>
<li>Often <em>very</em> robust to noise and more adaptable to new
segments</li>
</ul></li>
</ul>
<hr />
<h3 id="the-output-is-a-frame-by-frame-vector-full-of-0-and-1">The
output is a frame-by-frame vector full of 0 and 1</h3>
<ul>
<li><p>0 indicates ‘No speech in this frame’</p></li>
<li><p>1 indicates ‘There be speech here’</p></li>
<li><p>Some algorithms give you probabilities too</p>
<ul>
<li>“42% chance of speech here”</li>
</ul></li>
</ul>
<hr />
<h3 id="vad-can-be-hard">VAD can be hard!</h3>
<ul>
<li><p>Speech in noise is hard to detect, especially when quiet</p></li>
<li><p>Complex approaches may not detect un-trained kinds of speech</p>
<ul>
<li>Fricatives, clicks, or segments outside the training data</li>
</ul></li>
<li><p>Human voices are very variable</p>
<ul>
<li>Children vs. Adults</li>
</ul></li>
<li><p>Environments vary a lot</p>
<ul>
<li>Voice detection in a crowded restaurant is a different task</li>
</ul></li>
<li><p>Cost is a factor</p>
<ul>
<li>The ‘best’ algorithm may be too slow or expensive for the use
case</li>
</ul></li>
</ul>
<hr />
<h3 id="tuning-for-desired-types-of-error">Tuning for desired types of
error</h3>
<ul>
<li><a href="https://en.wikipedia.org/wiki/All_models_are_wrong">All
models are wrong, some are useful</a>
<ul>
<li>You should make sure your analysis fails optimally</li>
</ul></li>
<li>Do I want to <em>favor preserving as much speech as possible</em>,
and detect other things as speech more often?
<ul>
<li>More false positives</li>
<li>“Failing safe”</li>
</ul></li>
<li>Do I want to <em>favor preserving only speech</em>, and throw out
everything I’m not sure about?
<ul>
<li>More false negatives</li>
</ul></li>
</ul>
<hr />
<h3 id="evaluating-vad">Evaluating VAD</h3>
<ul>
<li><p>What amount of speech is clipped away (at the start and middle of
speech)?</p></li>
<li><p>What amount of noise is called speech at the end of
speech?</p></li>
<li><p>What amount of noise labeled as speech?</p></li>
</ul>
<hr />
<h3 id="what-is-vad-useful-for">What is VAD useful for?</h3>
<ul>
<li><p><strong>Constraining Analyses</strong>: Discard non-speech frames
when doing (e.g.) LPC</p></li>
<li><p><strong>Removing Silences</strong>: In this recording, mark the
places somebody’s talking for transcription</p></li>
<li><p><strong>Mute-when-not-talking</strong>: Don’t transmit the
mechanical keyboard clicking please</p></li>
<li><p><strong>Call Compression</strong>: Don’t send sound data to the
other person when no speech is occurring</p></li>
<li><p><strong>Telemarketing</strong>: Make robocalls, and then put a
human on the line only if a human answers</p></li>
</ul>
<hr />
<h3 id="ok-great-we-know-where-theres-talking">“OK, great, we know where
there’s talking”</h3>
<ul>
<li>“… but we still have to model the f0, how’s that work?”</li>
</ul>
<hr />
<h2 id="f0-detection">f0 Detection</h2>
<hr />
<h3 id="how-do-we-identify-the-f0-of-a-voice">How do we identify the f0
of a voice?</h3>
<ul>
<li>What is the fundamental frequency in a given frame?
<ul>
<li>“168 Hz in frame 134”</li>
</ul></li>
<li>Where are the boundaries of individual glottal pulses?
<ul>
<li>“Delineate the right edge of each period in the glottal cycle in
this file”</li>
</ul></li>
<li>What’s the irregularity of the signal?
<ul>
<li>Do we need to model variation in the cycle time?</li>
</ul></li>
</ul>
<hr />
<h3 id="dumb-and-cheap-f0-detection">Dumb and Cheap f0 Detection</h3>
<ul>
<li><p><strong>Zero Crossing Rate (with filtering)</strong>: I expect f0
to be between 80 and 350 Hz, so possible zero crossing rates associated
with the fundamental would be between X and Y….</p></li>
<li><p><strong>Cepstral Analysis</strong>: Look for prominences in the
Cepstral domain, where the voicing cycle should give a prominent
peak</p>
<ul>
<li>A related measure is <strong>Cepstral Peak Prominence</strong>, a
measure of noise in voicing signals</li>
</ul></li>
<li><p><strong>Harmonic Product Spectra</strong>: Take an FFT, then
downsample it, and multiply the original by the downsampled version,
then repeat</p>
<ul>
<li>The peaks associated with f0 (including harmonics) will grow
stronger, and the noise will fade away</li>
</ul></li>
</ul>
<hr />
<p><img class="r-stretch" src="phonmedia/spectrum_uh.png"></p>
<hr />
<h3 id="the-balanced-approach-autocorrelation-for-f0-tracking">The
Balanced Approach: Autocorrelation for f0 Tracking</h3>
<p><img class="r-stretch" src="diagrams/autocorrelation_animation.gif"></p>
<hr />
<h3
id="autocorrelation-is-a-very-common-approach-for-f0-tracking">Autocorrelation
is a very common approach for f0 tracking</h3>
<ul>
<li><p>Identify candidate pitches based on autocorrelation spikes for
lag</p>
<ul>
<li>“Huh, big spike at 10ms lag, this is probably a 100Hz signal”</li>
</ul></li>
<li><p>Filter out candidates which fall outside the expected pitch
domain (e.g. outside 50 to 400 Hz)</p>
<ul>
<li>This isn’t always desirable for some kinds of voice study</li>
</ul></li>
<li><p>These candidates go on to be post-processed, so we can choose the
right candidates!</p></li>
<li><p><strong>This is how Praat (and others) do f0
tracking</strong></p></li>
</ul>
<hr />
<h3 id="post-processing-constraints-on-f0-data-in-praat"><a
href="https://www.fon.hum.uva.nl/praat/manual/Sound__To_Pitch__filtered_ac____.html">Post-Processing
Constraints on f0 Data in Praat</a></h3>
<ul>
<li><p><strong>Silence threshold</strong>: How quiet should a frame be
before we call it voiceless?</p></li>
<li><p><strong>Voicing threshold</strong>: How much autocorrelation do
you need to consider something voiced?</p></li>
<li><p><strong>Octave cost</strong>: How much do you want to favor
higher-frequency candidates?</p></li>
<li><p><strong>Octave-jump cost</strong>: How much do you want to avoid
sudden octave jumps?</p></li>
<li><p><strong>Voiced/unvoiced cost</strong>: How much do you want to
penalize switching from ‘voiced’ to ‘voiceless’</p></li>
</ul>
<hr />
<h3 id="we-can-smooth-the-output-of-this-function-too">We can smooth the
output of this function, too</h3>
<ul>
<li><p>We don’t tend to suddenly jump way up or down in f0 in one or two
cycles</p></li>
<li><p>Dynamic Programming approaches work well for this</p>
<ul>
<li>“Find the smoothest path through all the candidates”</li>
</ul></li>
</ul>
<hr />
<h3 id="there-are-other-approaches">There are other approaches</h3>
<ul>
<li><p>Phase-based approaches</p></li>
<li><p>Very fancy signal processing approaches</p>
<ul>
<li>e.g. <a
href="https://en.wikipedia.org/wiki/Reassignment_method">Reassignment
Methods</a></li>
</ul></li>
<li><p>Neural Network approaches</p>
<ul>
<li>Good for extracting speech pitch from other pitched noise</li>
</ul></li>
</ul>
<hr />
<h3 id="aside-more-direct-measures-can-be-more-reliable">Aside: More
direct measures can be more reliable</h3>
<ul>
<li><p>Where knowing pitch is crucial (e.g. recording for
text-to-speech), other information sources are used</p></li>
<li><p>Electroglottography (EGG) gives much more reliable and detailed
f0 information</p></li>
</ul>
<hr />
<p><img class="r-stretch" src="phonmedia/egg_electrodes.jpg"></p>
<hr />
<h3 id="difficulties-with-f0-tracking">Difficulties with f0
tracking</h3>
<ul>
<li>Variability of f0
<ul>
<li>Children and Sopranos have higher f0 than expected</li>
</ul></li>
<li>Periodic Background Noise
<ul>
<li>Try doing f0 detection with sheep and goats in the background</li>
</ul></li>
<li>Octave Jumps
<ul>
<li>It’s very easy to find 300 Hz as the ‘f0’ of a 150Hz Signal</li>
</ul></li>
</ul>
<hr />
<h3 id="difficulties-with-f0-tracking-continued">Difficulties with f0
tracking (Continued)</h3>
<ul>
<li>Speaker overlaps
<ul>
<li>Two people talking over each other make complex signals</li>
<li>… but f0 differences are a great way to identify when there are two
speakers!</li>
</ul></li>
<li>f0 has some variability which you need to model
<ul>
<li>Both in period (‘jitter’) and amplitude (‘shimmer’)</li>
</ul></li>
<li>Modeling when f0 stops can be tricky
<ul>
<li>… and if you do it wrong, it’s very much not good</li>
</ul></li>
</ul>
<hr />
<h3 id="why-is-f0-tracking-useful">Why is f0 tracking useful?</h3>
<ul>
<li><p><strong>Modeling the voice:</strong> LPC requires good
information about the excitation signal</p></li>
<li><p><strong>Measuring f0:</strong> f0 is a great cue for many
phenomena in many languages</p>
<ul>
<li>Particularly in tone languages!</li>
</ul></li>
<li><p><strong>Identifying musical notes:</strong> These same algorithms
work great for finding the pitch of musical instruments, too!</p></li>
<li><p><strong>Finding pulses:</strong> Once you’ve found a fundamental,
you can identify pulses too</p></li>
<li><p>… and of course….</p></li>
</ul>
<hr />
<h3 id="modifying-the-pitch-of-a-voice">Modifying the pitch of a
voice!</h3>
<ul>
<li>Next time!</li>
</ul>
<hr />
<h3 id="wrapping-up">Wrapping up</h3>
<ul>
<li><p>Voice Activity Detection identifies frames where there’s
speech</p></li>
<li><p>There are many approaches to VAD, suiting many needs</p></li>
<li><p>f0 detection helps us identify the pitch of the voice over many
frames</p></li>
<li><p>There are many methods, but autocorrelation is unreasonably
effective</p></li>
</ul>
<hr />
<h3 id="for-next-time">For Next Time</h3>
<ul>
<li>We’ll think about how to modify the source, and the filter!</li>
</ul>
<hr />
<p><huge>Thank you!</huge></p>
</body>
</html>
